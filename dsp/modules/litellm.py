from typing import Any, Optional, Literal, Dict, List, Union, cast
from pydantic import BaseModel
import os

if "LITELLM_LOCAL_MODEL_COST_MAP" not in os.environ:
    os.environ["LITELLM_LOCAL_MODEL_COST_MAP"] = "True"

import litellm  
litellm.telemetry = False

from litellm.types.utils import Choices, ModelResponse, Usage
from litellm.caching import Cache

import dspy
from dspy.utils.logging import logger

from dsp.modules.lm import LM
from dsp.utils.settings import settings

# Initialize the cache
litellm.cache = Cache(disk_cache_dir=".dspy_cache", type="disk")

# Use this for development testing
litellm.set_verbose = True

class Message(BaseModel):
    role: str
    content: Optional[str] # Only null for assistant messages with function calls
    name: Optional[str]
    function_call: Optional[Dict[str, Any]]

    def __init__(self, **data):  
        super().__init__(**data)  
        self.__dict__ = {  
            k: v for k, v in self.__dict__.items() if v is not None  
        }  

    # TODO: test this
    @classmethod  
    def create_from_kwargs(cls, kwargs_list):  
        kwargs_dict = {}  
        for kwargs in kwargs_list:  
            kwargs_dict.update(kwargs)  
        return cls(**kwargs_dict)
    
    def to_dict(self):  
        kwargs = self.model_dump(exclude_unset=True)
        return kwargs

class LLM(BaseModel):
    ### required params
    model: str
    # model_type: str # This is needed for litellm.get_supported_openai_params call - this needs to be the name of the model itself (not the deployment name).
    messages: Optional[List[Message]] = []
    ### dspy params 
    history: Optional[List[Dict[str, Any]]] = []
    provider: Optional[str] = "openai" # TODO: do we need the provider?
    # These need to be removed from litellm calls TODO: this list isn't needed
    # non_litellm_params: List[str] = ["model_type", "history", "provider"]

    # Allows extra kwargs to be passed in
    class Config:
        extra = "allow"

    def request(self, **kwargs):
        """
        Litellm request
        """
        print(kwargs)

        response = litellm.completion(**kwargs)
        return response

    def update_messages(self, input, input_type):
        if input_type == "prompt":
            self.messages.append(Message(role="user", content=input, name=None, function_call=None))
        else: #list of messages
            for msg in input:
                self.messages.append(Message.create_from_kwargs(msg))
        return

    def set_provider(self):
        # TODO: confirm if it's ok to assume the provider is openai or the prefix as https://docs.litellm.ai/docs/providers/aleph_alpha doesn't seem to include a prefix
        # TODO: might not be needed unless we want to use litellm.get_supported_openai_params() to get list of params for each model + provider and validate all the args before the llm call
        if "/" in self.model:
            self.provider = self.model.split("/")[0]
        
    def postprocess(self, litellm_response, input_type, only_completed, org_kwargs, litellm_kwargs):

        # extract litellm response

        # to standardize the history, the input is always in the form of the message sent to litellm, however input_type indicates whether it was passed in as a prompt or as is. TODO: review & confirm.

        org_kwargs["only_completed"] = only_completed
        # drop_params already in litellm_kwargs

        # This could technically be a update_history function
        # TODO: I changed the fields in the history, does that impact anything?
        history = {
            "input": [msg.to_dict() for msg in self.messages],
            "input_type": input_type,
            "response": litellm_response,
            "kwargs": org_kwargs,
            "litellm_kwargs": litellm_kwargs,
        }

        self.history.append(history)
        self.messages = [] #Â TODO: change this if we want to accumlate message history for litellm

        result = []

        if only_completed:
            for choice in litellm_response.choices: 
                if choice.finish_reason == "stop":
                    result.append(choice)
            return result
        else:
            return litellm_response.choices


    def preprocess(self, drop_params, input, input_type, **kwargs):   
        """
        Update the kwargs with the necessary parameters to make the litellm call
        """
        # Add caching to kwargs
        enable_cache = dspy.settings.enable_cache
        if "enable_cache" in kwargs: 
            # request param takes precedence
            enable_cache = kwargs["enable_cache"]
        if enable_cache: 
            kwargs['cache'] = {"no-cache": False, "no-store": False}
        else:
            kwargs['cache'] = {"no-cache": True, "no-store": True}

        # get litellm params and exclude rest
        if drop_params:
            kwargs["drop_params"] = True

        # add the litellm params to kwargs
        kwargs["model"] = self.model

        # TODO: see notes in function def
        self.set_provider()

        self.update_messages(input, input_type) # TODO: should we always send all the messages to the litellm call? Or do we need to have a parameter for "keep n messages?" Note that it's not as simple as just truncating the start of the list if we're considering the system message, which would need to stay regardless of the truncation.

        kwargs["messages"] = []
        for msg in self.messages:
            kwargs["messages"].append(msg.to_dict())

        return kwargs

    def __call__(
        self, 
        prompt: Optional[str] = None,
        messages = None, #providing this as an option if someone wants to pass in >1 messsages, especially if they want to include a system message. TODO: better way to do it? Does it make sense to allow a system message in init?
        only_completed: bool = False,
        drop_params: bool = True,
        **kwargs
    ) -> list[dict[str, Any]]:
        
        assert (prompt is not None) != (messages is not None), "Either 'prompt' or 'message' must be set, but not both."

        if prompt: 
            input = prompt
            input_type = "prompt"
        else: 
            input = messages
            input_type = "message"

        # all_kwargs = self.model_dump()   
          
        # # Filter out the standard fields to get only the extra kwargs  
        # standard_fields = set(self.__fields__.keys())  
        # extra_kwargs = {k: v for k, v in all_kwargs.items() if k not in standard_fields}
        extra_kwargs = self.model_extra
  
        # Merge the kwargs with any additional ones passed directly to the method
        kwargs = kwargs | extra_kwargs  

        litellm_kwargs = self.preprocess(drop_params, input, input_type, **kwargs) 
        litellm_response = self.request(**litellm_kwargs)
        result = self.postprocess(litellm_response, input_type, only_completed, kwargs, litellm_kwargs) 

        return result