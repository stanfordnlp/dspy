{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Contextual AI Reranker Integration with DSPy\n",
        "\n",
        "This tutorial demonstrates how to integrate Contextual AI's instruction-following reranker with DSPy for improved retrieval-augmented generation (RAG) performance. We'll show how DSPy's optimization capabilities can work with Contextual AI's reranking to achieve better results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dspy\n",
        "import os\n",
        "import requests\n",
        "import ujson\n",
        "import random\n",
        "from dspy.utils import download\n",
        "from dspy.evaluate import SemanticF1\n",
        "\n",
        "# Set API keys\n",
        "# Get this key at http://app.contextual.ai/\n",
        "CONTEXTUAL_API_KEY = \"your_contextual_api_key\"\n",
        "OPENAI_API_KEY = \"your_openai_api_key\"\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "lm = dspy.LM(model=\"openai/gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
        "dspy.settings.configure(lm=lm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contextual AI Reranker Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ContextualReranker(dspy.Retrieve):\n",
        "    def __init__(self, api_key, base_retriever=None, k=5, rerank_instructions=\"\"):\n",
        "        super().__init__(k=k)\n",
        "        self.api_key = api_key\n",
        "        self.base_retriever = base_retriever\n",
        "        self.rerank_instructions = rerank_instructions\n",
        "    \n",
        "    def forward(self, query_or_queries):\n",
        "        if self.base_retriever:\n",
        "            # Get initial documents from base retriever\n",
        "            initial_docs = self.base_retriever(query_or_queries)\n",
        "            documents = initial_docs.passages\n",
        "        else:\n",
        "            documents = query_or_queries if isinstance(query_or_queries, list) else [query_or_queries]\n",
        "        \n",
        "        url = \"https://api.contextual.ai/v1/rerank\"\n",
        "        headers = {\n",
        "            \"accept\": \"application/json\",\n",
        "            \"content-type\": \"application/json\",\n",
        "            \"authorization\": f\"Bearer {self.api_key}\"\n",
        "        }\n",
        "        \n",
        "        payload = {\n",
        "            \"query\": query_or_queries if isinstance(query_or_queries, str) else query_or_queries[0],\n",
        "            \"documents\": documents,\n",
        "            \"model\": \"ctxl-rerank-v2-instruct-multilingual\",\n",
        "            \"top_n\": self.k,\n",
        "            \"instruction\": self.rerank_instructions\n",
        "        }\n",
        "        \n",
        "        response = requests.post(url, headers=headers, json=payload)\n",
        "        result = response.json()\n",
        "        \n",
        "        if 'results' not in result:\n",
        "            print(\"Contextual API Error:\", result)\n",
        "            return dspy.Prediction(passages=documents[:self.k])\n",
        "        \n",
        "        reranked_results = result['results']\n",
        "        reranked_results.sort(key=lambda x: x['relevance_score'], reverse=True)\n",
        "        \n",
        "        top_docs = []\n",
        "        for item in reranked_results[:self.k]:\n",
        "            doc = documents[item['index']]\n",
        "            if isinstance(doc, str):\n",
        "                top_docs.append(doc)\n",
        "            else:\n",
        "                top_docs.append(doc.get('content', doc))\n",
        "        \n",
        "        return dspy.Prediction(passages=top_docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 500 documents\n"
          ]
        }
      ],
      "source": [
        "# Download and load the RAG-QA Arena Tech dataset\n",
        "download(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_corpus.jsonl\")\n",
        "\n",
        "with open(\"ragqa_arena_tech_corpus.jsonl\") as f:\n",
        "    corpus_lines = f.readlines()[:500]\n",
        "\n",
        "corpus = []\n",
        "for line in corpus_lines:\n",
        "    data = ujson.loads(line)\n",
        "    corpus.append(data['text'])\n",
        "\n",
        "print(f\"Loaded {len(corpus)} documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 'ragqa_arena_tech_examples.jsonl'...\n",
            "Training: 200, Dev: 300, Test: 500\n"
          ]
        }
      ],
      "source": [
        "# Load question-answer pairs for evaluation\n",
        "download(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_examples.jsonl\")\n",
        "\n",
        "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
        "    data = [ujson.loads(line) for line in f]\n",
        "\n",
        "data = [dspy.Example(**d).with_inputs('question') for d in data]\n",
        "\n",
        "# Split data for training, validation, and testing\n",
        "random.Random(0).shuffle(data)\n",
        "trainset, devset, testset = data[:200], data[200:500], data[500:1000]\n",
        "\n",
        "print(f\"Training: {len(trainset)}, Dev: {len(devset)}, Test: {len(testset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Base Retriever\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create base embedding retriever\n",
        "embedder = dspy.Embedder('openai/text-embedding-3-small', dimensions=512, api_key=OPENAI_API_KEY)\n",
        "base_search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=10)\n",
        "base_search_wide = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create RAG Systems\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAG(dspy.Module):\n",
        "    def __init__(self, retriever):\n",
        "        super().__init__()\n",
        "        self.retriever = retriever\n",
        "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
        "    \n",
        "    def forward(self, question):\n",
        "        if hasattr(question, 'question'):\n",
        "            question = question.question\n",
        "        elif isinstance(question, dict) and 'question' in question:\n",
        "            question = question['question']\n",
        "        context = self.retriever(question).passages\n",
        "        return self.respond(context=context, question=question)\n",
        "\n",
        "base_rag = RAG(base_search)\n",
        "\n",
        "# RAG with Contextual AI reranking -> Changing the prompt can yield different results\n",
        "contextual_reranker = ContextualReranker(\n",
        "    api_key=CONTEXTUAL_API_KEY,\n",
        "    base_retriever=base_search_wide,\n",
        "    k=10,\n",
        "    rerank_instructions=\"Prioritize documents that provide specific, actionable technical solutions and step-by-step instructions.\"\n",
        ")\n",
        "reranked_rag = RAG(contextual_reranker)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup evaluation metric\n",
        "metric = SemanticF1(decompositional=True)\n",
        "evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=4, display_progress=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Reranker\n",
        "\n",
        "Let's test our reranker with a simple example to see how it works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: How do I fix my Linux system\n",
            "\n",
            "Base retrieval results:\n",
            "1. For some reason the disk my Mac was booting from was the macOS Installer Disk, all I had to do: Hold down the option (-alt) key before the Apple logo shows. Then just select your Macintosh HD (or how ...\n",
            "\n",
            "2. In addition to the preceding answers, which mention only Windows, and since theres a dup-closed question Does WannaCry infect Linux? pointing to this one, Id like to add that Linux machines can get in...\n",
            "\n",
            "3. Riffing off of Prabhats question above, I had this issue in macos high sierra when I stranded an encfs process, rebooting solved it, but this ps -ef | grep name-of-busy-dir Showed me the process and t...\n",
            "\n",
            "Reranked results:\n",
            "1. You dont typically clear the journal yourself. That is managed by systemd itself and old logs are rotated out as new data comes in. The correct thing to do would be to schedule journald to only keep a...\n",
            "\n",
            "2. You need to give permission to the path. run this in command line and you will be fine to go. It worked for me: sudo chown -R $USER /usr/local...\n",
            "\n",
            "3. My resolution was similar to Roman Ts however I needed to add a few extra steps. In my case I had Ubuntu Server 14 VM running on a Windows 8 Desktop in Windows 2008 domain. If I tried NAT or Bridge I ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test query - something relevant to the tech corpus\n",
        "test_query = \"How do I fix my Linux system\"\n",
        "\n",
        "print(f\"Query: {test_query}\\n\")\n",
        "\n",
        "base_results = base_search(test_query)\n",
        "print(\"Base retrieval results:\")\n",
        "for i, doc in enumerate(base_results.passages[:3]):\n",
        "    print(f\"{i+1}. {doc[:200]}...\\n\")\n",
        "\n",
        "reranked_results = contextual_reranker(test_query)\n",
        "print(\"Reranked results:\")\n",
        "for i, doc in enumerate(reranked_results.passages[:3]):\n",
        "    print(f\"{i+1}. {doc[:200]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyzing the Results\n",
        "\n",
        "Notice the difference in quality between the two responses:\n",
        "\n",
        "**Base Retrieval Issues:**\n",
        "- Retrieved documents about macOS booting issues (not Linux-specific)\n",
        "- Included content about WannaCry malware (not relevant to fixing systems)\n",
        "- Retrieved macOS High Sierra troubleshooting (wrong OS)\n",
        "\n",
        "**Reranked Retrieval Improvements:**\n",
        "- Focused on systemd journal management (actual Linux system administration)\n",
        "- Provided file permission fixes with specific commands (`sudo chown`)\n",
        "- Delivered actionable solutions for common Linux issues\n",
        "\n",
        "**Impact on Response Quality:**\n",
        "- **Base RAG**: Gave generic troubleshooting steps not specific to the retrieved context\n",
        "- **Reranked RAG**: Provided concrete, actionable Linux commands with file paths and configuration details\n",
        "\n",
        "The reranker successfully filtered out irrelevant OS-specific content and prioritized documents with \"specific, actionable technical solutions\" as specified in our rerank instructions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base RAG Response:To fix your Linux system, start by identifying any running processes that might be causing issues. You can use the command `ps aux` to list all processes. If you suspect a specific process is problematic, you can terminate it using `sudo kill -15 <PID>` where `<PID>` is the process ID. Additionally, check your disk partitions with `df -Th` to ensure they are correctly formatted and mounted. If you encounter file system errors, running `fsck` can help resolve them. If these steps do not resolve the issue, consider backing up your data and performing a clean installation of the operating system.\n"
          ]
        }
      ],
      "source": [
        "# Same Query\n",
        "base_response = base_rag(question=test_query)\n",
        "reranked_response = reranked_rag(question=test_query)\n",
        "\n",
        "print(f\"Base RAG Response:{base_response.response}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reranked RAG Response:To fix your Linux system, first identify the specific issue you're encountering. If it's a permission problem, use `sudo chown -R $USER /path/to/directory` to change ownership. For network issues, ensure your network interfaces are configured correctly in `/etc/network/interfaces` and restart the networking service. If you're having trouble with packages, use `dpkg-deb` to manage them. For log management, adjust the settings in `/etc/systemd/journald.conf` to control log size. If you provide more details about the specific problem, I can offer more targeted advice.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Reranked RAG Response:{reranked_response.response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quantitative Evaluation Results\n",
        "\n",
        "Now let's compare the performance of different RAG configurations on our development set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 121.75 / 300 (40.6%): 100%|██████████| 300/300 [16:48<00:00,  3.36s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/13 12:36:26 INFO dspy.evaluate.evaluate: Average Metric: 121.75266058090479 / 300 (40.6%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "EvaluationResult(score=40.58, results=<list of 300 results>)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(base_rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Contextual AI reranked RAG system...\n",
            "Average Metric: 127.27 / 300 (42.4%): 100%|██████████| 300/300 [20:48<00:00,  4.16s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/10/13 13:44:46 INFO dspy.evaluate.evaluate: Average Metric: 127.27461854472807 / 300 (42.4%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluating Contextual AI reranked RAG system...\")\n",
        "reranked_score = evaluate(reranked_rag)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results Comparison\n",
        "\n",
        "| Configuration | SemanticF1 Score | Improvement | Description |\n",
        "|--------------|------------------|-------------|-------------|\n",
        "| **Base RAG (CoT)** | **40.6%** | Baseline | Embeddings retrieval + Chain-of-Thought |\n",
        "| **Reranked RAG** | **42.4%** | **+1.8%** | Contextual AI reranking + Chain-of-Thought |\n",
        "\n",
        "### Understanding the Results\n",
        "\n",
        "**Why scores seem low:** F1 score is strict—it penalizes different wording even when meaning is correct. Scores of 40-45% are typical for RAG systems since there are many correct ways to answer a question, but F1 only compares to one reference answer.\n",
        "\n",
        "**What matters:** \n",
        "With only 500 documents and answers to choose from this improvement is impressive for a reranker. Our 1.8 percentage point gain  shows the reranker consistently produces better answers across 300 test questions—that's 2 additional correct answers per 100 queries with minimal cost and latency.\n",
        "\n",
        "**Why it works:**\n",
        "- Reranker examines 50 documents instead of 10\n",
        "- Finds better technical content ranked lower by embeddings alone\n",
        "- Uses instructions to prioritize actionable, step-by-step solutions\n",
        "\n",
        "**Note:** In RAG research, 2-5% improvements are meaningful when building on strong baselines. Larger corpora (1000+ docs) typically show 5-10% gains.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Insights:**\n",
        "\n",
        "1. **Instruction-Following Advantage**: Contextual AI's reranker lets you:\n",
        "   - Customize ranking through natural language instructions\n",
        "   - Adapt to domains without retraining\n",
        "   - Understand context (e.g., \"fix\" means troubleshooting, not just semantic similarity)\n",
        "\n",
        "2. **When to Use Reranking:**\n",
        "   - Domain-specific requirements (e.g., \"prioritize official docs\" or \"prefer recent content\")\n",
        "   - Initial retrieval returns too many marginally relevant documents (**in this case there was only 500 documents**)\n",
        "   - Need filtering by criteria embeddings can't capture (metadata, recency, source authority)\n",
        "   - Working with larger corpora where improvement potential is higher\n",
        "\n",
        "The reranker's instruction-following capability means you can adapt to different use cases by just changing instructions—no retraining required!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
