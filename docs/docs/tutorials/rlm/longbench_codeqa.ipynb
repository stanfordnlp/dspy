{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Tutorial: RLM on LongBench-v2 Code Repository Understanding\n\nThis tutorial demonstrates **Recursive Language Models (RLM)** on the [LongBench-v2](https://huggingface.co/datasets/THUDM/LongBench-v2) Code Repository Understanding benchmark.\n\nLongBench-v2 is a challenging benchmark with 503 multiple-choice questions requiring deep understanding and reasoning over contexts ranging from 8K to 2M words. The **Code Repository Understanding** split contains 50 questions about real code repositories.\n\nReference:\n- [\"Recursive Language Models\" (Zhang, Kraska, Khattab, 2025)](https://arxiv.org/abs/placeholder)\n- [\"LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\" (Bai et al., 2024)](https://arxiv.org/abs/2412.15204)\n\nInstall dependencies: `pip install dspy datasets`"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": "## Setup\n\nConfigure DSPy with an LLM."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pydantic')\n",
    "\n",
    "import dspy\n",
    "\n",
    "lm = dspy.LM(\"openai/gpt-5\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load the LongBench-v2 Dataset\n",
    "\n",
    "LongBench-v2 contains 503 challenging multiple-choice questions across 6 domains:\n",
    "- Single-Document QA (175)\n",
    "- Multi-Document QA (125)\n",
    "- Long In-context Learning (81)\n",
    "- Code Repository Understanding (50)\n",
    "- Long-dialogue History Understanding (39)\n",
    "- Long Structured Data Understanding (33)\n",
    "\n",
    "We focus on **Code Repository Understanding** - questions about real code repositories with contexts up to 2M characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('THUDM/LongBench-v2', split='train')\n",
    "\n",
    "print(f\"Total examples: {len(dataset)}\")\n",
    "print(f\"\\nDomains:\")\n",
    "for domain, count in sorted(Counter(row['domain'] for row in dataset).items()):\n",
    "    print(f\"  {domain}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Code Repository Understanding\n",
    "code_examples = [row for row in dataset if row['domain'] == 'Code Repository Understanding']\n",
    "\n",
    "print(f\"Code Repository Understanding: {len(code_examples)} examples\")\n",
    "print(f\"\\nDifficulty distribution:\")\n",
    "print(f\"  {dict(Counter(row['difficulty'] for row in code_examples))}\")\n",
    "print(f\"\\nLength distribution:\")\n",
    "print(f\"  {dict(Counter(row['length'] for row in code_examples))}\")\n",
    "\n",
    "# Show context length statistics\n",
    "context_lens = [len(row['context']) for row in code_examples]\n",
    "print(f\"\\nContext lengths:\")\n",
    "print(f\"  Min: {min(context_lens):,} chars\")\n",
    "print(f\"  Max: {max(context_lens):,} chars\")\n",
    "print(f\"  Mean: {sum(context_lens)//len(context_lens):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Prepare Examples for Evaluation\n",
    "\n",
    "Convert the dataset to `dspy.Example` format. Each question has 4 choices (A, B, C, D) and one correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def make_example(row):\n",
    "    \"\"\"Convert a LongBench-v2 row to a DSPy example.\"\"\"\n",
    "    # Format choices as part of the query - emphasize single letter answer\n",
    "    choices_text = f\"\"\"\n",
    "\n",
    "Choices:\n",
    "A) {row['choice_A']}\n",
    "B) {row['choice_B']}\n",
    "C) {row['choice_C']}\n",
    "D) {row['choice_D']}\"\"\"\n",
    "    \n",
    "    return dspy.Example(\n",
    "        id=row[\"_id\"],\n",
    "        context=row[\"context\"],\n",
    "        query=row[\"question\"] + choices_text,\n",
    "        answer=row[\"answer\"],  # A, B, C, or D\n",
    "        difficulty=row[\"difficulty\"],\n",
    "        length=row[\"length\"],\n",
    "        choice_A=row[\"choice_A\"],\n",
    "        choice_B=row[\"choice_B\"],\n",
    "        choice_C=row[\"choice_C\"],\n",
    "        choice_D=row[\"choice_D\"],\n",
    "    ).with_inputs(\"context\", \"query\")\n",
    "\n",
    "# Create examples and shuffle\n",
    "examples = [make_example(row) for row in code_examples]\n",
    "random.shuffle(examples)\n",
    "\n",
    "# Split: first 10 as devset, rest as testset\n",
    "devset = examples[:25]\n",
    "testset = examples[25:]\n",
    "\n",
    "print(f\"Devset: {len(devset)} examples\")\n",
    "print(f\"Testset: {len(testset)} examples\")\n",
    "\n",
    "# Show an example\n",
    "ex = devset[0]\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Difficulty: {ex.difficulty}\")\n",
    "print(f\"  Length: {ex.length}\")\n",
    "print(f\"  Context: {len(ex.context):,} chars\")\n",
    "print(f\"  Question: {ex.query[:200]}...\")\n",
    "print(f\"  Answer: {ex.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Define the Metric and Signature\n",
    "\n",
    "Simple multiple-choice accuracy: exact match on the letter (A, B, C, or D).\n",
    "\n",
    "We use a `Literal` type in the signature to constrain the output to valid choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longbench_code_metric(example, pred, trace=None):\n",
    "    \"\"\"Multiple choice accuracy metric.\"\"\"\n",
    "    gold = example.answer.strip().upper()\n",
    "    predicted = pred.answer.strip().upper() if pred.answer else \"\"\n",
    "    return 1.0 if predicted == gold else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## Initialize RLM with LocalSandbox\n\nRLM uses `LocalSandbox` which runs code in a secure local WASM sandbox via Deno/Pyodide. This is ideal for:\n- **Very long contexts**: Code repositories can be 1-2M characters\n- **Security**: Code runs in an isolated WASM environment\n- **Tool support**: `llm_query()` is available for semantic analysis\n\nThe LLM can:\n- Navigate the code repository using string operations, regex, etc.\n- Call `llm_query(prompt)` to ask questions about code snippets\n- Use `FINAL(\"A\")` to submit the final answer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "from dspy.predict.rlm import RLM\nfrom typing import Literal\n\n# Define signature with Literal type for answer\nclass CodeQA(dspy.Signature):\n    \"\"\"Answer a multiple choice question about a code repository.\"\"\"\n    context: str = dspy.InputField(desc=\"The code repository contents\")\n    query: str = dspy.InputField(desc=\"The question with choices A, B, C, D\")\n    answer: Literal[\"A\", \"B\", \"C\", \"D\"] = dspy.OutputField(desc=\"The answer: A, B, C, or D\")\n\n# Create RLM module with typed signature (uses LocalSandbox by default)\nrlm = RLM(\n    CodeQA,\n    max_iterations=15,\n    verbose=True,\n)\n\nprint(\"RLM initialized with LocalSandbox\")\nprint(f\"  Max iterations: {rlm.max_iterations}\")\nprint(f\"  LLM: {dspy.settings.lm.model}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Run on a Single Example\n",
    "\n",
    "Let's test RLM on one example to see how it explores the code repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trajectory(trajectory):\n",
    "    \"\"\"Pretty-print an RLM trajectory.\"\"\"\n",
    "    for i, step in enumerate(trajectory):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Step {i+1}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if step.get(\"reasoning\"):\n",
    "            reasoning = step['reasoning']\n",
    "            if len(reasoning) > 300:\n",
    "                reasoning = reasoning[:300] + \"...\"\n",
    "            print(f\"\\nReasoning: {reasoning}\")\n",
    "        \n",
    "        print(f\"\\nCode:\")\n",
    "        print(f\"```python\")\n",
    "        print(step[\"code\"])\n",
    "        print(f\"```\")\n",
    "        \n",
    "        print(f\"\\nOutput:\")\n",
    "        output = step[\"output\"]\n",
    "        if len(output) > 500:\n",
    "            print(output[:500] + \"\\n... (truncated)\")\n",
    "        else:\n",
    "            print(output if output else \"(no output)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick an example (prefer shorter context for faster demo)\n",
    "example = min(devset, key=lambda x: len(x.context))\n",
    "\n",
    "print(f\"Example:\")\n",
    "print(f\"  Difficulty: {example.difficulty}\")\n",
    "print(f\"  Context length: {len(example.context):,} chars\")\n",
    "print(f\"  Question: {example.query}\")\n",
    "print(f\"  Expected answer: {example.answer}\")\n",
    "print(f\"\\nRunning RLM...\")\n",
    "\n",
    "result = rlm(context=example.context, query=example.query)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Predicted: {result.answer}\")\n",
    "print(f\"Expected: {example.answer}\")\n",
    "print(f\"Correct: {result.answer == example.answer}\")\n",
    "print(f\"Steps: {len(result.trajectory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the trajectory\n",
    "print_trajectory(result.trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Run Evaluation on Devset\n",
    "\n",
    "Use `dspy.Evaluate` to run RLM on the devset with parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# For evaluation, RLM defaults to LocalSandbox (local Deno/Pyodide sandbox).\nrlm_eval = RLM(\n    CodeQA,\n    max_iterations=15,\n    verbose=False,\n)\n\n# Run evaluation on a subset (full devset takes longer due to large contexts)\neval_set = devset\n\nevaluate = dspy.Evaluate(\n    devset=eval_set,\n    metric=longbench_code_metric,\n    num_threads=1,\n    display_progress=True,\n    display_table=5,\n    provide_traceback=True,\n)\n\nprint(f\"Evaluating on {len(eval_set)} examples...\")\nresults = evaluate(rlm_eval)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLongBench-v2 Code Repository Understanding Results:\")\n",
    "print(f\"  Examples: {len(eval_set)}\")\n",
    "print(f\"  Accuracy: {results.score:.1f}%\")\n",
    "print(f\"  LLM: {dspy.settings.lm.model}\")\n",
    "\n",
    "# Trajectory statistics\n",
    "if results.results:\n",
    "    trajectory_lengths = [len(r[1].trajectory) for r in results.results if hasattr(r[1], 'trajectory')]\n",
    "    if trajectory_lengths:\n",
    "        print(f\"  Avg trajectory length: {sum(trajectory_lengths)/len(trajectory_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "LongBench-v2 Code Repository Understanding is challenging because:\n",
    "\n",
    "1. **Massive contexts**: Code repositories can be 1-2M characters\n",
    "2. **Deep understanding required**: Questions require understanding code structure, function signatures, and logic\n",
    "3. **Human expert baseline is 53.7%**: Even experts struggle with these questions\n",
    "\n",
    "RLM helps by:\n",
    "- Allowing the LLM to programmatically search through the codebase\n",
    "- Using `llm_query()` to reason about specific code snippets\n",
    "- Building up understanding iteratively through multiple code executions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Try different LLMs**: Stronger models like GPT-4 may improve accuracy\n",
    "2. **Increase iterations**: Complex questions may need more exploration steps\n",
    "3. **Optimize prompts**: The RLM prompts can be tuned for code understanding\n",
    "4. **Run full evaluation**: Use `testset` for complete benchmark results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}