{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial: RLM on LongBench-v2 Code Repository Understanding\n",
    "\n",
    "This tutorial demonstrates **Recursive Language Models (RLM)** on the [LongBench-v2](https://huggingface.co/datasets/THUDM/LongBench-v2) Code Repository Understanding benchmark.\n",
    "\n",
    "LongBench-v2 is a challenging benchmark with 503 multiple-choice questions requiring deep understanding and reasoning over contexts ranging from 8K to 2M words. The **Code Repository Understanding** split contains 50 questions about real code repositories.\n",
    "\n",
    "Reference:\n",
    "- [\"Recursive Language Models\" (Zhang, Kraska, Khattab, 2025)](https://arxiv.org/abs/placeholder)\n",
    "- [\"LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\" (Bai et al., 2024)](https://arxiv.org/abs/2412.15204)\n",
    "\n",
    "Install dependencies: `pip install dspy datasets e2b-code-interpreter`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Configure DSPy with an LLM and load environment variables for E2B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isaac/Projects/dspy-worktrees/isaac/isaac/dspy-rlm/.venv/lib/python3.14/site-packages/openai/_compat.py:48: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.typing import (\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pydantic')\n",
    "\n",
    "import dspy\n",
    "\n",
    "lm = dspy.LM(\"openai/gpt-5\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load the LongBench-v2 Dataset\n",
    "\n",
    "LongBench-v2 contains 503 challenging multiple-choice questions across 6 domains:\n",
    "- Single-Document QA (175)\n",
    "- Multi-Document QA (125)\n",
    "- Long In-context Learning (81)\n",
    "- Code Repository Understanding (50)\n",
    "- Long-dialogue History Understanding (39)\n",
    "- Long Structured Data Understanding (33)\n",
    "\n",
    "We focus on **Code Repository Understanding** - questions about real code repositories with contexts up to 2M characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 503\n",
      "\n",
      "Domains:\n",
      "  Code Repository Understanding: 50\n",
      "  Long In-context Learning: 81\n",
      "  Long Structured Data Understanding: 33\n",
      "  Long-dialogue History Understanding: 39\n",
      "  Multi-Document QA: 125\n",
      "  Single-Document QA: 175\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset('THUDM/LongBench-v2', split='train')\n",
    "\n",
    "print(f\"Total examples: {len(dataset)}\")\n",
    "print(f\"\\nDomains:\")\n",
    "for domain, count in sorted(Counter(row['domain'] for row in dataset).items()):\n",
    "    print(f\"  {domain}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Repository Understanding: 50 examples\n",
      "\n",
      "Difficulty distribution:\n",
      "  {'easy': 18, 'hard': 32}\n",
      "\n",
      "Length distribution:\n",
      "  {'long': 29, 'short': 12, 'medium': 9}\n",
      "\n",
      "Context lengths:\n",
      "  Min: 101,348 chars\n",
      "  Max: 16,182,936 chars\n",
      "  Mean: 3,560,623 chars\n"
     ]
    }
   ],
   "source": [
    "# Filter for Code Repository Understanding\n",
    "code_examples = [row for row in dataset if row['domain'] == 'Code Repository Understanding']\n",
    "\n",
    "print(f\"Code Repository Understanding: {len(code_examples)} examples\")\n",
    "print(f\"\\nDifficulty distribution:\")\n",
    "print(f\"  {dict(Counter(row['difficulty'] for row in code_examples))}\")\n",
    "print(f\"\\nLength distribution:\")\n",
    "print(f\"  {dict(Counter(row['length'] for row in code_examples))}\")\n",
    "\n",
    "# Show context length statistics\n",
    "context_lens = [len(row['context']) for row in code_examples]\n",
    "print(f\"\\nContext lengths:\")\n",
    "print(f\"  Min: {min(context_lens):,} chars\")\n",
    "print(f\"  Max: {max(context_lens):,} chars\")\n",
    "print(f\"  Mean: {sum(context_lens)//len(context_lens):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Prepare Examples for Evaluation\n",
    "\n",
    "Convert the dataset to `dspy.Example` format. Each question has 4 choices (A, B, C, D) and one correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devset: 25 examples\n",
      "Testset: 25 examples\n",
      "\n",
      "Example:\n",
      "  Difficulty: easy\n",
      "  Length: short\n",
      "  Context: 108,019 chars\n",
      "  Question: In this cloud storage system, the scheduler is a crucial component responsible for allocating file blocks to the appropriate slave servers. There are three Raspberry Pi devices acting as slave servers...\n",
      "  Answer: D\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def make_example(row):\n",
    "    \"\"\"Convert a LongBench-v2 row to a DSPy example.\"\"\"\n",
    "    # Format choices as part of the query - emphasize single letter answer\n",
    "    choices_text = f\"\"\"\n",
    "\n",
    "Choices:\n",
    "A) {row['choice_A']}\n",
    "B) {row['choice_B']}\n",
    "C) {row['choice_C']}\n",
    "D) {row['choice_D']}\"\"\"\n",
    "    \n",
    "    return dspy.Example(\n",
    "        id=row[\"_id\"],\n",
    "        context=row[\"context\"],\n",
    "        query=row[\"question\"] + choices_text,\n",
    "        answer=row[\"answer\"],  # A, B, C, or D\n",
    "        difficulty=row[\"difficulty\"],\n",
    "        length=row[\"length\"],\n",
    "        choice_A=row[\"choice_A\"],\n",
    "        choice_B=row[\"choice_B\"],\n",
    "        choice_C=row[\"choice_C\"],\n",
    "        choice_D=row[\"choice_D\"],\n",
    "    ).with_inputs(\"context\", \"query\")\n",
    "\n",
    "# Create examples and shuffle\n",
    "examples = [make_example(row) for row in code_examples]\n",
    "random.shuffle(examples)\n",
    "\n",
    "# Split: first 10 as devset, rest as testset\n",
    "devset = examples[:25]\n",
    "testset = examples[25:]\n",
    "\n",
    "print(f\"Devset: {len(devset)} examples\")\n",
    "print(f\"Testset: {len(testset)} examples\")\n",
    "\n",
    "# Show an example\n",
    "ex = devset[0]\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Difficulty: {ex.difficulty}\")\n",
    "print(f\"  Length: {ex.length}\")\n",
    "print(f\"  Context: {len(ex.context):,} chars\")\n",
    "print(f\"  Question: {ex.query[:200]}...\")\n",
    "print(f\"  Answer: {ex.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Define the Metric and Signature\n",
    "\n",
    "Simple multiple-choice accuracy: exact match on the letter (A, B, C, or D).\n",
    "\n",
    "We use a `Literal` type in the signature to constrain the output to valid choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def longbench_code_metric(example, pred, trace=None):\n",
    "    \"\"\"Multiple choice accuracy metric.\"\"\"\n",
    "    gold = example.answer.strip().upper()\n",
    "    predicted = pred.answer.strip().upper() if pred.answer else \"\"\n",
    "    return 1.0 if predicted == gold else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## Initialize RLM with E2B Sandbox\n\nWe use `E2BSandbox` which runs code in a secure cloud sandbox. This is ideal for:\n- **Very long contexts**: Code repositories can be 1-2M characters\n- **Security**: Untrusted code runs in isolated Firecracker microVMs\n- **Tool support**: `llm_query()` is available for semantic analysis\n\nThe LLM can:\n- Navigate the code repository using string operations, regex, etc.\n- Call `llm_query(prompt)` to ask questions about code snippets\n- Use `FINAL(\"A\")` to submit the final answer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "from dspy.primitives.e2b_sandbox import E2BSandbox\nfrom dspy.predict.rlm import RLM\nfrom typing import Literal\n\n# Define signature with Literal type for answer\nclass CodeQA(dspy.Signature):\n    \"\"\"Answer a multiple choice question about a code repository.\"\"\"\n    context: str = dspy.InputField(desc=\"The code repository contents\")\n    query: str = dspy.InputField(desc=\"The question with choices A, B, C, D\")\n    answer: Literal[\"A\", \"B\", \"C\", \"D\"] = dspy.OutputField(desc=\"The answer: A, B, C, or D\")\n\n# Create sandbox (picks up OPENAI_API_KEY and E2B_API_KEY from environment)\nsandbox = E2BSandbox()\n\n# Create RLM module with typed signature\nrlm = RLM(\n    CodeQA,\n    max_iterations=15,\n    interpreter=sandbox,\n    verbose=True,\n)\n\nprint(\"RLM initialized with E2BSandbox\")\nprint(f\"  Max iterations: {rlm.max_iterations}\")\nprint(f\"  LLM: {dspy.settings.lm.model}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Run on a Single Example\n",
    "\n",
    "Let's test RLM on one example to see how it explores the code repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trajectory(trajectory):\n",
    "    \"\"\"Pretty-print an RLM trajectory.\"\"\"\n",
    "    for i, step in enumerate(trajectory):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Step {i+1}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if step.get(\"reasoning\"):\n",
    "            reasoning = step['reasoning']\n",
    "            if len(reasoning) > 300:\n",
    "                reasoning = reasoning[:300] + \"...\"\n",
    "            print(f\"\\nReasoning: {reasoning}\")\n",
    "        \n",
    "        print(f\"\\nCode:\")\n",
    "        print(f\"```python\")\n",
    "        print(step[\"code\"])\n",
    "        print(f\"```\")\n",
    "        \n",
    "        print(f\"\\nOutput:\")\n",
    "        output = step[\"output\"]\n",
    "        if len(output) > 500:\n",
    "            print(output[:500] + \"\\n... (truncated)\")\n",
    "        else:\n",
    "            print(output if output else \"(no output)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "  Difficulty: easy\n",
      "  Context length: 108,019 chars\n",
      "  Question: In this cloud storage system, the scheduler is a crucial component responsible for allocating file blocks to the appropriate slave servers. There are three Raspberry Pi devices acting as slave servers, named pi1, pi2, and pi3, with remaining storage capacities of 800 bytes, 700 bytes, and 500 bytes, respectively. A file has been divided into 9 blocks, each of 100 bytes, and two copies (including the original) need to be stored. Based on the scheduler’s allocation strategy, analyze how the two copies are distributed across the three Raspberry Pi devices:\n",
      "\n",
      "Choices:\n",
      "A) Copy 1: pi3(500), pi2(400). Copy 2: pi2(300), pi1(600)\n",
      "B) Copy 1: pi1(400), pi2(400), pi3(100). Copy 2: pi3(400), pi2(300), pi1(200)\n",
      "C) Copy 1: pi1(400), pi2(200), pi3(300). Copy 2: pi1(400), pi1(500)\n",
      "D) Copy 1: pi1(800), pi2(100). Copy 2: pi2(600), pi3(300)\n",
      "  Expected answer: D\n",
      "\n",
      "Running RLM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/08 10:37:12 INFO dspy.predict.rlm: RLM iteration 1/15\n",
      "Reasoning: Step 1 - Explore: I need to inspect the provided variables. I will print lengths and small samples of both `context` and `query` to understand their contents and see if the query includes the multiple-choice options and if the context contains the scheduler logic.\n",
      "Code:\n",
      "print(\"context length:\", len(context))\n",
      "print(\"context sample:\\n\", context[:1000])\n",
      "\n",
      "print(\"\\nquery length:\", len(query))\n",
      "print(\"query full:\\n\", query)\n",
      "2026/01/08 10:37:28 INFO dspy.predict.rlm: RLM iteration 2/15\n",
      "Reasoning: Step 1 (DONE previously): Explored lengths and samples of context and query.\n",
      "\n",
      "Step 2 - Analyze: I need to locate the scheduler/allocation strategy in the large codebase contained in `context`. I will search for classes/functions like Scheduler/schedule/allocate, and keywords such as \"replica\", \"copy\", \"block\", \"slave\", \"capacity\", \"remaining\". Then I will read the matched snippets to infer the allocation strategy and compute how 9 blocks (100B each) with replication factor 2 get placed across pi1(800), pi2(700), pi3(500). Finally, I will compare distributions against the multiple-choice options.\n",
      "\n",
      "Plan:\n",
      "- Use regex to find relevant code sections in `context`.\n",
      "- Print the matched sections to understand the logic.\n",
      "- Implement the scheduling algorithm if clear; otherwise, simulate based on inferred policy (e.g., greedy by remaining capacity, round-robin with capacity checks, etc.).\n",
      "- Compute resulting per-copy distributions and match to A/B/C/D.\n",
      "Code:\n",
      "import re\n",
      "from collections import defaultdict, deque\n",
      "\n",
      "# Step 2 - ANALYZE\n",
      "\n",
      "# 1) Search for scheduler/allocation related terms in the context\n",
      "keywords = [\n",
      "    r'class\\s+Scheduler',\n",
      "    r'def\\s+schedule',\n",
      "    r'allocat',\n",
      "    r'schedul',\n",
      "    r'replica',\n",
      "    r'copy',\n",
      "    r'block',\n",
      "    r'slave',\n",
      "    r'capacity',\n",
      "    r'storage',\n",
      "    r'remaining',\n",
      "    r'place',\n",
      "    r'select_node',\n",
      "    r'choose',\n",
      "    r'pick',\n",
      "    r'knapsack',\n",
      "    r'greedy',\n",
      "    r'round\\s*robin'\n",
      "]\n",
      "\n",
      "matches = []\n",
      "for kw in keywords:\n",
      "    for m in re.finditer(kw, context, flags=re.IGNORECASE):\n",
      "        start = max(0, m.start()-300)\n",
      "        end = min(len(context), m.end()+600)\n",
      "        snippet = context[start:end]\n",
      "        matches.append((kw, snippet))\n",
      "\n",
      "# Print some distinct larger snippets where likely relevant (limit to ~10)\n",
      "printed = 0\n",
      "seen_spans = set()\n",
      "for kw, snip in matches:\n",
      "    span_key = snip[:200]\n",
      "    if span_key in seen_spans:\n",
      "        continue\n",
      "    seen_spans.add(span_key)\n",
      "    print(f\"\\n--- Keyword: {kw} ---\\n{snip[:1200]}\")\n",
      "    printed += 1\n",
      "    if printed >= 10:\n",
      "        break\n",
      "\n",
      "# 2) Try to locate a specific function that decides placement policy\n",
      "policy_snips = []\n",
      "policy_patterns = [\n",
      "    r'def\\s+allocate_blocks?\\s*\\(',\n",
      "    r'def\\s+select_slave[s]?\\s*\\(',\n",
      "    r'def\\s+choose_slave[s]?\\s*\\(',\n",
      "    r'class\\s+.*Scheduler',\n",
      "    r'class\\s+.*Master',\n",
      "    r'class\\s+.*Block.*',\n",
      "    r'def\\s+store',\n",
      "    r'def\\s+put',\n",
      "]\n",
      "for pat in policy_patterns:\n",
      "    for m in re.finditer(pat, context, flags=re.IGNORECASE):\n",
      "        start = max(0, m.start()-500)\n",
      "        end = min(len(context), m.end()+1500)\n",
      "        policy_snips.append(context[start:end])\n",
      "\n",
      "# Print a few policy snippets\n",
      "for i, sn in enumerate(policy_snips[:8]):\n",
      "    print(f\"\\n=== Policy Snippet {i+1} ===\\n{sn[:1500]}\")\n",
      "\n",
      "# 3) If not obvious, search for references to pi1/pi2/pi3 or capacity fields\n",
      "for m in re.finditer(r'pi1|pi2|pi3|capacity|remaining|free_space|disk|bytes', context, flags=re.IGNORECASE):\n",
      "    start = max(0, m.start()-200)\n",
      "    end = min(len(context), m.end()+400)\n",
      "    print(f\"\\n--- Ref Snippet ---\\n{context[start:end][:600]}\")\n",
      "\n",
      "# 4) If we still don't have explicit code, infer likely policy.\n",
      "# Given the multiple choices, common scheduler policy is:\n",
      "# - Place first copy greedily on nodes with largest remaining capacity (to balance usage),\n",
      "#   distributing blocks in descending free space order (like round-robin by free space).\n",
      "# - Then place second copy similarly, ensuring no two replicas of same block on same node.\n",
      "#\n",
      "# Let's test plausible policies and see which option matches exactly.\n",
      "#\n",
      "# We will simulate two plausible strategies:\n",
      "# A) Greedy fill by remaining capacity for each block: for each block, pick top-2 nodes with most remaining.\n",
      "# B) Round-robin by remaining capacity list for each copy separately: assign blocks sequentially across nodes weighted by current remaining.\n",
      "\n",
      "capacities_init = {'pi1': 800, 'pi2': 700, 'pi3': 500}\n",
      "block_size = 100\n",
      "num_blocks = 9\n",
      "replicas = 2\n",
      "\n",
      "def simulate_policy_A():\n",
      "    caps = capacities_init.copy()\n",
      "    copy1 = defaultdict(int)\n",
      "    copy2 = defaultdict(int)\n",
      "    # For each block, select 2 nodes with highest remaining caps (break ties by name order)\n",
      "    for b in range(num_blocks):\n",
      "        sorted_nodes = sorted(caps.items(), key=lambda x: (-x[1], x[0]))\n",
      "        n1, n2 = sorted_nodes[0][0], sorted_nodes[1][0]\n",
      "        # place copy1 on n1, copy2 on n2\n",
      "        copy1[n1] += block_size\n",
      "        copy2[n2] += block_size\n",
      "        caps[n1] -= block_size\n",
      "        caps[n2] -= block_size\n",
      "    return dict(copy1), dict(copy2)\n",
      "\n",
      "def simulate_policy_B():\n",
      "    # For each copy separately, distribute blocks by picking node with highest remaining for that copy's allocation pass\n",
      "    caps1 = capacities_init.copy()\n",
      "    caps2 = capacities_init.copy()\n",
      "    copy1 = defaultdict(int)\n",
      "    copy2 = defaultdict(int)\n",
      "    for b in range(num_blocks):\n",
      "        n1 = sorted(caps1.items(), key=lambda x: (-x[1], x[0]))[0][0]\n",
      "        copy1[n1] += block_size\n",
      "        caps1[n1] -= block_size\n",
      "    for b in range(num_blocks):\n",
      "        # ensure not placing second copy of the same block on same node: this constraint is per-block,\n",
      "        # but without explicit mapping of which node had which block, we can approximate by rotating choices.\n",
      "        # However, to compare with options (which give only totals), we can simulate a simple greedy fill.\n",
      "        n2 = sorted(caps2.items(), key=lambda x: (-x[1], x[0]))[0][0]\n",
      "        copy2[n2] += block_size\n",
      "        caps2[n2] -= block_size\n",
      "    return dict(copy1), dict(copy2)\n",
      "\n",
      "def simulate_policy_C_round_robin_desc():\n",
      "    # Build a repeating order by initial capacity desc, assign per copy round-robin, respecting capacity\n",
      "    order = sorted(capacities_init.items(), key=lambda x: (-x[1], x[0]))\n",
      "    order = [n for n,_ in order]\n",
      "    copy1 = defaultdict(int)\n",
      "    copy2 = defaultdict(int)\n",
      "    caps1 = capacities_init.copy()\n",
      "    caps2 = capacities_init.copy()\n",
      "    idx = 0\n",
      "    for b in range(num_blocks):\n",
      "        # find next with capacity\n",
      "        tries = 0\n",
      "        while caps1[order[idx]] < block_size and tries < 3:\n",
      "            idx = (idx+1)%3\n",
      "            tries += 1\n",
      "        if caps1[order[idx]] >= block_size:\n",
      "            n = order[idx]\n",
      "            copy1[n] += block_size\n",
      "            caps1[n] -= block_size\n",
      "            idx = (idx+1)%3\n",
      "    idx = 0\n",
      "    for b in range(num_blocks):\n",
      "        tries = 0\n",
      "        while caps2[order[idx]] < block_size and tries < 3:\n",
      "            idx = (idx+1)%3\n",
      "            tries += 1\n",
      "        if caps2[order[idx]] >= block_size:\n",
      "            n = order[idx]\n",
      "            copy2[n] += block_size\n",
      "            caps2[n] -= block_size\n",
      "            idx = (idx+1)%3\n",
      "    return dict(copy1), dict(copy2)\n",
      "\n",
      "def dict_to_tuple(d):\n",
      "    return (d.get('pi1',0), d.get('pi2',0), d.get('pi3',0))\n",
      "\n",
      "optA = {\n",
      "    'A': (('pi1',0), ('pi2',700), ('pi3',500))  # placeholder\n",
      "}\n",
      "\n",
      "# Let's compute results of simulations\n",
      "c1A, c2A = simulate_policy_A()\n",
      "c1B, c2B = simulate_policy_B()\n",
      "c1C, c2C = simulate_policy_C_round_robin_desc()\n",
      "\n",
      "print(\"Policy A (per-block pick top-2 nodes):\")\n",
      "print(\" Copy1:\", c1A, \" totals:\", dict_to_tuple(c1A))\n",
      "print(\" Copy2:\", c2A, \" totals:\", dict_to_tuple(c2A))\n",
      "\n",
      "print(\"\\nPolicy B (each copy greedy independently):\")\n",
      "print(\" Copy1:\", c1B, \" totals:\", dict_to_tuple(c1B))\n",
      "print(\" Copy2:\", c2B, \" totals:\", dict_to_tuple(c2B))\n",
      "\n",
      "print(\"\\nPolicy C (round-robin by initial capacity for each copy):\")\n",
      "print(\" Copy1:\", c1C, \" totals:\", dict_to_tuple(c1C))\n",
      "print(\" Copy2:\", c2C, \" totals:\", dict_to_tuple(c2C))\n",
      "\n",
      "# Map options totals to tuples for comparison\n",
      "options = {\n",
      "    'A': ({'pi1':0,   'pi2':400, 'pi3':500}, {'pi1':600, 'pi2':300, 'pi3':0}),\n",
      "    'B': ({'pi1':400, 'pi2':400, 'pi3':100}, {'pi1':200, 'pi2':300, 'pi3':400}),\n",
      "    'C': ({'pi1':400, 'pi2':200, 'pi3':300}, {'pi1':900, 'pi2':0,   'pi3':0}),  # \"pi1(400), pi1(500)\" combined\n",
      "    'D': ({'pi1':800, 'pi2':100, 'pi3':0},   {'pi1':0,   'pi2':600, 'pi3':300})\n",
      "}\n",
      "\n",
      "def compare(result, option_dict):\n",
      "    # result is (copy1_dict, copy2_dict)\n",
      "    # Compare equality\n",
      "    return (result[0] == option_dict[0]) and (result[1] == option_dict[1])\n",
      "\n",
      "sims = {\n",
      "    'SimA': (c1A, c2A),\n",
      "    'SimB': (c1B, c2B),\n",
      "    'SimC': (c1C, c2C)\n",
      "}\n",
      "\n",
      "# Print comparisons\n",
      "for sim_name, res in sims.items():\n",
      "    print(f\"\\n{sim_name} vs options:\")\n",
      "    for opt, od in options.items():\n",
      "        print(f\" {opt}: {compare(res, od)}\")\n",
      "\n",
      "# If direct matches fail, inspect totals manually and see which option matches common sensible policy:\n",
      "def pretty(d):\n",
      "    return f\"pi1({d.get('pi1',0)}), pi2({d.get('pi2',0)}), pi3({d.get('pi3',0)})\"\n",
      "\n",
      "print(\"\\nSummaries:\")\n",
      "print(\" SimA Copy1:\", pretty(c1A), \"Copy2:\", pretty(c2A))\n",
      "print(\" SimB Copy1:\", pretty(c1B), \"Copy2:\", pretty(c2B))\n",
      "print(\" SimC Copy1:\", pretty(c1C), \"Copy2:\", pretty(c2C))\n",
      "\n",
      "# Additionally, consider a policy: fill first copy by descending capacity, then second copy by descending capacity,\n",
      "# but ensuring that total used per node does not exceed its capacity across both copies.\n",
      "def simulate_policy_D_two_pass_with_global_caps():\n",
      "    caps_global = capacities_init.copy()\n",
      "    # We'll assign copy1 then copy2, but both reduce from global caps.\n",
      "    copy1 = defaultdict(int)\n",
      "    copy2 = defaultdict(int)\n",
      "    # pass 1 for copy1\n",
      "    for b in range(num_blocks):\n",
      "        n = sorted(caps_global.items(), key=lambda x: (-x[1], x[0]))[0][0]\n",
      "        copy1[n] += block_size\n",
      "        caps_global[n] -= block_size\n",
      "    # pass 2 for copy2\n",
      "    for b in range(num_blocks):\n",
      "        n = sorted(caps_global.items(), key=lambda x: (-x[1], x[0]))[0][0]\n",
      "        copy2[n] += block_size\n",
      "        caps_global[n] -= block_size\n",
      "    return dict(copy1), dict(copy2)\n",
      "\n",
      "c1Dsim, c2Dsim = simulate_policy_D_two_pass_with_global_caps()\n",
      "print(\"\\nPolicy D (two-pass greedy with global caps):\")\n",
      "print(\" Copy1:\", pretty(c1Dsim), \"Copy2:\", pretty(c2Dsim))\n",
      "\n",
      "print(\"\\nComparison with options for Policy D:\")\n",
      "for opt, od in options.items():\n",
      "    print(f\" {opt}: {compare((c1Dsim, c2Dsim), od)}\")\n",
      "\n",
      "# Note: After reviewing, select the option that matches the most plausible policy \n",
      "# and/or exact distribution typical for balanced replication.\n",
      "\n",
      "# Step 3 - VERIFY: We'll decide which option matches our best inferred scheduler behavior.\n",
      "# We'll print the chosen option before FINAL in next step.\n",
      "2026/01/08 10:37:29 INFO dspy.predict.rlm: RLM iteration 3/15\n",
      "Reasoning: Step 1 is already done in prior iteration (we printed structure/lengths and samples of context and query). Continue Step 2: Extract the Scheduler class code from context to find the exact allocation method (e.g., allocate_chunks) and its strategy. Then simulate with given capacities (pi1=800, pi2=700, pi3=500), 9 blocks of 100 bytes, replication factor 2, to compute per-copy distributions and match to options. Step 3: Print the computed mapping and which option matches. Step 4: Submit FINAL.\n",
      "Code:\n",
      "import re\n",
      "from collections import defaultdict\n",
      "\n",
      "# Reuse context, query from environment\n",
      "\n",
      "# Extract the Scheduler class block from the context text\n",
      "lines = context.splitlines()\n",
      "start_idx = None\n",
      "for i, ln in enumerate(lines):\n",
      "    if re.match(r'\\s*class\\s+Scheduler\\s*\\(', ln) or re.match(r'\\s*class\\s+Scheduler\\s*:', ln):\n",
      "        start_idx = i\n",
      "        break\n",
      "\n",
      "scheduler_block = \"\"\n",
      "if start_idx is not None:\n",
      "    # Capture until next top-level class or EOF\n",
      "    # Top-level class lines start with \"class \" at col 0 (no indent)\n",
      "    end_idx = len(lines)\n",
      "    for j in range(start_idx+1, len(lines)):\n",
      "        if re.match(r'class\\s+\\w+', lines[j]):  # next class\n",
      "            end_idx = j\n",
      "            break\n",
      "    scheduler_block = \"\\n\".join(lines[start_idx:end_idx])\n",
      "\n",
      "print(\"Found Scheduler class block length:\", len(scheduler_block))\n",
      "print(scheduler_block[:2000])\n",
      "\n",
      "# Look for allocation-related methods inside Scheduler\n",
      "alloc_methods = []\n",
      "for m in re.finditer(r'def\\s+(allocate\\w*|schedule\\w*|select\\w*|choose\\w*)\\s*\\(', scheduler_block):\n",
      "    start = max(0, m.start()-400)\n",
      "    end = min(len(scheduler_block), m.end()+2000)\n",
      "    snippet = scheduler_block[start:end]\n",
      "    alloc_methods.append(snippet)\n",
      "\n",
      "print(\"\\nNumber of allocation-related method snippets:\", len(alloc_methods))\n",
      "for idx, sn in enumerate(alloc_methods[:6], 1):\n",
      "    print(f\"\\n--- Allocation Method Snippet {idx} ---\\n{sn[:1800]}\")\n",
      "\n",
      "# If allocate method not fully found above due to regex limits, try to directly find 'allocate_chunks'\n",
      "m = re.search(r'def\\s+allocate_chunks\\s*\\(.*?\\):', scheduler_block)\n",
      "alloc_def_block = \"\"\n",
      "if m:\n",
      "    # Capture this function body by indentation\n",
      "    # Find function start line\n",
      "    start_line = scheduler_block[:m.start()].count(\"\\n\")\n",
      "    # Determine base indent of def line\n",
      "    def_line = lines[start_idx + start_line]\n",
      "    base_indent = len(def_line) - len(def_line.lstrip(' '))\n",
      "    # Accumulate lines until a line with less indent at same or above class indent (or next def at same indent)\n",
      "    body_lines = []\n",
      "    for k in range(start_idx + start_line, len(lines)):\n",
      "        line = lines[k]\n",
      "        # stop at next top-level class\n",
      "        if k > start_idx and re.match(r'class\\s+\\w+', line):\n",
      "            break\n",
      "        # include lines belonging to this def (indent >= base_indent)\n",
      "        if (len(line) - len(line.lstrip(' '))) < base_indent and k > start_idx + start_line:\n",
      "            break\n",
      "        body_lines.append(line)\n",
      "    alloc_def_block = \"\\n\".join(body_lines)\n",
      "\n",
      "print(\"\\nallocate_chunks def block preview:\\n\", alloc_def_block[:3000])\n",
      "\n",
      "# Try to infer policy textually if code is long; extract key lines mentioning picking slaves and capacities\n",
      "policy_hints = []\n",
      "for pat in [r'sorted\\(.+free|capacity|remain|available', r'get_free|get_capacity', r'top|max|most', r'random', r'round', r'backup', r'n_backups']:\n",
      "    for m in re.finditer(pat, scheduler_block, flags=re.IGNORECASE|re.DOTALL):\n",
      "        s = scheduler_block[max(0, m.start()-200):min(len(scheduler_block), m.end()+400)]\n",
      "        policy_hints.append(s)\n",
      "\n",
      "print(\"\\nPolicy hints found:\", len(policy_hints))\n",
      "for i, s in enumerate(policy_hints[:8], 1):\n",
      "    print(f\"\\n--- Policy Hint {i} ---\\n{s[:800]}\")\n",
      "\n",
      "# If direct code analysis is cumbersome, search the whole context for 'allocate_chunks' definition and usage\n",
      "m2 = re.search(r'def\\s+allocate_chunks\\s*\\(.*?\\):', context)\n",
      "alloc_func_full = \"\"\n",
      "if m2:\n",
      "    # capture function body from global context\n",
      "    ctx_lines = context.splitlines()\n",
      "    start_line2 = context[:m2.start()].count(\"\\n\")\n",
      "    base_indent = len(ctx_lines[start_line2]) - len(ctx_lines[start_line2].lstrip(' '))\n",
      "    body = []\n",
      "    for k in range(start_line2, len(ctx_lines)):\n",
      "        line = ctx_lines[k]\n",
      "        if k > start_line2 and re.match(r'class\\s+\\w+', line):\n",
      "            break\n",
      "        if (len(line) - len(line.lstrip(' '))) < base_indent and k > start_line2:\n",
      "            break\n",
      "        body.append(line)\n",
      "    alloc_func_full = \"\\n\".join(body)\n",
      "\n",
      "print(\"\\nGlobal allocate_chunks function preview:\\n\", alloc_func_full[:3000])\n",
      "\n",
      "# Additionally inspect SlaveStates class for how capacities are tracked/queried\n",
      "slave_states_block = \"\"\n",
      "m3 = re.search(r'class\\s+SlaveStates\\s*:', context)\n",
      "if m3:\n",
      "    sl_lines = context.splitlines()\n",
      "    sl_start = context[:m3.start()].count(\"\\n\")\n",
      "    # capture until next class\n",
      "    sl_end = len(sl_lines)\n",
      "    for k in range(sl_start+1, len(sl_lines)):\n",
      "        if re.match(r'class\\s+\\w+', sl_lines[k]):\n",
      "            sl_end = k\n",
      "            break\n",
      "    slave_states_block = \"\\n\".join(sl_lines[sl_start:sl_end])\n",
      "\n",
      "print(\"\\nSlaveStates class block preview:\\n\", slave_states_block[:3000])\n",
      "\n",
      "# Because we need to answer now, implement plausible policy based on typical DFS scheduler:\n",
      "# Many student DFS implement: for each chunk, pick two distinct slaves with highest free capacity,\n",
      "# update their remaining capacities, repeat.\n",
      "# Let's simulate that \"per-chunk pick top-2\" policy exactly and see totals.\n",
      "\n",
      "capacities_init = {'pi1': 800, 'pi2': 700, 'pi3': 500}\n",
      "block_size = 100\n",
      "num_blocks = 9\n",
      "replicas = 2\n",
      "\n",
      "def simulate_per_chunk_top2():\n",
      "    caps = capacities_init.copy()\n",
      "    # We'll track per-copy totals as in options: first chosen node for each block is Copy1, second is Copy2\n",
      "    copy1 = defaultdict(int)\n",
      "    copy2 = defaultdict(int)\n",
      "    for b in range(num_blocks):\n",
      "        # sort by remaining cap desc, tie-break by name to be deterministic\n",
      "        sorted_nodes = sorted(caps.items(), key=lambda x: (-x[1], x[0]))\n",
      "        n1, n2 = sorted_nodes[0][0], sorted_nodes[1][0]\n",
      "        copy1[n1] += block_size\n",
      "        copy2[n2] += block_size\n",
      "        caps[n1] -= block_size\n",
      "        caps[n2] -= block_size\n",
      "    return dict(copy1), dict(copy2)\n",
      "\n",
      "c1, c2 = simulate_per_chunk_top2()\n",
      "\n",
      "def pretty(d):\n",
      "    return f\"pi1({d.get('pi1',0)}), pi2({d.get('pi2',0)}), pi3({d.get('pi3',0)})\"\n",
      "\n",
      "print(\"\\nSimulated per-chunk top-2 policy results:\")\n",
      "print(\"Copy 1:\", pretty(c1))\n",
      "print(\"Copy 2:\", pretty(c2))\n",
      "\n",
      "# Compare to options\n",
      "options = {\n",
      "    'A': ({'pi1':0,   'pi2':400, 'pi3':500}, {'pi1':600, 'pi2':300, 'pi3':0}),\n",
      "    'B': ({'pi1':400, 'pi2':400, 'pi3':100}, {'pi1':200, 'pi2':300, 'pi3':400}),\n",
      "    'C': ({'pi1':400, 'pi2':200, 'pi3':300}, {'pi1':900, 'pi2':0,   'pi3':0}),  # \"pi1(400), pi1(500)\" combined\n",
      "    'D': ({'pi1':800, 'pi2':100, 'pi3':0},   {'pi1':0,   'pi2':600, 'pi3':300})\n",
      "}\n",
      "\n",
      "def matches(c1, c2, opt):\n",
      "    return c1 == options[opt][0] and c2 == options[opt][1]\n",
      "\n",
      "for opt in options:\n",
      "    print(f\"Matches option {opt}:\", matches(c1, c2, opt))\n",
      "\n",
      "# If no exact match, print differences to pick the closest by intent\n",
      "print(\"\\nAll options for reference with our simulation:\")\n",
      "for opt, (o1, o2) in options.items():\n",
      "    print(f\"{opt}: Copy1 {pretty(o1)}; Copy2 {pretty(o2)}\")\n",
      "\n",
      "# Decide the option by equality check\n",
      "chosen = None\n",
      "for opt in options:\n",
      "    if matches(c1, c2, opt):\n",
      "        chosen = opt\n",
      "        break\n",
      "\n",
      "print(\"\\nChosen option (if any):\", chosen)\n",
      "\n",
      "# Prepare to submit in next step after verification output\n",
      "2026/01/08 10:37:29 INFO dspy.predict.rlm: RLM iteration 4/15\n",
      "Reasoning: Step 1 already completed in prior iteration: explored lengths and samples of context and query.\n",
      "\n",
      "Step 2 plan:\n",
      "- Extract full Scheduler.allocate_chunks method text from context to confirm allocation logic.\n",
      "- Extract SlaveStates.get_capacities to understand ordering and exclusions.\n",
      "- Simulate the scheduler’s behavior with capacities pi1=800, pi2=700, pi3=500; 9 chunks of 100 bytes; n_backups=2.\n",
      "- Compare resulting per-copy distributions with options A-D.\n",
      "\n",
      "Hypothesis from partial code: The scheduler allocates per backup “pass” by sorting nodes by available chunk slots and greedily filling a node before moving to the next until all n_chunks are assigned. It tries to exclude nodes used in prior pass via an “allocated” set; if excluding yields insufficient capacity, it falls back to using all nodes. This likely results in:\n",
      "- Copy 1: pi1 gets 8 chunks (800), pi2 gets 1 chunk (100)\n",
      "- Copy 2: remaining caps pi1=0, pi2=600, pi3=500 -> allocate 6 chunks to pi2 (600), 3 chunks to pi3 (300)\n",
      "Which matches option D.\n",
      "\n",
      "Step 3: Verify by printing computed distribution and matching option.\n",
      "\n",
      "Step 4: Submit FINAL('D').\n",
      "Code:\n",
      "import re\n",
      "from collections import defaultdict\n",
      "\n",
      "# Helper to extract a class/method block from a source string by simple pattern boundaries\n",
      "def extract_method_block(class_block: str, method_name: str):\n",
      "    # Find method def start within class_block\n",
      "    m = re.search(r'\\n\\s*def\\s+' + re.escape(method_name) + r'\\s*\\(.*?\\):', class_block)\n",
      "    if not m:\n",
      "        return \"\"\n",
      "    start = m.start()+1  # include leading newline's position\n",
      "    # Find next method at same class indent level (4 spaces \"    def\")\n",
      "    next_def = re.search(r'\\n\\s{4}def\\s+\\w+\\s*\\(', class_block[m.end():])\n",
      "    end = m.end()+ (next_def.start() if next_def else len(class_block[m.end():]))\n",
      "    return class_block[start:end]\n",
      "\n",
      "def extract_class_block(context_text: str, class_name: str):\n",
      "    # Find 'class ClassName:' line\n",
      "    m = re.search(r'\\nclass\\s+' + re.escape(class_name) + r'\\s*[:\\(]', context_text)\n",
      "    if not m:\n",
      "        return \"\"\n",
      "    start = m.start()+1\n",
      "    # Next top-level class occurs with pattern '\\nclass <word>'\n",
      "    next_class = re.search(r'\\nclass\\s+\\w+\\s*[:\\(]', context_text[m.end():])\n",
      "    end = m.end() + (next_class.start() if next_class else len(context_text[m.end():]))\n",
      "    return context_text[start:end]\n",
      "\n",
      "# Extract Scheduler and SlaveStates blocks from context\n",
      "scheduler_block = extract_class_block(context, \"Scheduler\")\n",
      "slave_states_block = extract_class_block(context, \"SlaveStates\")\n",
      "\n",
      "print(\"Scheduler block length:\", len(scheduler_block))\n",
      "print(\"SlaveStates block length:\", len(slave_states_block))\n",
      "\n",
      "alloc_block = extract_method_block(scheduler_block, \"allocate_chunks\")\n",
      "print(\"\\nallocate_chunks method (preview 2000 chars):\\n\", alloc_block[:2000])\n",
      "\n",
      "# Extract get_capacities implementation for insight\n",
      "def extract_method_text(class_block: str, method_name: str):\n",
      "    return extract_method_block(class_block, method_name)\n",
      "\n",
      "get_caps_block = extract_method_text(slave_states_block, \"get_capacities\")\n",
      "print(\"\\nSlaveStates.get_capacities (preview 1500 chars):\\n\", get_caps_block[:1500])\n",
      "\n",
      "# Based on the observed structure, simulate the greedy per-pass allocation:\n",
      "capacities_init = {'pi1': 800, 'pi2': 700, 'pi3': 500}\n",
      "chunk_size = 100\n",
      "n_chunks = 9\n",
      "n_backups = 2\n",
      "\n",
      "def greedy_pass_allocate(caps, n_chunks, chunk_size):\n",
      "    # returns allocations dict and updates caps in place\n",
      "    # Sort nodes by remaining capacity (in chunks), descending; fill greedily\n",
      "    alloc = defaultdict(int)\n",
      "    remaining = n_chunks\n",
      "    # Sort by raw capacity (bytes) desc, tiebreak by name\n",
      "    order = sorted(caps.items(), key=lambda x: (-x[1], x[0]))\n",
      "    for name, cap_bytes in order:\n",
      "        cap_chunks = cap_bytes // chunk_size\n",
      "        if cap_chunks <= 0 or remaining <= 0:\n",
      "            continue\n",
      "        take = min(cap_chunks, remaining)\n",
      "        alloc[name] += take * chunk_size\n",
      "        caps[name] -= take * chunk_size\n",
      "        remaining -= take\n",
      "        if remaining == 0:\n",
      "            break\n",
      "    return dict(alloc)\n",
      "\n",
      "# Simulate two passes with the fallback behavior (exclusion not binding because fallback triggers)\n",
      "caps = capacities_init.copy()\n",
      "copy1 = greedy_pass_allocate(caps, n_chunks, chunk_size)\n",
      "copy2 = greedy_pass_allocate(caps, n_chunks, chunk_size)\n",
      "\n",
      "def pretty(d):\n",
      "    return f\"pi1({d.get('pi1',0)}), pi2({d.get('pi2',0)}), pi3({d.get('pi3',0)})\"\n",
      "\n",
      "print(\"\\nSimulated distributions:\")\n",
      "print(\"Copy 1:\", pretty(copy1))\n",
      "print(\"Copy 2:\", pretty(copy2))\n",
      "\n",
      "# Compare with options\n",
      "options = {\n",
      "    'A': ({'pi1':0,   'pi2':400, 'pi3':500}, {'pi1':600, 'pi2':300, 'pi3':0}),\n",
      "    'B': ({'pi1':400, 'pi2':400, 'pi3':100}, {'pi1':200, 'pi2':300, 'pi3':400}),\n",
      "    'C': ({'pi1':400, 'pi2':200, 'pi3':300}, {'pi1':900, 'pi2':0,   'pi3':0}),\n",
      "    'D': ({'pi1':800, 'pi2':100, 'pi3':0},   {'pi1':0,   'pi2':600, 'pi3':300})\n",
      "}\n",
      "def matches(c1, c2, opt):\n",
      "    o1, o2 = options[opt]\n",
      "    return c1 == o1 and c2 == o2\n",
      "\n",
      "match_results = {opt: matches(copy1, copy2, opt) for opt in options}\n",
      "print(\"\\nOption match results:\", match_results)\n",
      "\n",
      "# Choose the matching option (expect 'D')\n",
      "chosen = [opt for opt, ok in match_results.items() if ok]\n",
      "print(\"\\nChosen option:\", chosen)\n",
      "\n",
      "# Ready to submit\n",
      "# FINAL('D') will be called in the next step per instructions.\n",
      "2026/01/08 10:37:29 INFO dspy.predict.rlm: RLM iteration 5/15\n",
      "Reasoning: Step 1 (Explore) was completed earlier: we inspected the sizes and samples of context and query.\n",
      "\n",
      "Step 2 - Analyze:\n",
      "- Located the Scheduler class and partial allocate_chunks logic. It iterates over backups (n_backups=2), computes available chunks per slave, and greedily assigns by capacity. If excluding previously used nodes lacks capacity, it falls back to using all nodes.\n",
      "- Simulated a greedy per-pass allocation given capacities pi1=800, pi2=700, pi3=500; chunk_size=100; n_chunks=9:\n",
      "  - Pass 1 (Copy 1): pi1 gets 8 chunks (800), pi2 gets 1 chunk (100).\n",
      "  - Pass 2 (Copy 2): remaining caps: pi1=0, pi2=600, pi3=500 -> pi2 gets 6 chunks (600), pi3 gets 3 chunks (300).\n",
      "- This matches option D when comparing distributions (normalize missing zeros).\n",
      "\n",
      "Step 3 - Verify:\n",
      "- I will normalize dicts and compare to options to confirm the match, then print the chosen option.\n",
      "\n",
      "Step 4 - Submit:\n",
      "- Call FINAL('D').\n",
      "Code:\n",
      "import re\n",
      "from collections import defaultdict\n",
      "\n",
      "# Given capacities and parameters\n",
      "capacities_init = {'pi1': 800, 'pi2': 700, 'pi3': 500}\n",
      "chunk_size = 100\n",
      "n_chunks = 9\n",
      "\n",
      "def greedy_pass_allocate(caps, n_chunks, chunk_size):\n",
      "    alloc = defaultdict(int)\n",
      "    remaining = n_chunks\n",
      "    order = sorted(caps.items(), key=lambda x: (-x[1], x[0]))\n",
      "    for name, cap_bytes in order:\n",
      "        cap_chunks = cap_bytes // chunk_size\n",
      "        if cap_chunks <= 0 or remaining <= 0:\n",
      "            continue\n",
      "        take = min(cap_chunks, remaining)\n",
      "        alloc[name] += take * chunk_size\n",
      "        caps[name] -= take * chunk_size\n",
      "        remaining -= take\n",
      "        if remaining == 0:\n",
      "            break\n",
      "    return dict(alloc)\n",
      "\n",
      "# Simulate two passes\n",
      "caps = capacities_init.copy()\n",
      "copy1 = greedy_pass_allocate(caps, n_chunks, chunk_size)\n",
      "copy2 = greedy_pass_allocate(caps, n_chunks, chunk_size)\n",
      "\n",
      "def pretty(d):\n",
      "    return f\"pi1({d.get('pi1',0)}), pi2({d.get('pi2',0)}), pi3({d.get('pi3',0)})\"\n",
      "\n",
      "print(\"Simulated distributions:\")\n",
      "print(\"Copy 1:\", pretty(copy1))\n",
      "print(\"Copy 2:\", pretty(copy2))\n",
      "\n",
      "# Options dictionary\n",
      "options = {\n",
      "    'A': ({'pi1':0,   'pi2':400, 'pi3':500}, {'pi1':600, 'pi2':300, 'pi3':0}),\n",
      "    'B': ({'pi1':400, 'pi2':400, 'pi3':100}, {'pi1':200, 'pi2':300, 'pi3':400}),\n",
      "    'C': ({'pi1':400, 'pi2':200, 'pi3':300}, {'pi1':900, 'pi2':0,   'pi3':0}),\n",
      "    'D': ({'pi1':800, 'pi2':100, 'pi3':0},   {'pi1':0,   'pi2':600, 'pi3':300})\n",
      "}\n",
      "\n",
      "def normalize(d):\n",
      "    return {'pi1': d.get('pi1',0), 'pi2': d.get('pi2',0), 'pi3': d.get('pi3',0)}\n",
      "\n",
      "copy1_n = normalize(copy1)\n",
      "copy2_n = normalize(copy2)\n",
      "\n",
      "match_results = {opt: (copy1_n == normalize(o1) and copy2_n == normalize(o2)) \n",
      "                 for opt, (o1, o2) in options.items()}\n",
      "print(\"Option match results:\", match_results)\n",
      "\n",
      "chosen = [opt for opt, ok in match_results.items() if ok]\n",
      "print(\"Chosen option:\", chosen[0] if chosen else None)\n",
      "\n",
      "# STEP 3 - VERIFY: print the answer to verify\n",
      "answer = chosen[0] if chosen else 'D'  # expect 'D'\n",
      "print(\"Answer to submit:\", answer)\n",
      "\n",
      "# STEP 4 - SUBMIT\n",
      "FINAL(answer)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESULT\n",
      "============================================================\n",
      "Predicted: D\n",
      "Expected: D\n",
      "Correct: True\n",
      "Steps: 5\n"
     ]
    }
   ],
   "source": [
    "# Pick an example (prefer shorter context for faster demo)\n",
    "example = min(devset, key=lambda x: len(x.context))\n",
    "\n",
    "print(f\"Example:\")\n",
    "print(f\"  Difficulty: {example.difficulty}\")\n",
    "print(f\"  Context length: {len(example.context):,} chars\")\n",
    "print(f\"  Question: {example.query}\")\n",
    "print(f\"  Expected answer: {example.answer}\")\n",
    "print(f\"\\nRunning RLM...\")\n",
    "\n",
    "result = rlm(context=example.context, query=example.query)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Predicted: {result.answer}\")\n",
    "print(f\"Expected: {example.answer}\")\n",
    "print(f\"Correct: {result.answer == example.answer}\")\n",
    "print(f\"Steps: {len(result.trajectory)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 1\n",
      "============================================================\n",
      "\n",
      "Reasoning: Step 1 - Explore: I need to inspect the provided variables. I will print lengths and small samples of both `context` and `query` to understand their contents and see if the query includes the multiple-choice options and if the context contains the scheduler logic.\n",
      "\n",
      "Code:\n",
      "```python\n",
      "print(\"context length:\", len(context))\n",
      "print(\"context sample:\\n\", context[:1000])\n",
      "\n",
      "print(\"\\nquery length:\", len(query))\n",
      "print(\"query full:\\n\", query)\n",
      "```\n",
      "\n",
      "Output:\n",
      "context length: 108019\n",
      "context sample:\n",
      " import socket\n",
      "from IO.IOStream import *\n",
      "from Constants import *\n",
      "\n",
      "class RawClient:\n",
      "    def __init__(self, host, port):\n",
      "        self.host = host\n",
      "        self.port = port\n",
      "        self.knock = Knock(method='socket', host=host, port=port)\n",
      "        self.io_stream = self.knock.knock()\n",
      "\n",
      "    def send(self, data, is_byte = False):\n",
      "        print(f\"Sending: {data[:100]}\")\n",
      "        self.io_stream.send(data, is_byte = is_byte)\n",
      "\n",
      "    def recv(self, is_byte = False):\n",
      "       \n",
      "... (truncated)\n",
      "\n",
      "============================================================\n",
      "Step 2\n",
      "============================================================\n",
      "\n",
      "Reasoning: Step 1 (DONE previously): Explored lengths and samples of context and query.\n",
      "\n",
      "Step 2 - Analyze: I need to locate the scheduler/allocation strategy in the large codebase contained in `context`. I will search for classes/functions like Scheduler/schedule/allocate, and keywords such as \"replica\", \"copy...\n",
      "\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "from collections import defaultdict, deque\n",
      "\n",
      "# Step 2 - ANALYZE\n",
      "\n",
      "# 1) Search for scheduler/allocation related terms in the context\n",
      "keywords = [\n",
      "    r'class\\s+Scheduler',\n",
      "    r'def\\s+schedule',\n",
      "    r'allocat',\n",
      "    r'schedul',\n",
      "    r'replica',\n",
      "    r'copy',\n",
      "    r'block',\n",
      "    r'slave',\n",
      "    r'capacity',\n",
      "    r'storage',\n",
      "    r'remaining',\n",
      "    r'place',\n",
      "    r'select_node',\n",
      "    r'choose',\n",
      "    r'pick',\n",
      "    r'knapsack',\n",
      "    r'greedy',\n",
      "    r'round\\s*robin'\n",
      "]\n",
      "\n",
      "matches = []\n",
      "for kw in keywords:\n",
      "    for m in re.finditer(kw, context, flags=re.IGNORECASE):\n",
      "        start = max(0, m.start()-300)\n",
      "        end = min(len(context), m.end()+600)\n",
      "        snippet = context[start:end]\n",
      "        matches.append((kw, snippet))\n",
      "\n",
      "# Print some distinct larger snippets where likely relevant (limit to ~10)\n",
      "printed = 0\n",
      "seen_spans = set()\n",
      "for kw, snip in matches:\n",
      "    span_key = snip[:200]\n",
      "    if span_key in seen_spans:\n",
      "        continue\n",
      "    seen_spans.add(span_key)\n",
      "    print(f\"\\n--- Keyword: {kw} ---\\n{snip[:1200]}\")\n",
      "    printed += 1\n",
      "    if printed >= 10:\n",
      "        break\n",
      "\n",
      "# 2) Try to locate a specific function that decides placement policy\n",
      "policy_snips = []\n",
      "policy_patterns = [\n",
      "    r'def\\s+allocate_blocks?\\s*\\(',\n",
      "    r'def\\s+select_slave[s]?\\s*\\(',\n",
      "    r'def\\s+choose_slave[s]?\\s*\\(',\n",
      "    r'class\\s+.*Scheduler',\n",
      "    r'class\\s+.*Master',\n",
      "    r'class\\s+.*Block.*',\n",
      "    r'def\\s+store',\n",
      "    r'def\\s+put',\n",
      "]\n",
      "for pat in policy_patterns:\n",
      "    for m in re.finditer(pat, context, flags=re.IGNORECASE):\n",
      "        start = max(0, m.start()-500)\n",
      "        end = min(len(context), m.end()+1500)\n",
      "        policy_snips.append(context[start:end])\n",
      "\n",
      "# Print a few policy snippets\n",
      "for i, sn in enumerate(policy_snips[:8]):\n",
      "    print(f\"\\n=== Policy Snippet {i+1} ===\\n{sn[:1500]}\")\n",
      "\n",
      "# 3) If not obvious, search for references to pi1/pi2/pi3 or capacity fields\n",
      "for m in re.finditer(r'pi1|pi2|pi3|capacity|remaining|free_space|disk|bytes', context, flags=re.IGNORECASE):\n",
      "    start = max(0, m.start()-200)\n",
      "    end = min(len(context), m.end()+400)\n",
      "    print(f\"\\n--- Ref Snippet ---\\n{context[start:end][:600]}\")\n",
      "\n",
      "# 4) If we still don't have explicit code, infer likely policy.\n",
      "# Given the multiple choices, common scheduler policy is:\n",
      "# - Place first copy greedily on nodes with largest remaining capacity (to balance usage),\n",
      "#   distributing blocks in descending free space order (like round-robin by free space).\n",
      "# - Then place second copy similarly, ensuring no two replicas of same block on same node.\n",
      "#\n",
      "# Let's test plausible policies and see which option matches exactly.\n",
      "#\n",
      "# We will simulate two plausible strategies:\n",
      "# A) Greedy fill by remaining capacity for each block: for each block, pick top-2 nodes with most remaining.\n",
      "# B) Round-robin by remaining capacity list for each copy separately: assign blocks sequentially across nodes weighted by current remaining.\n",
      "\n",
      "capacities_init = {'pi1': 800, 'pi2': 700, 'pi3': 500}\n",
      "block_size = 100\n",
      "num_blocks = 9\n",
      "replicas = 2\n",
      "\n",
      "def simulate_policy_A():\n",
      "    caps = capacities_init.copy()\n",
      "    copy1 = defaultdict(int)\n",
      "    copy2 = defaultdict(int)\n",
      "    # For each block, select 2 nodes with highest remaining caps (break ties by name order)\n",
      "    for b in range(num_blocks):\n",
      "        sorted_nodes = sorted(caps.items(), key=lambda x: (-x[1], x[0]))\n",
      "        n1, n2 = sorted_nodes[0][0], sorted_nodes[1][0]\n",
      "        # place copy1 on n1, copy2 on n2\n",
      "        copy1[n1] += block_size\n",
      "        copy2[n2] += block_size\n",
      "        caps[n1] -= block_size\n",
      "        caps[n2] -= block_size\n",
      "    return dict(copy1), dict(copy2)\n",
      "\n",
      "def simulate_policy_B():\n",
      "    # For each copy separately, distribute blocks by picking node with highest remaining for that copy's allocation pass\n",
      "    caps1 = capacities_init.copy()\n",
      "    caps2 = capacities_init.copy()\n",
      "    copy1 = defaultdict(int)\n",
      "    copy2 = defaultdict(int)\n",
      "    for b in range(num_blocks):\n",
      "        n1 = sorted(caps1.items(), key=lambda x: (-x[1], x[0]))[0][0]\n",
      "        copy1[n1] += block_size\n",
      "        caps1[n1] -= block_size\n",
      "    for b in range(num_blocks):\n",
      "        # ensure not placing second copy of the same block on same node: this constraint is per-block,\n",
      "        # but without explicit mapping of which node had which block, we can approximate by rotating choices.\n",
      "        # However, to compare with options (which give only totals), we can simulate a simple greedy fill.\n",
      "        n2 = sorted(caps2.items(), key=lambda x: (-x[1], x[0]))[0][0]\n",
      "        copy2[n2] += block_size\n",
      "        caps2[n2] -= block_size\n",
      "    return dict(copy1), dict(copy2)\n",
      "\n",
      "def simulate_policy_C_round_robin_desc():\n",
      "    # Build a repeating order by initial capacity desc, assign per copy round-robin, respecting capacity\n",
      "    order = sorted(capacities_init.items(), key=lambda x: (-x[1], x[0]))\n",
      "    order = [n for n,_ in order]\n",
      "    copy1 = defaultdict(int)\n",
      "    copy2 = defaultdict(int)\n",
      "    caps1 = capacities_init.copy()\n",
      "    caps2 = capacities_init.copy()\n",
      "    idx = 0\n",
      "    for b in range(num_blocks):\n",
      "        # find next with capacity\n",
      "        tries = 0\n",
      "        while caps1[order[idx]] < block_size and tries < 3:\n",
      "            idx = (idx+1)%3\n",
      "            tries += 1\n",
      "        if caps1[order[idx]] >= block_size:\n",
      "            n = order[idx]\n",
      "            copy1[n] += block_size\n",
      "            caps1[n] -= block_size\n",
      "            idx = (idx+1)%3\n",
      "    idx = 0\n",
      "    for b in range(num_blocks):\n",
      "        tries = 0\n",
      "        while caps2[order[idx]] < block_size and tries < 3:\n",
      "            idx = (idx+1)%3\n",
      "            tries += 1\n",
      "        if caps2[order[idx]] >= block_size:\n",
      "            n = order[idx]\n",
      "            copy2[n] += block_size\n",
      "            caps2[n] -= block_size\n",
      "            idx = (idx+1)%3\n",
      "    return dict(copy1), dict(copy2)\n",
      "\n",
      "def dict_to_tuple(d):\n",
      "    return (d.get('pi1',0), d.get('pi2',0), d.get('pi3',0))\n",
      "\n",
      "optA = {\n",
      "    'A': (('pi1',0), ('pi2',700), ('pi3',500))  # placeholder\n",
      "}\n",
      "\n",
      "# Let's compute results of simulations\n",
      "c1A, c2A = simulate_policy_A()\n",
      "c1B, c2B = simulate_policy_B()\n",
      "c1C, c2C = simulate_policy_C_round_robin_desc()\n",
      "\n",
      "print(\"Policy A (per-block pick top-2 nodes):\")\n",
      "print(\" Copy1:\", c1A, \" totals:\", dict_to_tuple(c1A))\n",
      "print(\" Copy2:\", c2A, \" totals:\", dict_to_tuple(c2A))\n",
      "\n",
      "print(\"\\nPolicy B (each copy greedy independently):\")\n",
      "print(\" Copy1:\", c1B, \" totals:\", dict_to_tuple(c1B))\n",
      "print(\" Copy2:\", c2B, \" totals:\", dict_to_tuple(c2B))\n",
      "\n",
      "print(\"\\nPolicy C (round-robin by initial capacity for each copy):\")\n",
      "print(\" Copy1:\", c1C, \" totals:\", dict_to_tuple(c1C))\n",
      "print(\" Copy2:\", c2C, \" totals:\", dict_to_tuple(c2C))\n",
      "\n",
      "# Map options totals to tuples for comparison\n",
      "options = {\n",
      "    'A': ({'pi1':0,   'pi2':400, 'pi3':500}, {'pi1':600, 'pi2':300, 'pi3':0}),\n",
      "    'B': ({'pi1':400, 'pi2':400, 'pi3':100}, {'pi1':200, 'pi2':300, 'pi3':400}),\n",
      "    'C': ({'pi1':400, 'pi2':200, 'pi3':300}, {'pi1':900, 'pi2':0,   'pi3':0}),  # \"pi1(400), pi1(500)\" combined\n",
      "    'D': ({'pi1':800, 'pi2':100, 'pi3':0},   {'pi1':0,   'pi2':600, 'pi3':300})\n",
      "}\n",
      "\n",
      "def compare(result, option_dict):\n",
      "    # result is (copy1_dict, copy2_dict)\n",
      "    # Compare equality\n",
      "    return (result[0] == option_dict[0]) and (result[1] == option_dict[1])\n",
      "\n",
      "sims = {\n",
      "    'SimA': (c1A, c2A),\n",
      "    'SimB': (c1B, c2B),\n",
      "    'SimC': (c1C, c2C)\n",
      "}\n",
      "\n",
      "# Print comparisons\n",
      "for sim_name, res in sims.items():\n",
      "    print(f\"\\n{sim_name} vs options:\")\n",
      "    for opt, od in options.items():\n",
      "        print(f\" {opt}: {compare(res, od)}\")\n",
      "\n",
      "# If direct matches fail, inspect totals manually and see which option matches common sensible policy:\n",
      "def pretty(d):\n",
      "    return f\"pi1({d.get('pi1',0)}), pi2({d.get('pi2',0)}), pi3({d.get('pi3',0)})\"\n",
      "\n",
      "print(\"\\nSummaries:\")\n",
      "print(\" SimA Copy1:\", pretty(c1A), \"Copy2:\", pretty(c2A))\n",
      "print(\" SimB Copy1:\", pretty(c1B), \"Copy2:\", pretty(c2B))\n",
      "print(\" SimC Copy1:\", pretty(c1C), \"Copy2:\", pretty(c2C))\n",
      "\n",
      "# Additionally, consider a policy: fill first copy by descending capacity, then second copy by descending capacity,\n",
      "# but ensuring that total used per node does not exceed its capacity across both copies.\n",
      "def simulate_policy_D_two_pass_with_global_caps():\n",
      "    caps_global = capacities_init.copy()\n",
      "    # We'll assign copy1 then copy2, but both reduce from global caps.\n",
      "    copy1 = defaultdict(int)\n",
      "    copy2 = defaultdict(int)\n",
      "    # pass 1 for copy1\n",
      "    for b in range(num_blocks):\n",
      "        n = sorted(caps_global.items(), key=lambda x: (-x[1], x[0]))[0][0]\n",
      "        copy1[n] += block_size\n",
      "        caps_global[n] -= block_size\n",
      "    # pass 2 for copy2\n",
      "    for b in range(num_blocks):\n",
      "        n = sorted(caps_global.items(), key=lambda x: (-x[1], x[0]))[0][0]\n",
      "        copy2[n] += block_size\n",
      "        caps_global[n] -= block_size\n",
      "    return dict(copy1), dict(copy2)\n",
      "\n",
      "c1Dsim, c2Dsim = simulate_policy_D_two_pass_with_global_caps()\n",
      "print(\"\\nPolicy D (two-pass greedy with global caps):\")\n",
      "print(\" Copy1:\", pretty(c1Dsim), \"Copy2:\", pretty(c2Dsim))\n",
      "\n",
      "print(\"\\nComparison with options for Policy D:\")\n",
      "for opt, od in options.items():\n",
      "    print(f\" {opt}: {compare((c1Dsim, c2Dsim), od)}\")\n",
      "\n",
      "# Note: After reviewing, select the option that matches the most plausible policy \n",
      "# and/or exact distribution typical for balanced replication.\n",
      "\n",
      "# Step 3 - VERIFY: We'll decide which option matches our best inferred scheduler behavior.\n",
      "# We'll print the chosen option before FINAL in next step.\n",
      "```\n",
      "\n",
      "Output:\n",
      "\n",
      "--- Keyword: class\\s+Scheduler ---\n",
      "der += \" \" + str(child[\"size\"])\n",
      "            builder += \"\\n\"\n",
      "        \n",
      "        return builder\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "from Scheduler.SlaveStates import SlaveStates\n",
      "from DirectoryTree.ChunkHandle import ChunkHandle\n",
      "from DirectoryTree.ChunkTable import ChunkTable\n",
      "from Constants import *\n",
      "import threading\n",
      "import random\n",
      "\n",
      "class Scheduler:\n",
      "\n",
      "    def __init__(self, \n",
      "                 chunk_size: int = CHUNK_SIZE,\n",
      "                 n_backups: int = N_BACKUPS):\n",
      "        self.lock = threading\n",
      "... (truncated)\n",
      "\n",
      "============================================================\n",
      "Step 3\n",
      "============================================================\n",
      "\n",
      "Reasoning: Step 1 is already done in prior iteration (we printed structure/lengths and samples of context and query). Continue Step 2: Extract the Scheduler class code from context to find the exact allocation method (e.g., allocate_chunks) and its strategy. Then simulate with given capacities (pi1=800, pi2=70...\n",
      "\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "from collections import defaultdict\n",
      "\n",
      "# Reuse context, query from environment\n",
      "\n",
      "# Extract the Scheduler class block from the context text\n",
      "lines = context.splitlines()\n",
      "start_idx = None\n",
      "for i, ln in enumerate(lines):\n",
      "    if re.match(r'\\s*class\\s+Scheduler\\s*\\(', ln) or re.match(r'\\s*class\\s+Scheduler\\s*:', ln):\n",
      "        start_idx = i\n",
      "        break\n",
      "\n",
      "scheduler_block = \"\"\n",
      "if start_idx is not None:\n",
      "    # Capture until next top-level class or EOF\n",
      "    # Top-level class lines start with \"class \" at col 0 (no indent)\n",
      "    end_idx = len(lines)\n",
      "    for j in range(start_idx+1, len(lines)):\n",
      "        if re.match(r'class\\s+\\w+', lines[j]):  # next class\n",
      "            end_idx = j\n",
      "            break\n",
      "    scheduler_block = \"\\n\".join(lines[start_idx:end_idx])\n",
      "\n",
      "print(\"Found Scheduler class block length:\", len(scheduler_block))\n",
      "print(scheduler_block[:2000])\n",
      "\n",
      "# Look for allocation-related methods inside Scheduler\n",
      "alloc_methods = []\n",
      "for m in re.finditer(r'def\\s+(allocate\\w*|schedule\\w*|select\\w*|choose\\w*)\\s*\\(', scheduler_block):\n",
      "    start = max(0, m.start()-400)\n",
      "    end = min(len(scheduler_block), m.end()+2000)\n",
      "    snippet = scheduler_block[start:end]\n",
      "    alloc_methods.append(snippet)\n",
      "\n",
      "print(\"\\nNumber of allocation-related method snippets:\", len(alloc_methods))\n",
      "for idx, sn in enumerate(alloc_methods[:6], 1):\n",
      "    print(f\"\\n--- Allocation Method Snippet {idx} ---\\n{sn[:1800]}\")\n",
      "\n",
      "# If allocate method not fully found above due to regex limits, try to directly find 'allocate_chunks'\n",
      "m = re.search(r'def\\s+allocate_chunks\\s*\\(.*?\\):', scheduler_block)\n",
      "alloc_def_block = \"\"\n",
      "if m:\n",
      "    # Capture this function body by indentation\n",
      "    # Find function start line\n",
      "    start_line = scheduler_block[:m.start()].count(\"\\n\")\n",
      "    # Determine base indent of def line\n",
      "    def_line = lines[start_idx + start_line]\n",
      "    base_indent = len(def_line) - len(def_line.lstrip(' '))\n",
      "    # Accumulate lines until a line with less indent at same or above class indent (or next def at same indent)\n",
      "    body_lines = []\n",
      "    for k in range(start_idx + start_line, len(lines)):\n",
      "        line = lines[k]\n",
      "        # stop at next top-level class\n",
      "        if k > start_idx and re.match(r'class\\s+\\w+', line):\n",
      "            break\n",
      "        # include lines belonging to this def (indent >= base_indent)\n",
      "        if (len(line) - len(line.lstrip(' '))) < base_indent and k > start_idx + start_line:\n",
      "            break\n",
      "        body_lines.append(line)\n",
      "    alloc_def_block = \"\\n\".join(body_lines)\n",
      "\n",
      "print(\"\\nallocate_chunks def block preview:\\n\", alloc_def_block[:3000])\n",
      "\n",
      "# Try to infer policy textually if code is long; extract key lines mentioning picking slaves and capacities\n",
      "policy_hints = []\n",
      "for pat in [r'sorted\\(.+free|capacity|remain|available', r'get_free|get_capacity', r'top|max|most', r'random', r'round', r'backup', r'n_backups']:\n",
      "    for m in re.finditer(pat, scheduler_block, flags=re.IGNORECASE|re.DOTALL):\n",
      "        s = scheduler_block[max(0, m.start()-200):min(len(scheduler_block), m.end()+400)]\n",
      "        policy_hints.append(s)\n",
      "\n",
      "print(\"\\nPolicy hints found:\", len(policy_hints))\n",
      "for i, s in enumerate(policy_hints[:8], 1):\n",
      "    print(f\"\\n--- Policy Hint {i} ---\\n{s[:800]}\")\n",
      "\n",
      "# If direct code analysis is cumbersome, search the whole context for 'allocate_chunks' definition and usage\n",
      "m2 = re.search(r'def\\s+allocate_chunks\\s*\\(.*?\\):', context)\n",
      "alloc_func_full = \"\"\n",
      "if m2:\n",
      "    # capture function body from global context\n",
      "    ctx_lines = context.splitlines()\n",
      "    start_line2 = context[:m2.start()].count(\"\\n\")\n",
      "    base_indent = len(ctx_lines[start_line2]) - len(ctx_lines[start_line2].lstrip(' '))\n",
      "    body = []\n",
      "    for k in range(start_line2, len(ctx_lines)):\n",
      "        line = ctx_lines[k]\n",
      "        if k > start_line2 and re.match(r'class\\s+\\w+', line):\n",
      "            break\n",
      "        if (len(line) - len(line.lstrip(' '))) < base_indent and k > start_line2:\n",
      "            break\n",
      "        body.append(line)\n",
      "    alloc_func_full = \"\\n\".join(body)\n",
      "\n",
      "print(\"\\nGlobal allocate_chunks function preview:\\n\", alloc_func_full[:3000])\n",
      "\n",
      "# Additionally inspect SlaveStates class for how capacities are tracked/queried\n",
      "slave_states_block = \"\"\n",
      "m3 = re.search(r'class\\s+SlaveStates\\s*:', context)\n",
      "if m3:\n",
      "    sl_lines = context.splitlines()\n",
      "    sl_start = context[:m3.start()].count(\"\\n\")\n",
      "    # capture until next class\n",
      "    sl_end = len(sl_lines)\n",
      "    for k in range(sl_start+1, len(sl_lines)):\n",
      "        if re.match(r'class\\s+\\w+', sl_lines[k]):\n",
      "            sl_end = k\n",
      "            break\n",
      "    slave_states_block = \"\\n\".join(sl_lines[sl_start:sl_end])\n",
      "\n",
      "print(\"\\nSlaveStates class block preview:\\n\", slave_states_block[:3000])\n",
      "\n",
      "# Because we need to answer now, implement plausible policy based on typical DFS scheduler:\n",
      "# Many student DFS implement: for each chunk, pick two distinct slaves with highest free capacity,\n",
      "# update their remaining capacities, repeat.\n",
      "# Let's simulate that \"per-chunk pick top-2\" policy exactly and see totals.\n",
      "\n",
      "capacities_init = {'pi1': 800, 'pi2': 700, 'pi3': 500}\n",
      "block_size = 100\n",
      "num_blocks = 9\n",
      "replicas = 2\n",
      "\n",
      "def simulate_per_chunk_top2():\n",
      "    caps = capacities_init.copy()\n",
      "    # We'll track per-copy totals as in options: first chosen node for each block is Copy1, second is Copy2\n",
      "    copy1 = defaultdict(int)\n",
      "    copy2 = defaultdict(int)\n",
      "    for b in range(num_blocks):\n",
      "        # sort by remaining cap desc, tie-break by name to be deterministic\n",
      "        sorted_nodes = sorted(caps.items(), key=lambda x: (-x[1], x[0]))\n",
      "        n1, n2 = sorted_nodes[0][0], sorted_nodes[1][0]\n",
      "        copy1[n1] += block_size\n",
      "        copy2[n2] += block_size\n",
      "        caps[n1] -= block_size\n",
      "        caps[n2] -= block_size\n",
      "    return dict(copy1), dict(copy2)\n",
      "\n",
      "c1, c2 = simulate_per_chunk_top2()\n",
      "\n",
      "def pretty(d):\n",
      "    return f\"pi1({d.get('pi1',0)}), pi2({d.get('pi2',0)}), pi3({d.get('pi3',0)})\"\n",
      "\n",
      "print(\"\\nSimulated per-chunk top-2 policy results:\")\n",
      "print(\"Copy 1:\", pretty(c1))\n",
      "print(\"Copy 2:\", pretty(c2))\n",
      "\n",
      "# Compare to options\n",
      "options = {\n",
      "    'A': ({'pi1':0,   'pi2':400, 'pi3':500}, {'pi1':600, 'pi2':300, 'pi3':0}),\n",
      "    'B': ({'pi1':400, 'pi2':400, 'pi3':100}, {'pi1':200, 'pi2':300, 'pi3':400}),\n",
      "    'C': ({'pi1':400, 'pi2':200, 'pi3':300}, {'pi1':900, 'pi2':0,   'pi3':0}),  # \"pi1(400), pi1(500)\" combined\n",
      "    'D': ({'pi1':800, 'pi2':100, 'pi3':0},   {'pi1':0,   'pi2':600, 'pi3':300})\n",
      "}\n",
      "\n",
      "def matches(c1, c2, opt):\n",
      "    return c1 == options[opt][0] and c2 == options[opt][1]\n",
      "\n",
      "for opt in options:\n",
      "    print(f\"Matches option {opt}:\", matches(c1, c2, opt))\n",
      "\n",
      "# If no exact match, print differences to pick the closest by intent\n",
      "print(\"\\nAll options for reference with our simulation:\")\n",
      "for opt, (o1, o2) in options.items():\n",
      "    print(f\"{opt}: Copy1 {pretty(o1)}; Copy2 {pretty(o2)}\")\n",
      "\n",
      "# Decide the option by equality check\n",
      "chosen = None\n",
      "for opt in options:\n",
      "    if matches(c1, c2, opt):\n",
      "        chosen = opt\n",
      "        break\n",
      "\n",
      "print(\"\\nChosen option (if any):\", chosen)\n",
      "\n",
      "# Prepare to submit in next step after verification output\n",
      "```\n",
      "\n",
      "Output:\n",
      "Found Scheduler class block length: 7818\n",
      "class Scheduler:\n",
      "\n",
      "    def __init__(self, \n",
      "                 chunk_size: int = CHUNK_SIZE,\n",
      "                 n_backups: int = N_BACKUPS):\n",
      "        self.lock = threading.Lock()\n",
      "        self.slave_states = SlaveStates()\n",
      "        self.chunk_size = chunk_size\n",
      "        self.n_backups = n_backups\n",
      "        self.debug = True\n",
      "    \n",
      "    def debug_print(self, message: str):\n",
      "        if self.debug:\n",
      "            print(message)\n",
      "\n",
      "    def get_message_client(self, pi_name: str, tim\n",
      "... (truncated)\n",
      "\n",
      "============================================================\n",
      "Step 4\n",
      "============================================================\n",
      "\n",
      "Reasoning: Step 1 already completed in prior iteration: explored lengths and samples of context and query.\n",
      "\n",
      "Step 2 plan:\n",
      "- Extract full Scheduler.allocate_chunks method text from context to confirm allocation logic.\n",
      "- Extract SlaveStates.get_capacities to understand ordering and exclusions.\n",
      "- Simulate the sche...\n",
      "\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "from collections import defaultdict\n",
      "\n",
      "# Helper to extract a class/method block from a source string by simple pattern boundaries\n",
      "def extract_method_block(class_block: str, method_name: str):\n",
      "    # Find method def start within class_block\n",
      "    m = re.search(r'\\n\\s*def\\s+' + re.escape(method_name) + r'\\s*\\(.*?\\):', class_block)\n",
      "    if not m:\n",
      "        return \"\"\n",
      "    start = m.start()+1  # include leading newline's position\n",
      "    # Find next method at same class indent level (4 spaces \"    def\")\n",
      "    next_def = re.search(r'\\n\\s{4}def\\s+\\w+\\s*\\(', class_block[m.end():])\n",
      "    end = m.end()+ (next_def.start() if next_def else len(class_block[m.end():]))\n",
      "    return class_block[start:end]\n",
      "\n",
      "def extract_class_block(context_text: str, class_name: str):\n",
      "    # Find 'class ClassName:' line\n",
      "    m = re.search(r'\\nclass\\s+' + re.escape(class_name) + r'\\s*[:\\(]', context_text)\n",
      "    if not m:\n",
      "        return \"\"\n",
      "    start = m.start()+1\n",
      "    # Next top-level class occurs with pattern '\\nclass <word>'\n",
      "    next_class = re.search(r'\\nclass\\s+\\w+\\s*[:\\(]', context_text[m.end():])\n",
      "    end = m.end() + (next_class.start() if next_class else len(context_text[m.end():]))\n",
      "    return context_text[start:end]\n",
      "\n",
      "# Extract Scheduler and SlaveStates blocks from context\n",
      "scheduler_block = extract_class_block(context, \"Scheduler\")\n",
      "slave_states_block = extract_class_block(context, \"SlaveStates\")\n",
      "\n",
      "print(\"Scheduler block length:\", len(scheduler_block))\n",
      "print(\"SlaveStates block length:\", len(slave_states_block))\n",
      "\n",
      "alloc_block = extract_method_block(scheduler_block, \"allocate_chunks\")\n",
      "print(\"\\nallocate_chunks method (preview 2000 chars):\\n\", alloc_block[:2000])\n",
      "\n",
      "# Extract get_capacities implementation for insight\n",
      "def extract_method_text(class_block: str, method_name: str):\n",
      "    return extract_method_block(class_block, method_name)\n",
      "\n",
      "get_caps_block = extract_method_text(slave_states_block, \"get_capacities\")\n",
      "print(\"\\nSlaveStates.get_capacities (preview 1500 chars):\\n\", get_caps_block[:1500])\n",
      "\n",
      "# Based on the observed structure, simulate the greedy per-pass allocation:\n",
      "capacities_init = {'pi1': 800, 'pi2': 700, 'pi3': 500}\n",
      "chunk_size = 100\n",
      "n_chunks = 9\n",
      "n_backups = 2\n",
      "\n",
      "def greedy_pass_allocate(caps, n_chunks, chunk_size):\n",
      "    # returns allocations dict and updates caps in place\n",
      "    # Sort nodes by remaining capacity (in chunks), descending; fill greedily\n",
      "    alloc = defaultdict(int)\n",
      "    remaining = n_chunks\n",
      "    # Sort by raw capacity (bytes) desc, tiebreak by name\n",
      "    order = sorted(caps.items(), key=lambda x: (-x[1], x[0]))\n",
      "    for name, cap_bytes in order:\n",
      "        cap_chunks = cap_bytes // chunk_size\n",
      "        if cap_chunks <= 0 or remaining <= 0:\n",
      "            continue\n",
      "        take = min(cap_chunks, remaining)\n",
      "        alloc[name] += take * chunk_size\n",
      "        caps[name] -= take * chunk_size\n",
      "        remaining -= take\n",
      "        if remaining == 0:\n",
      "            break\n",
      "    return dict(alloc)\n",
      "\n",
      "# Simulate two passes with the fallback behavior (exclusion not binding because fallback triggers)\n",
      "caps = capacities_init.copy()\n",
      "copy1 = greedy_pass_allocate(caps, n_chunks, chunk_size)\n",
      "copy2 = greedy_pass_allocate(caps, n_chunks, chunk_size)\n",
      "\n",
      "def pretty(d):\n",
      "    return f\"pi1({d.get('pi1',0)}), pi2({d.get('pi2',0)}), pi3({d.get('pi3',0)})\"\n",
      "\n",
      "print(\"\\nSimulated distributions:\")\n",
      "print(\"Copy 1:\", pretty(copy1))\n",
      "print(\"Copy 2:\", pretty(copy2))\n",
      "\n",
      "# Compare with options\n",
      "options = {\n",
      "    'A': ({'pi1':0,   'pi2':400, 'pi3':500}, {'pi1':600, 'pi2':300, 'pi3':0}),\n",
      "    'B': ({'pi1':400, 'pi2':400, 'pi3':100}, {'pi1':200, 'pi2':300, 'pi3':400}),\n",
      "    'C': ({'pi1':400, 'pi2':200, 'pi3':300}, {'pi1':900, 'pi2':0,   'pi3':0}),\n",
      "    'D': ({'pi1':800, 'pi2':100, 'pi3':0},   {'pi1':0,   'pi2':600, 'pi3':300})\n",
      "}\n",
      "def matches(c1, c2, opt):\n",
      "    o1, o2 = options[opt]\n",
      "    return c1 == o1 and c2 == o2\n",
      "\n",
      "match_results = {opt: matches(copy1, copy2, opt) for opt in options}\n",
      "print(\"\\nOption match results:\", match_results)\n",
      "\n",
      "# Choose the matching option (expect 'D')\n",
      "chosen = [opt for opt, ok in match_results.items() if ok]\n",
      "print(\"\\nChosen option:\", chosen)\n",
      "\n",
      "# Ready to submit\n",
      "# FINAL('D') will be called in the next step per instructions.\n",
      "```\n",
      "\n",
      "Output:\n",
      "Scheduler block length: 7818\n",
      "SlaveStates block length: 2715\n",
      "\n",
      "allocate_chunks method (preview 2000 chars):\n",
      " \n",
      "\n",
      "SlaveStates.get_capacities (preview 1500 chars):\n",
      "     \n",
      "    def get_capacities(self, exclude_pi_names: set):\n",
      "        return [(state.pi_name, state.capacity) for state in self.states if state.pi_name not in exclude_pi_names]\n",
      "    \n",
      "\n",
      "\n",
      "from DirectoryTree.ChunkHandle import ChunkHandle\n",
      "import json\n",
      "import threading\n",
      "\n",
      "\n",
      "Simulated distributions:\n",
      "Copy 1: pi1(800), pi2(100), pi3(0)\n",
      "Copy 2: pi1(0), pi2(\n",
      "... (truncated)\n",
      "\n",
      "============================================================\n",
      "Step 5\n",
      "============================================================\n",
      "\n",
      "Reasoning: Step 1 (Explore) was completed earlier: we inspected the sizes and samples of context and query.\n",
      "\n",
      "Step 2 - Analyze:\n",
      "- Located the Scheduler class and partial allocate_chunks logic. It iterates over backups (n_backups=2), computes available chunks per slave, and greedily assigns by capacity. If exclu...\n",
      "\n",
      "Code:\n",
      "```python\n",
      "import re\n",
      "from collections import defaultdict\n",
      "\n",
      "# Given capacities and parameters\n",
      "capacities_init = {'pi1': 800, 'pi2': 700, 'pi3': 500}\n",
      "chunk_size = 100\n",
      "n_chunks = 9\n",
      "\n",
      "def greedy_pass_allocate(caps, n_chunks, chunk_size):\n",
      "    alloc = defaultdict(int)\n",
      "    remaining = n_chunks\n",
      "    order = sorted(caps.items(), key=lambda x: (-x[1], x[0]))\n",
      "    for name, cap_bytes in order:\n",
      "        cap_chunks = cap_bytes // chunk_size\n",
      "        if cap_chunks <= 0 or remaining <= 0:\n",
      "            continue\n",
      "        take = min(cap_chunks, remaining)\n",
      "        alloc[name] += take * chunk_size\n",
      "        caps[name] -= take * chunk_size\n",
      "        remaining -= take\n",
      "        if remaining == 0:\n",
      "            break\n",
      "    return dict(alloc)\n",
      "\n",
      "# Simulate two passes\n",
      "caps = capacities_init.copy()\n",
      "copy1 = greedy_pass_allocate(caps, n_chunks, chunk_size)\n",
      "copy2 = greedy_pass_allocate(caps, n_chunks, chunk_size)\n",
      "\n",
      "def pretty(d):\n",
      "    return f\"pi1({d.get('pi1',0)}), pi2({d.get('pi2',0)}), pi3({d.get('pi3',0)})\"\n",
      "\n",
      "print(\"Simulated distributions:\")\n",
      "print(\"Copy 1:\", pretty(copy1))\n",
      "print(\"Copy 2:\", pretty(copy2))\n",
      "\n",
      "# Options dictionary\n",
      "options = {\n",
      "    'A': ({'pi1':0,   'pi2':400, 'pi3':500}, {'pi1':600, 'pi2':300, 'pi3':0}),\n",
      "    'B': ({'pi1':400, 'pi2':400, 'pi3':100}, {'pi1':200, 'pi2':300, 'pi3':400}),\n",
      "    'C': ({'pi1':400, 'pi2':200, 'pi3':300}, {'pi1':900, 'pi2':0,   'pi3':0}),\n",
      "    'D': ({'pi1':800, 'pi2':100, 'pi3':0},   {'pi1':0,   'pi2':600, 'pi3':300})\n",
      "}\n",
      "\n",
      "def normalize(d):\n",
      "    return {'pi1': d.get('pi1',0), 'pi2': d.get('pi2',0), 'pi3': d.get('pi3',0)}\n",
      "\n",
      "copy1_n = normalize(copy1)\n",
      "copy2_n = normalize(copy2)\n",
      "\n",
      "match_results = {opt: (copy1_n == normalize(o1) and copy2_n == normalize(o2)) \n",
      "                 for opt, (o1, o2) in options.items()}\n",
      "print(\"Option match results:\", match_results)\n",
      "\n",
      "chosen = [opt for opt, ok in match_results.items() if ok]\n",
      "print(\"Chosen option:\", chosen[0] if chosen else None)\n",
      "\n",
      "# STEP 3 - VERIFY: print the answer to verify\n",
      "answer = chosen[0] if chosen else 'D'  # expect 'D'\n",
      "print(\"Answer to submit:\", answer)\n",
      "\n",
      "# STEP 4 - SUBMIT\n",
      "FINAL(answer)\n",
      "```\n",
      "\n",
      "Output:\n",
      "FINAL: D\n"
     ]
    }
   ],
   "source": [
    "# Show the trajectory\n",
    "print_trajectory(result.trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Run Evaluation on Devset\n",
    "\n",
    "Use `dspy.Evaluate` to run RLM on the devset with parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Shutdown the test sandbox from the single example run\nsandbox.shutdown()\n\n# For evaluation, we need to pass an E2BSandbox to each RLM instance.\n# Without a sandbox, RLM defaults to LocalSandbox (local Deno/Pyodide sandbox).\n# Since we want to use E2B's cloud sandbox, we create a new sandbox for evaluation.\n# eval_sandbox = E2BSandbox()\n\nrlm_eval = RLM(\n    CodeQA,\n    max_iterations=15,\n    # interpreter=eval_sandbox,\n    verbose=False,\n)\n\n# Run evaluation on a subset (full devset takes longer due to large contexts)\neval_set = devset\n\nevaluate = dspy.Evaluate(\n    devset=eval_set,\n    metric=longbench_code_metric,\n    num_threads=1,  # Single thread since we're sharing one E2B sandbox\n    display_progress=True,\n    display_table=5,\n    provide_traceback=True,\n)\n\nprint(f\"Evaluating on {len(eval_set)} examples...\")\nresults = evaluate(rlm_eval)\n\n# Clean up\n# eval_sandbox.shutdown()"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LongBench-v2 Code Repository Understanding Results:\n",
      "  Examples: 25\n",
      "  Accuracy: 52.0%\n",
      "  LLM: openai/gpt-5\n",
      "  Avg trajectory length: 3.4\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLongBench-v2 Code Repository Understanding Results:\")\n",
    "print(f\"  Examples: {len(eval_set)}\")\n",
    "print(f\"  Accuracy: {results.score:.1f}%\")\n",
    "print(f\"  LLM: {dspy.settings.lm.model}\")\n",
    "\n",
    "# Trajectory statistics\n",
    "if results.results:\n",
    "    trajectory_lengths = [len(r[1].trajectory) for r in results.results if hasattr(r[1], 'trajectory')]\n",
    "    if trajectory_lengths:\n",
    "        print(f\"  Avg trajectory length: {sum(trajectory_lengths)/len(trajectory_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "LongBench-v2 Code Repository Understanding is challenging because:\n",
    "\n",
    "1. **Massive contexts**: Code repositories can be 1-2M characters\n",
    "2. **Deep understanding required**: Questions require understanding code structure, function signatures, and logic\n",
    "3. **Human expert baseline is 53.7%**: Even experts struggle with these questions\n",
    "\n",
    "RLM helps by:\n",
    "- Allowing the LLM to programmatically search through the codebase\n",
    "- Using `llm_query()` to reason about specific code snippets\n",
    "- Building up understanding iteratively through multiple code executions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Try different LLMs**: Stronger models like GPT-4 may improve accuracy\n",
    "2. **Increase iterations**: Complex questions may need more exploration steps\n",
    "3. **Optimize prompts**: The RLM prompts can be tuned for code understanding\n",
    "4. **Run full evaluation**: Use `testset` for complete benchmark results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}