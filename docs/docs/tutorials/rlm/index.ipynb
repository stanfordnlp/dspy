{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "1. **Try different LLMs**: Stronger models like GPT-4 may improve accuracy\n",
    "2. **Increase iterations**: Complex questions may need more exploration steps\n",
    "3. **Optimize prompts**: The RLM prompts can be tuned for code understanding\n",
    "4. **Run full evaluation**: Use `longbench_testset` for complete benchmark results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "LongBench-v2 Code Repository Understanding is challenging because:\n",
    "\n",
    "1. **Massive contexts**: Code repositories can be 1-2M characters\n",
    "2. **Deep understanding required**: Questions require understanding code structure, function signatures, and logic\n",
    "3. **Human expert baseline is 53.7%**: Even experts struggle with these questions\n",
    "\n",
    "RLM helps by:\n",
    "- Allowing the LLM to programmatically search through the codebase\n",
    "- Using `llm_query()` to reason about specific code snippets\n",
    "- Building up understanding iteratively through multiple code executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nLongBench-v2 Code Repository Understanding Results:\")\n",
    "print(f\"  Examples: {len(longbench_eval_set)}\")\n",
    "print(f\"  Accuracy: {longbench_results.score:.1f}%\")\n",
    "print(f\"  LLM: {dspy.settings.lm.model}\")\n",
    "\n",
    "# Trajectory statistics\n",
    "if longbench_results.results:\n",
    "    trajectory_lengths = [len(r[1].trajectory) for r in longbench_results.results if hasattr(r[1], 'trajectory')]\n",
    "    if trajectory_lengths:\n",
    "        print(f\"  Avg trajectory length: {sum(trajectory_lengths)/len(trajectory_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation, create a non-verbose RLM\n",
    "longbench_rlm_eval = RLM(\n",
    "    CodeQA,\n",
    "    max_iterations=15,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "longbench_eval_set = longbench_devset\n",
    "\n",
    "longbench_evaluate = dspy.Evaluate(\n",
    "    devset=longbench_eval_set,\n",
    "    metric=longbench_code_metric,\n",
    "    num_threads=1,\n",
    "    display_progress=True,\n",
    "    display_table=5,\n",
    "    provide_traceback=True,\n",
    ")\n",
    "\n",
    "print(f\"Evaluating on {len(longbench_eval_set)} examples...\")\n",
    "longbench_results = longbench_evaluate(longbench_rlm_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation on Devset\n",
    "\n",
    "Use `dspy.Evaluate` to run RLM on the devset with parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the trajectory\n",
    "print_trajectory(longbench_result.trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RLM module with typed signature\n",
    "longbench_rlm = RLM(\n",
    "    CodeQA,\n",
    "    max_iterations=15,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Pick an example (prefer shorter context for faster demo)\n",
    "longbench_example = min(longbench_devset, key=lambda x: len(x.context))\n",
    "\n",
    "print(f\"Example:\")\n",
    "print(f\"  Difficulty: {longbench_example.difficulty}\")\n",
    "print(f\"  Context length: {len(longbench_example.context):,} chars\")\n",
    "print(f\"  Question: {longbench_example.query}\")\n",
    "print(f\"  Expected answer: {longbench_example.answer}\")\n",
    "print(f\"\\nRunning RLM...\")\n",
    "\n",
    "longbench_result = longbench_rlm(context=longbench_example.context, query=longbench_example.query)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"RESULT\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Predicted: {longbench_result.answer}\")\n",
    "print(f\"Expected: {longbench_example.answer}\")\n",
    "print(f\"Correct: {longbench_result.answer == longbench_example.answer}\")\n",
    "print(f\"Steps: {len(longbench_result.trajectory)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on a Single Example\n",
    "\n",
    "Let's test RLM on one example to see how it explores the code repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def longbench_code_metric(example, pred, trace=None):\n",
    "    \"\"\"Multiple choice accuracy metric.\"\"\"\n",
    "    gold = example.answer.strip().upper()\n",
    "    predicted = pred.answer.strip().upper() if pred.answer else \"\"\n",
    "    return 1.0 if predicted == gold else 0.0\n",
    "\n",
    "# Define signature with Literal type for answer\n",
    "class CodeQA(dspy.Signature):\n",
    "    \"\"\"Answer a multiple choice question about a code repository.\"\"\"\n",
    "    context: str = dspy.InputField(desc=\"The code repository contents\")\n",
    "    query: str = dspy.InputField(desc=\"The question with choices A, B, C, D\")\n",
    "    answer: Literal[\"A\", \"B\", \"C\", \"D\"] = dspy.OutputField(desc=\"The answer: A, B, C, or D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Metric and Signature\n",
    "\n",
    "Simple multiple-choice accuracy: exact match on the letter (A, B, C, or D).\n",
    "\n",
    "We use a `Literal` type in the signature to constrain the output to valid choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_longbench_example(row):\n",
    "    \"\"\"Convert a LongBench-v2 row to a DSPy example.\"\"\"\n",
    "    # Format choices as part of the query - emphasize single letter answer\n",
    "    choices_text = f\"\"\"\n",
    "\n",
    "Choices:\n",
    "A) {row['choice_A']}\n",
    "B) {row['choice_B']}\n",
    "C) {row['choice_C']}\n",
    "D) {row['choice_D']}\"\"\"\n",
    "    \n",
    "    return dspy.Example(\n",
    "        id=row[\"_id\"],\n",
    "        context=row[\"context\"],\n",
    "        query=row[\"question\"] + choices_text,\n",
    "        answer=row[\"answer\"],  # A, B, C, or D\n",
    "        difficulty=row[\"difficulty\"],\n",
    "        length=row[\"length\"],\n",
    "        choice_A=row[\"choice_A\"],\n",
    "        choice_B=row[\"choice_B\"],\n",
    "        choice_C=row[\"choice_C\"],\n",
    "        choice_D=row[\"choice_D\"],\n",
    "    ).with_inputs(\"context\", \"query\")\n",
    "\n",
    "# Create examples and shuffle\n",
    "longbench_examples = [make_longbench_example(row) for row in code_examples]\n",
    "random.shuffle(longbench_examples)\n",
    "\n",
    "# Split: first 25 as devset, rest as testset\n",
    "longbench_devset = longbench_examples[:25]\n",
    "longbench_testset = longbench_examples[25:]\n",
    "\n",
    "print(f\"Devset: {len(longbench_devset)} examples\")\n",
    "print(f\"Testset: {len(longbench_testset)} examples\")\n",
    "\n",
    "# Show an example\n",
    "ex = longbench_devset[0]\n",
    "print(f\"\\nExample:\")\n",
    "print(f\"  Difficulty: {ex.difficulty}\")\n",
    "print(f\"  Length: {ex.length}\")\n",
    "print(f\"  Context: {len(ex.context):,} chars\")\n",
    "print(f\"  Question: {ex.query[:200]}...\")\n",
    "print(f\"  Answer: {ex.answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Examples for Evaluation\n",
    "\n",
    "Convert the dataset to `dspy.Example` format. Each question has 4 choices (A, B, C, D) and one correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Code Repository Understanding\n",
    "code_examples = [row for row in longbench_dataset if row['domain'] == 'Code Repository Understanding']\n",
    "\n",
    "print(f\"Code Repository Understanding: {len(code_examples)} examples\")\n",
    "print(f\"\\nDifficulty distribution:\")\n",
    "print(f\"  {dict(Counter(row['difficulty'] for row in code_examples))}\")\n",
    "print(f\"\\nLength distribution:\")\n",
    "print(f\"  {dict(Counter(row['length'] for row in code_examples))}\")\n",
    "\n",
    "# Show context length statistics\n",
    "context_lens = [len(row['context']) for row in code_examples]\n",
    "print(f\"\\nContext lengths:\")\n",
    "print(f\"  Min: {min(context_lens):,} chars\")\n",
    "print(f\"  Max: {max(context_lens):,} chars\")\n",
    "print(f\"  Mean: {sum(context_lens)//len(context_lens):,} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longbench_dataset = load_dataset('THUDM/LongBench-v2', split='train')\n",
    "\n",
    "print(f\"Total examples: {len(longbench_dataset)}\")\n",
    "print(f\"\\nDomains:\")\n",
    "for domain, count in sorted(Counter(row['domain'] for row in longbench_dataset).items()):\n",
    "    print(f\"  {domain}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the LongBench-v2 Dataset\n",
    "\n",
    "LongBench-v2 contains 503 challenging multiple-choice questions across 6 domains:\n",
    "- Single-Document QA (175)\n",
    "- Multi-Document QA (125)\n",
    "- Long In-context Learning (81)\n",
    "- Code Repository Understanding (50)\n",
    "- Long-dialogue History Understanding (39)\n",
    "- Long Structured Data Understanding (33)\n",
    "\n",
    "We focus on **Code Repository Understanding** - questions about real code repositories with contexts up to 2M characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Recursive Language Models (RLM)\n",
    "\n",
    "This tutorial demonstrates **Recursive Language Models (RLM)**, an inference strategy where LLMs treat long contexts as part of an external environment rather than feeding them directly to the model. The LLM writes Python code to programmatically examine, decompose, and recursively call sub-LLMs over snippets.\n",
    "\n",
    "Reference: [\"Recursive Language Models\" (Zhang, Kraska, Khattab, 2025)](https://arxiv.org/abs/placeholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usecases\n",
    "\n",
    "RLMs are good for usecases where you want to let an llm delegate a task to potentially recursive \"subagents\". The primary benefit of this is that you can avoid polluting the context of the main agent.\n",
    "\n",
    "This lets the LLM operate symbolically on whatever the outputs of the subagents are.\n",
    "\n",
    "A simple example is when you need to perform multiple operations sequentially, say a map reduce.\n",
    "\n",
    "If you tell a typical coding agent to summarize all of the files in a codebase relevant to X feature, it would need to generally do a gathering step, and a synthesis step, all in context.\n",
    "\n",
    "If you have a subagent built in, such as the explore tool in claude code, it might return a list of files to context with summaries.\n",
    "\n",
    "To be more concrete, when you call the explore subagent in claude code, it returns a string in the format that the main agent provides. But this is just a string!\n",
    "\n",
    "If you want to perform any operations on this string, it needs to be written to a file, or to be rewritten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "We'll evaluate RLM on the [Oolong benchmark](https://huggingface.co/datasets/oolongbench/oolong-synth), which tests long context reasoning and aggregation capabilities.\n",
    "\n",
    "Install dependencies: `pip install dspy datasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Configure DSPy with an LLM. RLM uses this LLM both for generating code and for the `llm_query()` tool inside the sandbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "lm = dspy.LM(\"openai/gpt-5\")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='pydantic')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Oolong Dataset\n",
    "\n",
    "Oolong is a benchmark for evaluating long context reasoning and aggregation. Tasks include counting labels, finding needles in haystacks, and comparing frequencies.\n",
    "\n",
    "The dataset has two splits with different source datasets:\n",
    "- **validation** (1,300 examples): `spam` (650), `trec_coarse` (650)\n",
    "- **test** (5,200 examples): `agnews`, `app_reviews`, `formality`, `imdb`, `metaphors`, `multinli`, `negation`, `yahoo` (650 each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "\n",
    "# Load both splits\n",
    "dataset = load_dataset(\"oolongbench/oolong-synth\")\n",
    "val_data = dataset[\"validation\"]\n",
    "test_data = dataset[\"test\"]\n",
    "\n",
    "print(f\"Validation: {len(val_data)} examples\")\n",
    "print(f\"  Datasets: {dict(Counter(val_data['dataset']))}\")\n",
    "\n",
    "print(f\"\\nTest: {len(test_data)} examples\")\n",
    "print(f\"  Datasets: {dict(Counter(test_data['dataset']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Examples for Evaluation\n",
    "\n",
    "Convert the dataset to `dspy.Example` format, organized by source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def make_example(row):\n",
    "    return dspy.Example(\n",
    "        id=row[\"id\"],\n",
    "        context_window_id=row.get(\"context_window_id\"),\n",
    "        context=row[\"context_window_text\"],\n",
    "        query=row[\"question\"],\n",
    "        answer=row[\"answer\"],\n",
    "        task=row.get(\"task\", \"unknown\"),\n",
    "        answer_type=row.get(\"answer_type\", \"LABEL\"),\n",
    "        dataset=row.get(\"dataset\", \"unknown\"),\n",
    "        context_len=row.get(\"context_len\", len(row[\"context_window_text\"])),\n",
    "    ).with_inputs(\"context\", \"query\")\n",
    "\n",
    "# Separate validation by source dataset\n",
    "val_spam = [make_example(row) for row in val_data if row[\"dataset\"] == \"spam\"]\n",
    "val_trec = [make_example(row) for row in val_data if row[\"dataset\"] == \"trec_coarse\"]\n",
    "\n",
    "# IMPORTANT: Shuffle before splitting! The dataset is ordered by context_len,\n",
    "# so without shuffling, devset would only contain the shortest (easiest) examples.\n",
    "random.shuffle(val_spam)\n",
    "random.shuffle(val_trec)\n",
    "\n",
    "# For each validation dataset: first 50 as dev, rest as val\n",
    "devset_spam, valset_spam = val_spam[:50], val_spam[50:]\n",
    "devset_trec, valset_trec = val_trec[:50], val_trec[50:]\n",
    "\n",
    "# Combined devset and valset\n",
    "devset = devset_spam + devset_trec\n",
    "valset = valset_spam + valset_trec\n",
    "\n",
    "# Separate test by source dataset\n",
    "test_by_dataset = {}\n",
    "for row in test_data:\n",
    "    ds_name = row[\"dataset\"]\n",
    "    if ds_name not in test_by_dataset:\n",
    "        test_by_dataset[ds_name] = []\n",
    "    test_by_dataset[ds_name].append(make_example(row))\n",
    "\n",
    "# Full testset (also shuffle for fair sampling)\n",
    "testset = [make_example(row) for row in test_data]\n",
    "random.shuffle(testset)\n",
    "\n",
    "print(\"Validation splits (shuffled by context_len):\")\n",
    "print(f\"  devset_spam: {len(devset_spam)}, valset_spam: {len(valset_spam)}\")\n",
    "print(f\"  devset_trec: {len(devset_trec)}, valset_trec: {len(valset_trec)}\")\n",
    "print(f\"  devset (combined): {len(devset)}, valset (combined): {len(valset)}\")\n",
    "\n",
    "# Show context_len distribution in devset to verify shuffling worked\n",
    "from collections import Counter\n",
    "devset_lens = Counter(ex.context_len for ex in devset)\n",
    "print(f\"\\n  devset context_len distribution: {dict(sorted(devset_lens.items()))}\")\n",
    "\n",
    "print(f\"\\nTest splits:\")\n",
    "for name, examples in sorted(test_by_dataset.items()):\n",
    "    print(f\"  {name}: {len(examples)}\")\n",
    "print(f\"  testset (combined): {len(testset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Metric\n",
    "\n",
    "The metric matches the [official Oolong benchmark evaluation](https://github.com/abertsch72/oolong/blob/main/src/eval/eval_helpers.py):\n",
    "- Parses answers by taking text after the last `:`\n",
    "- Exact string matching for labels\n",
    "- Partial credit for numeric answers: `0.75 ** abs(gold - predicted)`\n",
    "- Special handling for comparison answers (\"more common\", \"less common\", \"same frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "\n",
    "def parse_answer(answer: str) -> tuple[str, str]:\n",
    "    \"\"\"Parse model answer, returning (parsed_answer, confidence).\n",
    "    \n",
    "    Matches the official Oolong benchmark parsing logic.\n",
    "    \"\"\"\n",
    "    parse_confidence = \"low\"\n",
    "    \n",
    "    if \":\" not in answer:\n",
    "        if len(answer) < 20:\n",
    "            return answer, parse_confidence\n",
    "        else:\n",
    "            return answer.split()[-1], parse_confidence\n",
    "    \n",
    "    # Take text after last \":\"\n",
    "    candidate = answer.split(\":\")[-1].strip()\n",
    "    \n",
    "    # Remove markdown bolding and brackets\n",
    "    candidate = candidate.replace(\"*\", \"\")\n",
    "    candidate = candidate.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    \n",
    "    parse_confidence = \"med\"\n",
    "    \n",
    "    # Higher confidence if answer follows expected format\n",
    "    if any(marker in answer for marker in [\"User:\", \"Answer:\", \"Date:\", \"Label\"]):\n",
    "        parse_confidence = \"high\"\n",
    "    \n",
    "    if len(candidate) < 20:\n",
    "        parse_confidence = \"vhigh\"\n",
    "    elif \"more common\" in candidate:\n",
    "        candidate = \"more common\"\n",
    "    elif \"less common\" in candidate:\n",
    "        candidate = \"less common\"\n",
    "    elif \"same frequency\" in candidate:\n",
    "        candidate = \"same frequency\"\n",
    "    \n",
    "    return candidate, parse_confidence\n",
    "\n",
    "\n",
    "def oolong_metric(example, pred, trace=None):\n",
    "    \"\"\"Official Oolong benchmark metric with partial credit for numeric answers.\"\"\"\n",
    "    \n",
    "    # Parse gold answer\n",
    "    expected = example.answer\n",
    "    if isinstance(expected, list):\n",
    "        gold = str(expected[0])\n",
    "    else:\n",
    "        expected_str = str(expected).strip()\n",
    "        if expected_str.startswith(\"[\") and expected_str.endswith(\"]\"):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(expected_str)\n",
    "                gold = str(parsed[0]) if isinstance(parsed, list) else expected_str\n",
    "            except:\n",
    "                gold = expected_str\n",
    "        else:\n",
    "            gold = expected_str\n",
    "    \n",
    "    # Parse model answer\n",
    "    trimmed_output, _ = parse_answer(pred.answer)\n",
    "    \n",
    "    # Exact match\n",
    "    if str(trimmed_output).strip().lower() == str(gold).strip().lower():\n",
    "        return 1.0\n",
    "    \n",
    "    # Comparison answers (more/less/same common)\n",
    "    if trimmed_output in [\"more common\", \"less common\", \"same frequency\"]:\n",
    "        if trimmed_output in str(gold).lower():\n",
    "            return 1.0\n",
    "    \n",
    "    # Numeric: partial credit with exponential decay\n",
    "    answer_type = getattr(example, \"answer_type\", \"\")\n",
    "    if answer_type == \"ANSWER_TYPE.NUMERIC\" or answer_type == \"NUMERIC\":\n",
    "        try:\n",
    "            pred_num = int(trimmed_output)\n",
    "            gold_num = int(gold)\n",
    "            return 0.75 ** abs(gold_num - pred_num)\n",
    "        except (ValueError, TypeError):\n",
    "            pass\n",
    "    \n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize RLM\n",
    "\n",
    "RLM executes in a secure Deno/Pyodide/WASM sandbox. The LLM can:\n",
    "- Access the `context` variable containing the input data\n",
    "- Call `llm_query(prompt)` to query a sub-LLM for semantic analysis\n",
    "- Use standard Python libraries (re, json, collections, etc.)\n",
    "- Build up answers iteratively through multiple code executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_query_nano(prompt: str = \"\") -> str:\n",
    "    \"\"\"Query gpt-5-nano with a prompt.\"\"\"\n",
    "    if not prompt:\n",
    "        raise ValueError(\"prompt is required\")\n",
    "    nano = dspy.LM(\"openai/gpt-5-nano\")\n",
    "    response = nano(prompt)\n",
    "    return response[0] if isinstance(response, list) else str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.predict.rlm import RLM\n",
    "\n",
    "rlm = RLM(\"context, query -> answer\", max_iterations=20, sub_lm=dspy.LM(\"openai/gpt-5-nano\"), verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation on Devset\n",
    "\n",
    "Use `dspy.Evaluate` to run RLM on the devset with parallelism. For full evaluation, replace `devset` with `valset` or `testset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available splits: devset_spam, devset_trec, valset_spam, valset_trec, test_by_dataset[name], testset\n",
    "dspy.configure(lm=dspy.LM(\"openai/gpt-5\"))\n",
    "\n",
    "# Use the pre-shuffled devset (now includes mixed context lengths)\n",
    "current_devset = devset_trec[:20]\n",
    "\n",
    "evaluate = dspy.Evaluate(\n",
    "    devset=current_devset,\n",
    "    metric=oolong_metric,\n",
    "    num_threads=10,\n",
    "    display_progress=True,\n",
    "    display_table=10,\n",
    "    provide_traceback=True,\n",
    ")\n",
    "\n",
    "results = evaluate(rlm)\n",
    "\n",
    "# Show results with context_len breakdown\n",
    "print(\"\\nOolong RLM Evaluation Results:\")\n",
    "print(f\"Split: devset_trec (shuffled), {len(current_devset)} examples\")\n",
    "print(f\"  LLM: {dspy.settings.lm.model}\")\n",
    "print(f\"  Alex RLM: 56.5%\")\n",
    "print(f\"  DSPy RLM: {results.score:.2f}%\")\n",
    "\n",
    "avg_trajectory_length = sum(len(r[1].trajectory) for r in results.results) / len(results.results)\n",
    "print(f\"  Avg trajectory length: {avg_trajectory_length:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_trajectory_length = sum(len(result[1].trajectory) for result in results.results) / len(results.results)\n",
    "print(f\"  Avg trajectory length: {avg_trajectory_length:.2f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Collect all trajectory lengths\n",
    "trajectory_lengths = [len(result[1].trajectory) for result in results.results]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "counts, bins, patches = plt.hist(\n",
    "    trajectory_lengths,\n",
    "    bins=range(1, max(trajectory_lengths) + 2),\n",
    "    edgecolor='black',\n",
    "    align='left'\n",
    ")\n",
    "plt.xlabel('Trajectory Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Trajectory Lengths')\n",
    "plt.xticks(range(1, max(trajectory_lengths)+1))\n",
    "\n",
    "# Add counts above each bar\n",
    "for count, bin_left, patch in zip(counts, bins[:-1], patches):\n",
    "    if count > 0:\n",
    "        plt.text(\n",
    "            bin_left + patch.get_width() / 2,\n",
    "            count + 0.02 * max(counts),  # A little above the bar\n",
    "            f\"{int(count)}\",\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=10\n",
    "        )\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect a Single Trajectory\n",
    "\n",
    "Let's run RLM on one example and examine the trajectory (the sequence of code executions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, ex in enumerate(current_devset):\n",
    "    #   print(f\"{i}: context_len={ex.context_len:,} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = current_devset[8]\n",
    "print(example.context_len)\n",
    "\n",
    "def print_trajectory(trajectory):\n",
    "    \"\"\"Pretty-print an RLM trajectory.\"\"\"\n",
    "    for i, step in enumerate(trajectory):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Step {i+1}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if step.get(\"reasoning\"):\n",
    "            reasoning = step['reasoning']\n",
    "            print(f\"\\nReasoning: {reasoning}\")\n",
    "        \n",
    "        print(f\"\\nCode:\")\n",
    "        print(f\"```python\")\n",
    "        print(step[\"code\"])\n",
    "        print(f\"```\")\n",
    "        \n",
    "        print(f\"\\nOutput:\")\n",
    "        output = step[\"output\"]\n",
    "        if len(output) > 500:\n",
    "            print(output[:500] + \"\\n... (truncated)\")\n",
    "        else:\n",
    "            print(output if output else \"(no output)\")\n",
    "\n",
    "print(f\"Query: {example.query}\")\n",
    "print(f\"Expected: {example.answer}\")\n",
    "print(f\"Context length: {len(example.context):,} chars, First 100 chars: {example.context[:100]}\")\n",
    "print(\"\\nRunning RLM...\")\n",
    "\n",
    "gpt5 = dspy.LM(\"openai/gpt-5\")\n",
    "dspy.configure(lm=gpt5)\n",
    "\n",
    "from dspy.predict.rlm import RLM\n",
    "\n",
    "rlm = RLM(\"context, query -> answer\", max_iterations=10, verbose=True)\n",
    "\n",
    "result = rlm(context=example.context, query=example.query)\n",
    "\n",
    "print_trajectory(result.trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Trajectory\n",
    "\n",
    "The trajectory shows each iteration: the code executed and the output received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# print_trajectory(result.trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use RLM\n",
    "\n",
    "RLM is particularly useful when:\n",
    "\n",
    "1. **Long contexts**: The context is too large to fit in the LLM's context window, or would be expensive to process directly\n",
    "2. **Aggregation tasks**: You need to count, compare, or aggregate information across a large document\n",
    "3. **Structured data**: The context has structure (JSON, tables, sections) that can be programmatically navigated\n",
    "4. **Iterative exploration**: The answer requires examining the data from multiple angles\n",
    "\n",
    "The key insight is that LLMs can write code to efficiently process data, calling back to sub-LLMs only when semantic understanding is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Tests: Complex Signatures\n",
    "\n",
    "The tests below validate RLM's ability to parse complex signatures with:\n",
    "- Multiple typed output fields (`list[T]`, `dict[K,V]`, `Literal[]`, `bool`, `int`, `float`)\n",
    "- Detailed docstrings with multi-paragraph instructions\n",
    "- Field-level constraints (`min_length`, `max_length`, `ge`, `le`)\n",
    "- Nested structures (`list[dict[str, str]]`, `dict[str, list[str]]`)\n",
    "\n",
    "Each test covers a different use case: RAG document analysis, needle-in-haystack search, and codebase understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: RAG Document Analysis\n",
    "\n",
    "This test validates a complex signature with 5 output fields including nested types (`dict[str, list[str]]`), Literal types, and numeric constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class DocumentAnalysisSig(dspy.Signature):\n",
    "    \"\"\"Analyze documents to extract structured information including topics, entities, \n",
    "    sentiment, and key facts. You must carefully read through all documents and \n",
    "    aggregate findings across the corpus.\n",
    "    \n",
    "    Requirements:\n",
    "    - topics: List the main topics discussed, each as a short phrase\n",
    "    - entities: Extract named entities grouped by their type (PERSON, ORG, LOCATION, etc.)\n",
    "    - sentiment: Overall sentiment assessment based on tone and content\n",
    "    - key_facts: List 3-5 most important facts as complete sentences\n",
    "    - confidence: Your confidence in the analysis accuracy (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    \n",
    "    documents: list[str] = dspy.InputField(\n",
    "        desc=\"Collection of documents to analyze, each representing a separate text source\"\n",
    "    )\n",
    "    query: str = dspy.InputField(\n",
    "        desc=\"Specific analysis question or focus area\"\n",
    "    )\n",
    "    \n",
    "    topics: list[str] = dspy.OutputField(\n",
    "        desc=\"Main topics identified across all documents\"\n",
    "    )\n",
    "    entities: dict[str, list[str]] = dspy.OutputField(\n",
    "        desc=\"Named entities grouped by type, e.g., {'PERSON': ['Alice', 'Bob'], 'ORG': ['Acme Corp']}\"\n",
    "    )\n",
    "    sentiment: Literal[\"positive\", \"negative\", \"neutral\", \"mixed\"] = dspy.OutputField(\n",
    "        desc=\"Overall sentiment of the document collection\"\n",
    "    )\n",
    "    key_facts: list[str] = dspy.OutputField(\n",
    "        desc=\"Most important factual statements extracted from documents (3-5 items)\"\n",
    "    )\n",
    "    confidence: float = dspy.OutputField(\n",
    "        desc=\"Confidence score for the analysis between 0.0 and 1.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for RAG analysis\n",
    "rag_documents = [\n",
    "    \"Acme Corporation announced record profits today. CEO Alice Johnson credited the company's innovative AI products. The stock rose 15% on the news.\",\n",
    "    \"Industry analysts remain skeptical about Acme's long-term growth. Bob Smith from TechAnalytics noted that competition from GlobalTech is intensifying.\",\n",
    "    \"Acme's new AI assistant product received positive reviews from early adopters. Users praised its accuracy and ease of use.\",\n",
    "    \"The company plans to expand into European markets next quarter. CFO Carol Williams stated that Acme has secured $50M in additional funding.\"\n",
    "]\n",
    "\n",
    "rag_query = \"Analyze the business outlook and key stakeholders for Acme Corporation\"\n",
    "\n",
    "# Configure and run RLM\n",
    "dspy.configure(lm=dspy.LM(\"openai/gpt-5-nano\"))\n",
    "\n",
    "rag_rlm = RLM(DocumentAnalysisSig, max_iterations=15, verbose=True)\n",
    "rag_result = rag_rlm(documents=rag_documents, query=rag_query)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RAG Analysis Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Topics: {rag_result.topics}\")\n",
    "print(f\"Entities: {rag_result.entities}\")\n",
    "print(f\"Sentiment: {rag_result.sentiment}\")\n",
    "print(f\"Key Facts: {rag_result.key_facts}\")\n",
    "print(f\"Confidence: {rag_result.confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate RAG test outputs\n",
    "def validate_rag_result(result):\n",
    "    \"\"\"Validate all output types and constraints for RAG analysis.\"\"\"\n",
    "    # Type checks\n",
    "    assert isinstance(result.topics, list), f\"topics should be list, got {type(result.topics)}\"\n",
    "    assert all(isinstance(t, str) for t in result.topics), \"all topics should be strings\"\n",
    "    \n",
    "    assert isinstance(result.entities, dict), f\"entities should be dict, got {type(result.entities)}\"\n",
    "    for key, values in result.entities.items():\n",
    "        assert isinstance(key, str), f\"entity type should be str, got {type(key)}\"\n",
    "        assert isinstance(values, list), f\"entity values should be list, got {type(values)}\"\n",
    "        assert all(isinstance(v, str) for v in values), \"all entity names should be strings\"\n",
    "    \n",
    "    assert result.sentiment in [\"positive\", \"negative\", \"neutral\", \"mixed\"], \\\n",
    "        f\"sentiment should be one of the Literal values, got {result.sentiment}\"\n",
    "    \n",
    "    assert isinstance(result.key_facts, list), f\"key_facts should be list, got {type(result.key_facts)}\"\n",
    "    assert all(isinstance(f, str) for f in result.key_facts), \"all key_facts should be strings\"\n",
    "    \n",
    "    assert isinstance(result.confidence, (int, float)), f\"confidence should be numeric, got {type(result.confidence)}\"\n",
    "    assert 0.0 <= result.confidence <= 1.0, f\"confidence should be 0.0-1.0, got {result.confidence}\"\n",
    "    \n",
    "    print(\"RAG Test: All validations passed!\")\n",
    "\n",
    "validate_rag_result(rag_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: Needle in Haystack\n",
    "\n",
    "This test validates precise extraction with `bool`, `int` (with constraints), multiple `str` outputs, and `Literal` types. The signature has 6 output fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeedleSearchSig(dspy.Signature):\n",
    "    \"\"\"Search through a large text corpus to find specific hidden information.\n",
    "    \n",
    "    The text contains random filler content with a single \"needle\" - a specific \n",
    "    piece of information you must locate. Use programmatic search combined with \n",
    "    semantic understanding to find the needle efficiently.\n",
    "    \n",
    "    You MUST return the exact value found, with its location and surrounding context.\n",
    "    The search should be systematic - consider using regex, substring matching, or\n",
    "    line-by-line analysis depending on what you're looking for.\n",
    "    \"\"\"\n",
    "    \n",
    "    haystack: str = dspy.InputField(\n",
    "        desc=\"Large text corpus containing random content and one hidden needle\"\n",
    "    )\n",
    "    needle_description: str = dspy.InputField(\n",
    "        desc=\"Description of what to search for, e.g., 'a 7-digit magic number'\"\n",
    "    )\n",
    "    \n",
    "    found: bool = dspy.OutputField(\n",
    "        desc=\"Whether the needle was successfully located\"\n",
    "    )\n",
    "    needle_value: str = dspy.OutputField(\n",
    "        desc=\"The exact value of the needle if found, empty string if not found\"\n",
    "    )\n",
    "    line_number: int = dspy.OutputField(\n",
    "        desc=\"1-indexed line number where needle was found, 0 if not found\"\n",
    "    )\n",
    "    context_before: str = dspy.OutputField(\n",
    "        desc=\"The text immediately before the needle on the same line\"\n",
    "    )\n",
    "    context_after: str = dspy.OutputField(\n",
    "        desc=\"The text immediately after the needle on the same line\"\n",
    "    )\n",
    "    search_method: Literal[\"regex\", \"substring\", \"semantic\", \"hybrid\"] = dspy.OutputField(\n",
    "        desc=\"The primary method used to locate the needle\"\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_haystack(num_lines=200, needle_position=0.7, seed=42):\n",
    "    \"\"\"Generate a haystack with a hidden needle.\"\"\"\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    \n",
    "    words = [\"lorem\", \"ipsum\", \"dolor\", \"sit\", \"amet\", \"consectetur\", \"adipiscing\", \n",
    "             \"elit\", \"sed\", \"do\", \"eiusmod\", \"tempor\", \"incididunt\", \"ut\", \"labore\"]\n",
    "    lines = []\n",
    "    for i in range(num_lines):\n",
    "        line = \" \".join(random.choices(words, k=random.randint(8, 15)))\n",
    "        lines.append(line)\n",
    "    \n",
    "    needle_line = int(num_lines * needle_position)\n",
    "    magic_number = \"4827391\"\n",
    "    lines[needle_line] = f\"lorem ipsum dolor SECRET_CODE={magic_number} amet consectetur\"\n",
    "    \n",
    "    return \"\\n\".join(lines), magic_number, needle_line + 1  # 1-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate haystack and run needle search\n",
    "haystack_text, expected_needle, expected_line = generate_haystack()\n",
    "needle_desc = \"a 7-digit secret code in the format SECRET_CODE=XXXXXXX\"\n",
    "\n",
    "print(f\"Haystack: {len(haystack_text):,} chars, {len(haystack_text.splitlines())} lines\")\n",
    "print(f\"Expected needle: {expected_needle} at line {expected_line}\")\n",
    "\n",
    "# Run RLM\n",
    "needle_rlm = RLM(NeedleSearchSig, max_iterations=15, verbose=True)\n",
    "needle_result = needle_rlm(haystack=haystack_text, needle_description=needle_desc)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Needle Search Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Found: {needle_result.found}\")\n",
    "print(f\"Needle value: {needle_result.needle_value}\")\n",
    "print(f\"Line number: {needle_result.line_number}\")\n",
    "print(f\"Context before: {needle_result.context_before}\")\n",
    "print(f\"Context after: {needle_result.context_after}\")\n",
    "print(f\"Search method: {needle_result.search_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Needle Search outputs\n",
    "def validate_needle_result(result, expected_needle, expected_line):\n",
    "    \"\"\"Validate all output types and constraints for needle search.\"\"\"\n",
    "    # Type checks\n",
    "    assert isinstance(result.found, bool), f\"found should be bool, got {type(result.found)}\"\n",
    "    \n",
    "    assert isinstance(result.needle_value, str), f\"needle_value should be str, got {type(result.needle_value)}\"\n",
    "    \n",
    "    assert isinstance(result.line_number, int), f\"line_number should be int, got {type(result.line_number)}\"\n",
    "    assert result.line_number >= 0, f\"line_number should be >= 0, got {result.line_number}\"\n",
    "    \n",
    "    assert isinstance(result.context_before, str), f\"context_before should be str, got {type(result.context_before)}\"\n",
    "    assert isinstance(result.context_after, str), f\"context_after should be str, got {type(result.context_after)}\"\n",
    "    \n",
    "    assert result.search_method in [\"regex\", \"substring\", \"semantic\", \"hybrid\"], \\\n",
    "        f\"search_method should be one of the Literal values, got {result.search_method}\"\n",
    "    \n",
    "    # Value checks (the needle should be found correctly)\n",
    "    assert result.found == True, f\"Needle should be found, got found={result.found}\"\n",
    "    assert expected_needle in result.needle_value, \\\n",
    "        f\"needle_value should contain {expected_needle}, got {result.needle_value}\"\n",
    "    \n",
    "    print(\"Needle Search Test: All validations passed!\")\n",
    "\n",
    "validate_needle_result(needle_result, expected_needle, expected_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 3: Codebase Understanding\n",
    "\n",
    "This test validates deeply nested types with 7 output fields including `list[dict[str, str]]`, `dict[str, list[str]]`, and complex nested structures that require careful parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CodeAnalysisSig(dspy.Signature):\n",
    "    \"\"\"Analyze a codebase to extract structural information about modules, classes,\n",
    "    and functions. Identify dependencies, complexity metrics, and potential issues.\n",
    "    \n",
    "    This requires both syntactic parsing (examining code structure) and semantic\n",
    "    understanding (comprehending what the code does).\n",
    "    \n",
    "    You should:\n",
    "    1. Parse each file to identify classes, functions, and imports\n",
    "    2. Build a class hierarchy showing inheritance relationships\n",
    "    3. Map internal dependencies between modules\n",
    "    4. Identify potential code quality issues\n",
    "    5. Provide an executive summary of the codebase\n",
    "    \n",
    "    Return a comprehensive analysis with properly nested structure information.\n",
    "    \"\"\"\n",
    "    \n",
    "    codebase: dict[str, str] = dspy.InputField(\n",
    "        desc=\"Mapping of file paths to their source code contents\"\n",
    "    )\n",
    "    analysis_focus: str = dspy.InputField(\n",
    "        desc=\"Specific aspect to focus on: 'architecture', 'quality', 'dependencies', or 'all'\"\n",
    "    )\n",
    "    \n",
    "    modules: list[dict[str, str]] = dspy.OutputField(\n",
    "        desc=\"List of modules, each with keys: 'name', 'path', 'purpose' (brief description)\"\n",
    "    )\n",
    "    class_hierarchy: dict[str, list[str]] = dspy.OutputField(\n",
    "        desc=\"Mapping of base class names to list of derived class names\"\n",
    "    )\n",
    "    function_count: int = dspy.OutputField(\n",
    "        desc=\"Total number of functions/methods across all files\"\n",
    "    )\n",
    "    complexity_rating: Literal[\"low\", \"medium\", \"high\", \"very_high\"] = dspy.OutputField(\n",
    "        desc=\"Overall complexity assessment of the codebase\"\n",
    "    )\n",
    "    issues: list[dict[str, str]] = dspy.OutputField(\n",
    "        desc=\"Potential issues found, each with keys: 'severity' (low/medium/high), 'location', 'description'\"\n",
    "    )\n",
    "    dependencies: dict[str, list[str]] = dspy.OutputField(\n",
    "        desc=\"Internal dependencies: maps module name to list of modules it imports\"\n",
    "    )\n",
    "    summary: str = dspy.OutputField(\n",
    "        desc=\"2-3 sentence executive summary of the codebase analysis\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample codebase for analysis\n",
    "sample_codebase = {\n",
    "    \"models/user.py\": '''\n",
    "class BaseModel:\n",
    "    \"\"\"Base class for all models.\"\"\"\n",
    "    def save(self):\n",
    "        pass\n",
    "    \n",
    "    def delete(self):\n",
    "        pass\n",
    "\n",
    "class User(BaseModel):\n",
    "    \"\"\"User model with authentication.\"\"\"\n",
    "    def __init__(self, name, email):\n",
    "        self.name = name\n",
    "        self.email = email\n",
    "    \n",
    "    def authenticate(self, password):\n",
    "        return self._check_password(password)\n",
    "    \n",
    "    def _check_password(self, password):\n",
    "        # TODO: implement proper hashing\n",
    "        return password == \"secret\"\n",
    "''',\n",
    "    \"models/product.py\": '''\n",
    "from models.user import BaseModel\n",
    "\n",
    "class Product(BaseModel):\n",
    "    \"\"\"Product catalog item.\"\"\"\n",
    "    def __init__(self, name, price):\n",
    "        self.name = name\n",
    "        self.price = price\n",
    "    \n",
    "    def apply_discount(self, percent):\n",
    "        self.price *= (1 - percent / 100)\n",
    "    \n",
    "    def get_display_price(self):\n",
    "        return f\"${self.price:.2f}\"\n",
    "''',\n",
    "    \"services/auth.py\": '''\n",
    "from models.user import User\n",
    "\n",
    "def login(email, password):\n",
    "    \"\"\"Authenticate a user.\"\"\"\n",
    "    user = User(\"test\", email)\n",
    "    if user.authenticate(password):\n",
    "        return create_session(user)\n",
    "    return None\n",
    "\n",
    "def create_session(user):\n",
    "    \"\"\"Create a session token.\"\"\"\n",
    "    return f\"session_{user.email}\"\n",
    "\n",
    "def logout(session_token):\n",
    "    \"\"\"Invalidate a session.\"\"\"\n",
    "    pass\n",
    "''',\n",
    "    \"utils/helpers.py\": '''\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def validate_email(email):\n",
    "    \"\"\"Validate email format.\"\"\"\n",
    "    pattern = r\"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+$\"\n",
    "    return bool(re.match(pattern, email))\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Count word frequencies.\"\"\"\n",
    "    words = text.lower().split()\n",
    "    return dict(Counter(words))\n",
    "\n",
    "def sanitize_input(text):\n",
    "    \"\"\"Remove potentially dangerous characters.\"\"\"\n",
    "    return re.sub(r\"[<>\\\"']\", \"\", text)\n",
    "'''\n",
    "}\n",
    "\n",
    "# Run RLM\n",
    "code_rlm = RLM(CodeAnalysisSig, max_iterations=15, verbose=True)\n",
    "code_result = code_rlm(codebase=sample_codebase, analysis_focus=\"all\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Codebase Analysis Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Modules: {code_result.modules}\")\n",
    "print(f\"Class hierarchy: {code_result.class_hierarchy}\")\n",
    "print(f\"Function count: {code_result.function_count}\")\n",
    "print(f\"Complexity: {code_result.complexity_rating}\")\n",
    "print(f\"Issues: {code_result.issues}\")\n",
    "print(f\"Dependencies: {code_result.dependencies}\")\n",
    "print(f\"Summary: {code_result.summary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Codebase Analysis outputs\n",
    "def validate_code_result(result):\n",
    "    \"\"\"Validate all output types and nested structures for code analysis.\"\"\"\n",
    "    # modules: list[dict[str, str]]\n",
    "    assert isinstance(result.modules, list), f\"modules should be list, got {type(result.modules)}\"\n",
    "    for module in result.modules:\n",
    "        assert isinstance(module, dict), f\"each module should be dict, got {type(module)}\"\n",
    "        assert all(isinstance(k, str) and isinstance(v, str) for k, v in module.items()), \\\n",
    "            \"module dict should have str keys and values\"\n",
    "    \n",
    "    # class_hierarchy: dict[str, list[str]]\n",
    "    assert isinstance(result.class_hierarchy, dict), f\"class_hierarchy should be dict, got {type(result.class_hierarchy)}\"\n",
    "    for base, derived in result.class_hierarchy.items():\n",
    "        assert isinstance(base, str), f\"base class should be str, got {type(base)}\"\n",
    "        assert isinstance(derived, list), f\"derived classes should be list, got {type(derived)}\"\n",
    "        assert all(isinstance(d, str) for d in derived), \"all derived class names should be strings\"\n",
    "    \n",
    "    # function_count: int\n",
    "    assert isinstance(result.function_count, int), f\"function_count should be int, got {type(result.function_count)}\"\n",
    "    assert result.function_count >= 0, f\"function_count should be >= 0, got {result.function_count}\"\n",
    "    \n",
    "    # complexity_rating: Literal\n",
    "    assert result.complexity_rating in [\"low\", \"medium\", \"high\", \"very_high\"], \\\n",
    "        f\"complexity_rating should be one of the Literal values, got {result.complexity_rating}\"\n",
    "    \n",
    "    # issues: list[dict[str, str]]\n",
    "    assert isinstance(result.issues, list), f\"issues should be list, got {type(result.issues)}\"\n",
    "    for issue in result.issues:\n",
    "        assert isinstance(issue, dict), f\"each issue should be dict, got {type(issue)}\"\n",
    "        assert all(isinstance(k, str) and isinstance(v, str) for k, v in issue.items()), \\\n",
    "            \"issue dict should have str keys and values\"\n",
    "    \n",
    "    # dependencies: dict[str, list[str]]\n",
    "    assert isinstance(result.dependencies, dict), f\"dependencies should be dict, got {type(result.dependencies)}\"\n",
    "    for module, deps in result.dependencies.items():\n",
    "        assert isinstance(module, str), f\"module name should be str, got {type(module)}\"\n",
    "        assert isinstance(deps, list), f\"deps should be list, got {type(deps)}\"\n",
    "        assert all(isinstance(d, str) for d in deps), \"all dependency names should be strings\"\n",
    "    \n",
    "    # summary: str\n",
    "    assert isinstance(result.summary, str), f\"summary should be str, got {type(result.summary)}\"\n",
    "    assert len(result.summary) > 0, \"summary should not be empty\"\n",
    "    \n",
    "    print(\"Codebase Analysis Test: All validations passed!\")\n",
    "\n",
    "validate_code_result(code_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 4: Instructions-Only Signature\n",
    "\n",
    "This test validates that `signature.instructions` (the docstring) are properly transferred to RLM, even with **no field descriptions**. The signature relies entirely on the docstring to convey the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountXSig(dspy.Signature):\n",
    "    \"\"\"Count the number of times the letter 'x' appears in the input word.\n",
    "    \n",
    "    Return the exact count as an integer. Case-insensitive: both 'x' and 'X' should be counted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # No field descriptions - relies entirely on docstring instructions\n",
    "    word: str = dspy.InputField()\n",
    "    count: int = dspy.OutputField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases with known expected counts\n",
    "test_cases = [\n",
    "    (\"xerox\", 2),           # x at start and middle\n",
    "    (\"example\", 1),         # x in middle\n",
    "    (\"FOXBOX\", 2),          # uppercase X's\n",
    "    (\"python\", 0),          # no x\n",
    "    (\"xXxXx\", 5),           # mixed case, multiple\n",
    "]\n",
    "\n",
    "count_rlm = RLM(CountXSig, max_iterations=10, verbose=True)\n",
    "\n",
    "results_instructions_test = []\n",
    "for word, expected in test_cases:\n",
    "    result = count_rlm(word=word)\n",
    "    passed = result.count == expected\n",
    "    results_instructions_test.append({\n",
    "        \"word\": word,\n",
    "        \"expected\": expected,\n",
    "        \"got\": result.count,\n",
    "        \"passed\": passed\n",
    "    })\n",
    "    print(f\"word='{word}' | expected={expected} | got={result.count} | {'PASS' if passed else 'FAIL'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate instructions-only test\n",
    "all_passed = all(r[\"passed\"] for r in results_instructions_test)\n",
    "pass_count = sum(1 for r in results_instructions_test if r[\"passed\"])\n",
    "\n",
    "print(f\"\\nInstructions-Only Test: {pass_count}/{len(results_instructions_test)} passed\")\n",
    "assert all_passed, f\"Some test cases failed: {[r for r in results_instructions_test if not r['passed']]}\"\n",
    "print(\"Instructions-Only Test: All validations passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark: LongBench-v2 Code Repository Understanding\n",
    "\n",
    "This section demonstrates RLM on the [LongBench-v2](https://huggingface.co/datasets/THUDM/LongBench-v2) Code Repository Understanding benchmark.\n",
    "\n",
    "LongBench-v2 is a challenging benchmark with 503 multiple-choice questions requiring deep understanding and reasoning over contexts ranging from 8K to 2M words. The **Code Repository Understanding** split contains 50 questions about real code repositories.\n",
    "\n",
    "Reference: [\"LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks\" (Bai et al., 2024)](https://arxiv.org/abs/2412.15204)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
