{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohort Churn Analysis with RLM\n",
    "\n",
    "This tutorial demonstrates how to use DSPy's **RLM (Recursive Language Model)** module to investigate a retention problem across three large DataFrames.\n",
    "\n",
    "The RLM iteratively explores the data — computing retention curves, segmenting by acquisition channel, comparing feature usage between retained and churned users — until it identifies the root cause of churn.\n",
    "\n",
    "## The Dataset\n",
    "\n",
    "We use a synthetic SaaS dataset with an embedded signal:\n",
    "\n",
    "- **10,000 users** across 5 acquisition channels\n",
    "- **~250,000 feature usage events** spanning 10 feature types\n",
    "- **10,000 subscription records** with churn patterns\n",
    "\n",
    "The hidden signal the RLM should discover:\n",
    "- `paid_campaign_x` has ~45% churn (vs ~15% for organic/referral)\n",
    "- Churned users from that channel **never** use `advanced_reports`\n",
    "- Overall churn rate is ~23.8%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install dependencies if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q dspy pandas faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate the Data\n",
    "\n",
    "First, generate the synthetic dataset. The `generate_cohort_data.py` script creates three parquet files with carefully embedded churn signals.\n",
    "\n",
    "Run it once — the parquet files will be reused across experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating users...\n",
      "Generating subscriptions...\n",
      "Generating events...\n",
      "\n",
      "Dataset stats:\n",
      "  Users:             10,000 rows\n",
      "  Subscriptions:     10,000 rows\n",
      "  Events:           250,918 rows\n",
      "  Total memory:        16.3 MB\n",
      "\n",
      "Embedded signal:\n",
      "  paid_campaign_x churn rate: 45%\n",
      "  organic churn rate:         16%\n",
      "\n",
      "Saved to: users.parquet, subscriptions.parquet, events.parquet\n"
     ]
    }
   ],
   "source": [
    "!python generate_cohort_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Data\n",
    "\n",
    "Load the three DataFrames that the RLM will analyze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users:             10,000 rows\n",
      "Events:           250,918 rows\n",
      "Subscriptions:     10,000 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>email</th>\n",
       "      <th>name</th>\n",
       "      <th>signup_date</th>\n",
       "      <th>acquisition_channel</th>\n",
       "      <th>plan_at_signup</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>johnsonjoshua@example.org</td>\n",
       "      <td>Brian Yang</td>\n",
       "      <td>2024-01-29</td>\n",
       "      <td>organic</td>\n",
       "      <td>free</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>garzaanthony@example.org</td>\n",
       "      <td>Jonathan Johnson</td>\n",
       "      <td>2024-01-27</td>\n",
       "      <td>paid_campaign_x</td>\n",
       "      <td>pro</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>jennifermiles@example.com</td>\n",
       "      <td>Kevin Pacheco</td>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>organic</td>\n",
       "      <td>free</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>blakeerik@example.com</td>\n",
       "      <td>Christopher Bernard</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>paid_campaign_x</td>\n",
       "      <td>pro</td>\n",
       "      <td>AU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>curtis61@example.com</td>\n",
       "      <td>Lindsey Roman</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>organic</td>\n",
       "      <td>starter</td>\n",
       "      <td>BR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                      email                 name signup_date  \\\n",
       "0        1  johnsonjoshua@example.org           Brian Yang  2024-01-29   \n",
       "1        2   garzaanthony@example.org     Jonathan Johnson  2024-01-27   \n",
       "2        3  jennifermiles@example.com        Kevin Pacheco  2024-04-18   \n",
       "3        4      blakeerik@example.com  Christopher Bernard  2024-01-07   \n",
       "4        5       curtis61@example.com        Lindsey Roman  2024-04-17   \n",
       "\n",
       "  acquisition_channel plan_at_signup country  \n",
       "0             organic           free      US  \n",
       "1     paid_campaign_x            pro      US  \n",
       "2             organic           free      US  \n",
       "3     paid_campaign_x            pro      AU  \n",
       "4             organic        starter      BR  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "users = pd.read_parquet(\"users.parquet\")\n",
    "events = pd.read_parquet(\"events.parquet\")\n",
    "subscriptions = pd.read_parquet(\"subscriptions.parquet\")\n",
    "\n",
    "print(f\"Users:         {len(users):>10,} rows\")\n",
    "print(f\"Events:        {len(events):>10,} rows\")\n",
    "print(f\"Subscriptions: {len(subscriptions):>10,} rows\")\n",
    "\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>event_type</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>session_id</th>\n",
       "      <th>duration_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9910</td>\n",
       "      <td>report_export</td>\n",
       "      <td>2024-01-01 02:22:19</td>\n",
       "      <td>97b17398</td>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4854</td>\n",
       "      <td>file_upload</td>\n",
       "      <td>2024-01-01 04:24:55</td>\n",
       "      <td>cda2b255</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2520</td>\n",
       "      <td>search</td>\n",
       "      <td>2024-01-01 04:34:43</td>\n",
       "      <td>f833547a</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9206</td>\n",
       "      <td>report_export</td>\n",
       "      <td>2024-01-01 05:07:47</td>\n",
       "      <td>611423a4</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1550</td>\n",
       "      <td>dashboard_view</td>\n",
       "      <td>2024-01-01 05:39:15</td>\n",
       "      <td>275562c2</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id      event_type           timestamp session_id  duration_seconds\n",
       "0     9910   report_export 2024-01-01 02:22:19   97b17398               263\n",
       "1     4854     file_upload 2024-01-01 04:24:55   cda2b255                28\n",
       "2     2520          search 2024-01-01 04:34:43   f833547a               183\n",
       "3     9206   report_export 2024-01-01 05:07:47   611423a4                27\n",
       "4     1550  dashboard_view 2024-01-01 05:39:15   275562c2                67"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>subscription_start</th>\n",
       "      <th>subscription_end</th>\n",
       "      <th>plan</th>\n",
       "      <th>mrr</th>\n",
       "      <th>status</th>\n",
       "      <th>cancellation_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-01-29</td>\n",
       "      <td>NaT</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "      <td>active</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-01-27</td>\n",
       "      <td>NaT</td>\n",
       "      <td>pro</td>\n",
       "      <td>79</td>\n",
       "      <td>active</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>NaT</td>\n",
       "      <td>free</td>\n",
       "      <td>0</td>\n",
       "      <td>active</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>pro</td>\n",
       "      <td>79</td>\n",
       "      <td>active</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2024-04-17</td>\n",
       "      <td>2024-05-09</td>\n",
       "      <td>starter</td>\n",
       "      <td>29</td>\n",
       "      <td>cancelled</td>\n",
       "      <td>poor_support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id subscription_start subscription_end     plan  mrr     status  \\\n",
       "0        1         2024-01-29              NaT     free    0     active   \n",
       "1        2         2024-01-27              NaT      pro   79     active   \n",
       "2        3         2024-04-18              NaT     free    0     active   \n",
       "3        4         2024-01-07              NaT      pro   79     active   \n",
       "4        5         2024-04-17       2024-05-09  starter   29  cancelled   \n",
       "\n",
       "  cancellation_reason  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2                 NaN  \n",
       "3                 NaN  \n",
       "4        poor_support  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscriptions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define the Signature\n",
    "\n",
    "The signature tells the RLM what data it has and what outputs to produce. The docstring becomes the system prompt — it guides the model's investigation strategy.\n",
    "\n",
    "Note the use of `dspy.DataFrame` as the input type. This tells the RLM to serialize the DataFrame and make it available in the code sandbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "\n",
    "class CohortRetentionAnalysis(dspy.Signature):\n",
    "    \"\"\"You are a data analyst investigating why user retention is dropping.\n",
    "\n",
    "    You have access to three DataFrames:\n",
    "    - `users`: user profiles with signup dates and acquisition channels\n",
    "    - `events`: feature usage events with timestamps\n",
    "    - `subscriptions`: subscription status, plan, MRR, and cancellation reasons\n",
    "\n",
    "    Investigate the data step by step. Compute retention by cohort, segment by\n",
    "    acquisition channel, compare feature usage between retained and churned users,\n",
    "    and identify the root cause of churn.\n",
    "    \"\"\"\n",
    "\n",
    "    users: dspy.DataFrame = dspy.InputField(\n",
    "        desc=\"User profiles with signup_date, acquisition_channel, country\"\n",
    "    )\n",
    "    events: dspy.DataFrame = dspy.InputField(\n",
    "        desc=\"Feature usage events with user_id, event_type, timestamp\"\n",
    "    )\n",
    "    subscriptions: dspy.DataFrame = dspy.InputField(\n",
    "        desc=\"Subscription records with status, plan, mrr, cancellation_reason\"\n",
    "    )\n",
    "\n",
    "    overall_churn_rate: float = dspy.OutputField(\n",
    "        desc=\"Overall churn rate as a decimal (e.g. 0.25 for 25%)\"\n",
    "    )\n",
    "    worst_channel: str = dspy.OutputField(\n",
    "        desc=\"Acquisition channel with highest churn rate\"\n",
    "    )\n",
    "    key_finding: str = dspy.OutputField(\n",
    "        desc=\"The main insight about what differentiates churned users\"\n",
    "    )\n",
    "    recommendations: str = dspy.OutputField(\n",
    "        desc=\"2-3 actionable recommendations based on the analysis\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure the LM and Run\n",
    "\n",
    "Configure your language model and create the RLM. The `verbose=True` flag lets you watch the model's iterative investigation in real time.\n",
    "\n",
    "The RLM will (should):\n",
    "1. Examine the data schema\n",
    "2. Compute overall churn metrics\n",
    "3. Segment by acquisition channel\n",
    "4. Compare feature usage between churned and retained users\n",
    "5. Identify the root cause and formulate recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/11 12:50:09 INFO dspy.predict.rlm: RLM iteration 1/15\n",
      "Reasoning: Let me start by exploring the data to understand its structure, then compute churn rates, segment by channel, and analyze feature usage differences.\n",
      "Code:\n",
      "```python\n",
      "# First, let's explore the data\n",
      "print(\"=== USERS ===\")\n",
      "print(users.shape)\n",
      "print(users.head())\n",
      "print(\"\\nAcquisition channels:\", users['acquisition_channel'].value_counts())\n",
      "print(\"\\nPlan at signup:\", users['plan_at_signup'].value_counts())\n",
      "\n",
      "print(\"\\n=== SUBSCRIPTIONS ===\")\n",
      "print(subscriptions.shape)\n",
      "print(subscriptions.head())\n",
      "print(\"\\nStatus:\", subscriptions['status'].value_counts())\n",
      "print(\"\\nPlans:\", subscriptions['plan'].value_counts())\n",
      "print(\"\\nCancellation reasons:\", subscriptions['cancellation_reason'].value_counts())\n",
      "```\n",
      "/Users/kmad/dev/dspy-fresh/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='choices', input_value=Message(content='[[ ## re...one, 'reasoning': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ne, 'reasoning': None})), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "2026/02/11 12:50:15 INFO dspy.predict.rlm: Output (1,642 chars):\n",
      "=== USERS ===\n",
      "(10000, 7)\n",
      "   user_id                      email  ... plan_at_signup country\n",
      "0        1  johnsonjoshua@example.org  ...           free      US\n",
      "1        2   garzaanthony@example.org  ...            pro      US\n",
      "2        3  jennifermiles@example.com  ...           free      US\n",
      "3        4      blakeerik@example.com  ...            pro      AU\n",
      "4        5       curtis61@example.com  ...        starter      BR\n",
      "\n",
      "[5 rows x 7 columns]\n",
      "\n",
      "Acquisition channels: acquisition_channel\n",
      "organic            3030\n",
      "paid_campaign_x    2469\n",
      "social             1507\n",
      "paid_campaign_y    1499\n",
      "referral           1495\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Plan at signup: plan_at_signup\n",
      "free          3936\n",
      "starter       2977\n",
      "pro           2074\n",
      "enterprise    1013\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== SUBSCRIPTIONS ===\n",
      "(10000, 7)\n",
      "   user_id subscription_start  ...     status cancellation_reason\n",
      "0        1         2024-01-29  ...     active                None\n",
      "1        2         2024-01-27  ...     active                None\n",
      "2        3         2024-04-18  ...     active                None\n",
      "3        4         2024-01-07  ...     active                None\n",
      "4        5         2024-04-17  ...  cancelled        poor_support\n",
      "\n",
      "[5 rows x 7 columns]\n",
      "\n",
      "Status: status\n",
      "active       7623\n",
      "cancelled    2377\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Plans: plan\n",
      "free          3936\n",
      "starter       2977\n",
      "pro           2074\n",
      "enterprise    1013\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cancellation reasons: cancellation_reason\n",
      "no_longer_needed       413\n",
      "switched_competitor    407\n",
      "poor_support           405\n",
      "other                  403\n",
      "too_expensive          382\n",
      "missing_features       367\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2026/02/11 12:50:21 INFO dspy.predict.rlm: RLM iteration 2/15\n",
      "Reasoning: Now I have a good overview. Let me compute:\n",
      "1. Overall churn rate\n",
      "2. Churn rate by acquisition channel to find the worst channel\n",
      "3. Feature usage comparison between retained and churned users\n",
      "4. Cohort-based retention analysis\n",
      "\n",
      "Let me start with churn rate and channel analysis.\n",
      "Code:\n",
      "```python\n",
      "# Overall churn rate\n",
      "total_users = len(subscriptions)\n",
      "churned_users = len(subscriptions[subscriptions['status'] == 'cancelled'])\n",
      "overall_churn_rate = churned_users / total_users\n",
      "print(f\"Overall churn rate: {overall_churn_rate:.4f} ({overall_churn_rate*100:.2f}%)\")\n",
      "\n",
      "# Merge users with subscriptions to get channel info\n",
      "merged = users.merge(subscriptions, on='user_id')\n",
      "\n",
      "# Churn rate by acquisition channel\n",
      "channel_churn = merged.groupby('acquisition_channel').apply(\n",
      "    lambda x: (x['status'] == 'cancelled').mean()\n",
      ").sort_values(ascending=False)\n",
      "print(\"\\nChurn rate by acquisition channel:\")\n",
      "print(channel_churn)\n",
      "\n",
      "# Also look at absolute numbers\n",
      "channel_counts = merged.groupby('acquisition_channel')['status'].value_counts().unstack(fill_value=0)\n",
      "print(\"\\nChannel counts:\")\n",
      "print(channel_counts)\n",
      "```\n",
      "/Users/kmad/dev/dspy-fresh/.venv/lib/python3.13/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='choices', input_value=Message(content='[[ ## re...one, 'reasoning': None}), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...ne, 'reasoning': None})), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "2026/02/11 12:50:21 INFO dspy.predict.rlm: Output (537 chars):\n",
      "Overall churn rate: 0.2377 (23.77%)\n",
      "\n",
      "Churn rate by acquisition channel:\n",
      "acquisition_channel\n",
      "paid_campaign_x    0.447550\n",
      "social             0.230922\n",
      "organic            0.157756\n",
      "referral           0.151839\n",
      "paid_campaign_y    0.146097\n",
      "dtype: float64\n",
      "\n",
      "Channel counts:\n",
      "status               active  cancelled\n",
      "acquisition_channel                   \n",
      "organic                2552        478\n",
      "paid_campaign_x        1364       1105\n",
      "paid_campaign_y        1280        219\n",
      "referral               1268        227\n",
      "social                 1159        348\n",
      "\n",
      "2026/02/11 12:50:30 INFO dspy.predict.rlm: RLM iteration 3/15\n",
      "Reasoning: Great findings so far! `paid_campaign_x` has a dramatically higher churn rate (44.8%) compared to others. Let me now:\n",
      "1. Compare feature usage between retained and churned users\n",
      "2. Look at cohort-based retention\n",
      "3. Dig deeper into why paid_campaign_x churns so much\n",
      "4. Analyze cancellation reasons by channel\n",
      "Code:\n",
      "```python\n",
      "# Cancellation reasons by channel\n",
      "cancelled = merged[merged['status'] == 'cancelled']\n",
      "channel_reasons = cancelled.groupby('acquisition_channel')['cancellation_reason'].value_counts().unstack(fill_value=0)\n",
      "print(\"Cancellation reasons by channel:\")\n",
      "print(channel_reasons)\n",
      "\n",
      "# Feature usage comparison: retained vs churned\n",
      "events_with_status = events.merge(subscriptions[['user_id', 'status']], on='user_id')\n",
      "\n",
      "# Average events per user by status\n",
      "usage_by_status = events_with_status.groupby(['user_id', 'status']).size().reset_index(name='event_count')\n",
      "print(\"\\nAverage events per user:\")\n",
      "print(usage_by_status.groupby('status')['event_count'].describe())\n",
      "\n",
      "# Feature usage by event type for retained vs churned\n",
      "feature_usage = events_with_status.groupby(['status', 'event_type']).size().unstack(fill_value=0)\n",
      "# Normalize per user\n",
      "n_active = (subscriptions['status'] == 'active').sum()\n",
      "n_cancelled = (subscriptions['status'] == 'cancelled').sum()\n",
      "print(f\"\\nActive users: {n_active}, Cancelled users: {n_cancelled}\")\n",
      "\n",
      "feature_per_user = feature_usage.copy()\n",
      "feature_per_user.loc['active'] = feature_per_user.loc['active'] / n_active\n",
      "feature_per_user.loc['cancelled'] = feature_per_user.loc['cancelled'] / n_cancelled\n",
      "print(\"\\nAverage feature usage per user (by event type):\")\n",
      "print(feature_per_user.round(2))\n",
      "\n",
      "# Ratio of usage (active/cancelled) to see which features retained users use more\n",
      "ratio = feature_per_user.loc['active'] / feature_per_user.loc['cancelled']\n",
      "print(\"\\nUsage ratio (active/cancelled):\")\n",
      "print(ratio.sort_values(ascending=False).round(2))\n",
      "```\n",
      "2026/02/11 12:50:31 INFO dspy.predict.rlm: Output (1,503 chars):\n",
      "Cancellation reasons by channel:\n",
      "cancellation_reason  missing_features  ...  too_expensive\n",
      "acquisition_channel                    ...               \n",
      "organic                            76  ...             75\n",
      "paid_campaign_x                   165  ...            158\n",
      "paid_campaign_y                    37  ...             38\n",
      "referral                           32  ...             40\n",
      "social                             57  ...             71\n",
      "\n",
      "[5 rows x 6 columns]\n",
      "\n",
      "Average events per user:\n",
      "            count       mean        std  min   25%   50%   75%   max\n",
      "status                                                              \n",
      "active     7623.0  28.984127  15.277908  5.0  17.0  26.0  39.0  74.0\n",
      "cancelled  2377.0  12.609171   7.074695  5.0   6.0  11.0  17.0  36.0\n",
      "\n",
      "Active users: 7623, Cancelled users: 2377\n",
      "\n",
      "Average feature usage per user (by event type):\n",
      "event_type  advanced_reports  api_call  ...  settings_update  team_invite\n",
      "status                                  ...                              \n",
      "active                  2.90      2.95  ...             2.89         2.87\n",
      "cancelled               0.68      1.28  ...             1.32         1.32\n",
      "\n",
      "[2 rows x 10 columns]\n",
      "\n",
      "Usage ratio (active/cancelled):\n",
      "event_type\n",
      "advanced_reports      4.29\n",
      "api_call              2.30\n",
      "notification_click    2.24\n",
      "dashboard_view        2.22\n",
      "settings_update       2.19\n",
      "report_export         2.18\n",
      "team_invite           2.17\n",
      "search                2.15\n",
      "integration_setup     2.14\n",
      "file_upload           2.10\n",
      "dtype: float64\n",
      "\n",
      "2026/02/11 12:50:43 INFO dspy.predict.rlm: RLM iteration 4/15\n",
      "Reasoning: I have strong findings so far. Let me dig a bit deeper into paid_campaign_x to understand why it has such high churn, look at cohort retention, and then formulate my final answers.\n",
      "\n",
      "Key findings so far:\n",
      "1. Overall churn rate: 23.77%\n",
      "2. Worst channel: paid_campaign_x (44.8% churn vs ~15% for others)\n",
      "3. Churned users have much lower feature usage (avg 12.6 events vs 29 for active)\n",
      "4. Advanced reports usage is 4.29x higher for retained users - biggest differentiator\n",
      "5. paid_campaign_x accounts for 1105 of 2377 churned users (46.5% of all churn)\n",
      "\n",
      "Let me do a cohort analysis and check if paid_campaign_x users have specific cancellation patterns or lower engagement.\n",
      "Code:\n",
      "```python\n",
      "# Deeper look at paid_campaign_x vs others\n",
      "pcx = merged[merged['acquisition_channel'] == 'paid_campaign_x']\n",
      "others = merged[merged['acquisition_channel'] != 'paid_campaign_x']\n",
      "\n",
      "print(\"=== paid_campaign_x users ===\")\n",
      "print(\"Plan distribution:\")\n",
      "print(pcx['plan'].value_counts(normalize=True).round(3))\n",
      "print(\"\\nCancellation reasons (for churned):\")\n",
      "pcx_cancelled = pcx[pcx['status'] == 'cancelled']\n",
      "print(pcx_cancelled['cancellation_reason'].value_counts(normalize=True).round(3))\n",
      "\n",
      "print(\"\\n=== Other channel users ===\")\n",
      "print(\"Plan distribution:\")\n",
      "print(others['plan'].value_counts(normalize=True).round(3))\n",
      "print(\"\\nCancellation reasons (for churned):\")\n",
      "others_cancelled = others[others['acquisition_channel'] != 'paid_campaign_x'][others['status'] == 'cancelled']\n",
      "print(others_cancelled['cancellation_reason'].value_counts(normalize=True).round(3))\n",
      "\n",
      "# Feature usage for paid_campaign_x churned vs active\n",
      "pcx_users = set(pcx['user_id'])\n",
      "pcx_events = events[events['user_id'].isin(pcx_users)]\n",
      "pcx_events_status = pcx_events.merge(subscriptions[['user_id', 'status']], on='user_id')\n",
      "pcx_usage = pcx_events_status.groupby(['user_id', 'status']).size().reset_index(name='event_count')\n",
      "print(\"\\n=== Feature usage for paid_campaign_x users ===\")\n",
      "print(pcx_usage.groupby('status')['event_count'].describe())\n",
      "\n",
      "# Cohort analysis - signup month\n",
      "merged['signup_month'] = merged['signup_date'].dt.to_period('M')\n",
      "cohort_churn = merged.groupby('signup_month').apply(lambda x: (x['status'] == 'cancelled').mean())\n",
      "print(\"\\n=== Churn rate by signup cohort ===\")\n",
      "print(cohort_churn)\n",
      "\n",
      "# paid_campaign_x share by cohort\n",
      "pcx_share = merged.groupby('signup_month').apply(lambda x: (x['acquisition_channel'] == 'paid_campaign_x').mean())\n",
      "print(\"\\n=== paid_campaign_x share by cohort ===\")\n",
      "print(pcx_share)\n",
      "```\n",
      "2026/02/11 12:50:43 INFO dspy.predict.rlm: Output (1,538 chars):\n",
      "=== paid_campaign_x users ===\n",
      "Plan distribution:\n",
      "plan\n",
      "free          0.392\n",
      "starter       0.302\n",
      "pro           0.206\n",
      "enterprise    0.100\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Cancellation reasons (for churned):\n",
      "cancellation_reason\n",
      "switched_competitor    0.181\n",
      "other                  0.178\n",
      "no_longer_needed       0.177\n",
      "poor_support           0.171\n",
      "missing_features       0.149\n",
      "too_expensive          0.143\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== Other channel users ===\n",
      "Plan distribution:\n",
      "plan\n",
      "free          0.394\n",
      "starter       0.296\n",
      "pro           0.208\n",
      "enterprise    0.102\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Cancellation reasons (for churned):\n",
      "cancellation_reason\n",
      "too_expensive          0.176\n",
      "no_longer_needed       0.171\n",
      "poor_support           0.170\n",
      "switched_competitor    0.163\n",
      "other                  0.162\n",
      "missing_features       0.159\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "=== Feature usage for paid_campaign_x users ===\n",
      "            count       mean        std  min   25%   50%   75%   max\n",
      "status                                                              \n",
      "active     1364.0  29.318915  15.461885  5.0  17.0  27.0  39.0  73.0\n",
      "cancelled  1105.0  12.818100   7.332455  5.0   6.0  11.0  18.0  35.0\n",
      "\n",
      "=== Churn rate by signup cohort ===\n",
      "signup_month\n",
      "2024-01    0.240887\n",
      "2024-02    0.241247\n",
      "2024-03    0.214657\n",
      "2024-04    0.243094\n",
      "2024-05    0.250000\n",
      "Freq: M, dtype: float64\n",
      "\n",
      "=== paid_campaign_x share by cohort ===\n",
      "signup_month\n",
      "2024-01    0.257635\n",
      "2024-02    0.233042\n",
      "2024-03    0.243499\n",
      "2024-04    0.250126\n",
      "2024-05    0.249018\n",
      "Freq: M, dtype: float64\n",
      "\n",
      "2026/02/11 12:51:00 INFO dspy.predict.rlm: RLM iteration 5/15\n",
      "Reasoning: I now have a comprehensive picture. Let me summarize the key findings and prepare my final submission:\n",
      "\n",
      "1. **Overall churn rate**: 23.77% (0.2377)\n",
      "2. **Worst channel**: `paid_campaign_x` with 44.8% churn rate (nearly 3x the ~15% of other channels)\n",
      "3. **Key findings**:\n",
      "   - paid_campaign_x accounts for 1105 of 2377 total churned users (46.5% of all churn despite being only 24.7% of users)\n",
      "   - Churned users have dramatically lower feature engagement (avg 12.6 events vs 29 for active users)\n",
      "   - Advanced reports usage is the strongest differentiator (4.29x higher for retained users)\n",
      "   - paid_campaign_x churned users show similar low engagement patterns (avg 12.8 events)\n",
      "   - The plan distribution is similar across channels, so it's not a plan issue\n",
      "   - Cancellation reasons for paid_campaign_x are evenly distributed but slightly higher on \"switched_competitor\" and \"no_longer_needed\"\n",
      "   - Cohort churn rates are relatively stable (21-25%), suggesting this is an ongoing channel quality issue not a temporal one\n",
      "\n",
      "Let me formulate the final outputs and submit.\n",
      "Code:\n",
      "```python\n",
      "overall_churn_rate = 0.2377\n",
      "\n",
      "worst_channel = \"paid_campaign_x\"\n",
      "\n",
      "key_finding = (\n",
      "    \"The primary driver of user retention decline is the paid_campaign_x acquisition channel, which has a 44.8% churn rate — \"\n",
      "    \"nearly 3x higher than all other channels (~15%). This single channel accounts for 46.5% of all churned users despite \"\n",
      "    \"representing only 24.7% of the user base. Churned users across all channels show dramatically lower feature engagement \"\n",
      "    \"(avg 12.6 events vs 29.0 for active users), with advanced_reports usage being the strongest predictor of retention \"\n",
      "    \"(4.29x higher usage among retained users). The paid_campaign_x channel likely attracts low-intent users who never \"\n",
      "    \"develop habitual product usage, leading to churn across all cancellation reasons roughly equally.\"\n",
      ")\n",
      "\n",
      "recommendations = (\n",
      "    \"1. IMMEDIATE: Audit and restructure paid_campaign_x — review ad targeting, messaging, and landing pages to attract \"\n",
      "    \"higher-intent users. Consider pausing the campaign until ROI improves, as it drives nearly half of all churn. \"\n",
      "    \"2. ONBOARDING: Implement an activation program focused on driving usage of advanced_reports, the strongest retention \"\n",
      "    \"predictor (4.29x usage gap). Create guided onboarding flows that get new users to generate their first report within \"\n",
      "    \"the first session. \"\n",
      "    \"3. EARLY WARNING: Build a churn risk model using feature engagement thresholds — users with fewer than 15 events \"\n",
      "    \"in their first month should trigger proactive outreach (CSM contact, tutorials, incentives). \"\n",
      "    \"4. CHANNEL OPTIMIZATION: Shift acquisition budget from paid_campaign_x toward referral and paid_campaign_y channels, \"\n",
      "    \"which have the lowest churn rates (~15%) and likely attract users with stronger product-market fit. \"\n",
      "    \"5. ENGAGEMENT: Promote integration_setup and team_invite features early, as collaborative and integrated users \"\n",
      "    \"show 2x+ higher retention rates.\"\n",
      ")\n",
      "\n",
      "print(\"Overall churn rate:\", overall_churn_rate)\n",
      "print(\"Worst channel:\", worst_channel)\n",
      "print(\"Key finding:\", key_finding[:100], \"...\")\n",
      "print(\"Recommendations:\", recommendations[:100], \"...\")\n",
      "\n",
      "SUBMIT(overall_churn_rate, worst_channel, key_finding, recommendations)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Configure your LM (swap the model as needed)\n",
    "lm = dspy.LM(\"openrouter/anthropic/claude-opus-4.6\", cache=False)\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Create the RLM analyzer\n",
    "analyzer = dspy.RLM(\n",
    "    CohortRetentionAnalysis,\n",
    "    max_iterations=15,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Run the analysis\n",
    "result = analyzer(\n",
    "    users=users,\n",
    "    events=events,\n",
    "    subscriptions=subscriptions,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspect the Results\n",
    "\n",
    "The RLM returns structured outputs matching the signature's output fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RLM completed in 5 iterations\n",
      "Discovered Churn Rate: 23.8%\n",
      "Expected: 23.8%\n",
      "Worst Channel:     paid_campaign_x\n",
      "=====================\n",
      "\n",
      "Key Finding:\n",
      "  The primary driver of user retention decline is the paid_campaign_x acquisition\n",
      "  channel, which has a 44.8% churn rate — nearly 3x higher than all other channels\n",
      "  (~15%). This single channel accounts for 46.5% of all churned users despite\n",
      "  representing only 24.7% of the user base. Churned users across all channels show\n",
      "  dramatically lower feature engagement (avg 12.6 events vs 29.0 for active\n",
      "  users), with advanced_reports usage being the strongest predictor of retention\n",
      "  (4.29x higher usage among retained users). The paid_campaign_x channel likely\n",
      "  attracts low-intent users who never develop habitual product usage, leading to\n",
      "  churn across all cancellation reasons roughly equally.\n",
      "\n",
      "Recommendations:\n",
      "  1. IMMEDIATE: Audit and restructure paid_campaign_x — review ad targeting,\n",
      "  messaging, and landing pages to attract higher-intent users. Consider pausing\n",
      "  the campaign until ROI improves, as it drives nearly half of all churn. 2.\n",
      "  ONBOARDING: Implement an activation program focused on driving usage of\n",
      "  advanced_reports, the strongest retention predictor (4.29x usage gap). Create\n",
      "  guided onboarding flows that get new users to generate their first report within\n",
      "  the first session. 3. EARLY WARNING: Build a churn risk model using feature\n",
      "  engagement thresholds — users with fewer than 15 events in their first month\n",
      "  should trigger proactive outreach (CSM contact, tutorials, incentives). 4.\n",
      "  CHANNEL OPTIMIZATION: Shift acquisition budget from paid_campaign_x toward\n",
      "  referral and paid_campaign_y channels, which have the lowest churn rates (~15%)\n",
      "  and likely attract users with stronger product-market fit. 5. ENGAGEMENT:\n",
      "  Promote integration_setup and team_invite features early, as collaborative and\n",
      "  integrated users show 2x+ higher retention rates.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap80(text):\n",
    "    return \"\\n  \".join(textwrap.wrap(text, width=80))\n",
    "\n",
    "print(f\"\\nRLM completed in {len(result.trajectory)} iterations\")\n",
    "print(f\"Discovered Churn Rate: {result.overall_churn_rate:.1%}\")\n",
    "print(\"Expected: 23.8%\")\n",
    "print(f\"Worst Channel:     {result.worst_channel}\")\n",
    "print(\"=====================\")\n",
    "print(f\"\\nKey Finding:\\n  {wrap80(result.key_finding)}\")\n",
    "print(f\"\\nRecommendations:\\n  {wrap80(result.recommendations)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Output (Claude Opus 4.6, 6 iterations, ~90s)\n",
    "\n",
    "```\n",
    "Overall Churn Rate: 23.8%\n",
    "Worst Channel:     paid_campaign_x\n",
    "\n",
    "Key Finding:\n",
    "  The primary driver of churn is low adoption of the 'advanced_reports' feature,\n",
    "  especially among paid_campaign_x users. Only 48.8% of paid_campaign_x users\n",
    "  adopt advanced_reports (vs ~85% for other channels), and those who don't have\n",
    "  an 87.5% churn rate. Meanwhile, 100% of paid_campaign_x users who used\n",
    "  advanced_reports remained active.\n",
    "\n",
    "Recommendations:\n",
    "  1. Revamp paid_campaign_x onboarding: Create a dedicated flow that guides\n",
    "     users to discover and use advanced_reports within their first session.\n",
    "  2. Re-evaluate paid_campaign_x targeting and messaging: The campaign attracts\n",
    "     users who churn at 3x the rate of other channels.\n",
    "  3. Implement an advanced_reports activation trigger for all new users who\n",
    "     haven't used it within 7 days.\n",
    "\n",
    "RLM completed in 6 iterations\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examine the Trajectory\n",
    "\n",
    "The `trajectory` attribute contains each iteration's reasoning and code. This lets you see exactly how the model explored the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Iteration 1\n",
      "============================================================\n",
      "Reasoning: Let me start by exploring the data to understand its structure, then compute churn rates, segment by channel, and analyze feature usage differences....\n",
      "Code:\n",
      "# First, let's explore the data\n",
      "print(\"=== USERS ===\")\n",
      "print(users.shape)\n",
      "print(users.head())\n",
      "print(\"\\nAcquisition channels:\", users['acquisition_channel'].value_counts())\n",
      "print(\"\\nPlan at signup:\", users['plan_at_signup'].value_counts())\n",
      "\n",
      "print(\"\\n=== SUBSCRIPTIONS ===\")\n",
      "print(subscriptions.shape)\n",
      "...\n",
      "\n",
      "============================================================\n",
      "Iteration 2\n",
      "============================================================\n",
      "Reasoning: Now I have a good overview. Let me compute:\n",
      "1. Overall churn rate\n",
      "2. Churn rate by acquisition channel to find the worst channel\n",
      "3. Feature usage comparison between retained and churned users\n",
      "4. Cohor...\n",
      "Code:\n",
      "# Overall churn rate\n",
      "total_users = len(subscriptions)\n",
      "churned_users = len(subscriptions[subscriptions['status'] == 'cancelled'])\n",
      "overall_churn_rate = churned_users / total_users\n",
      "print(f\"Overall churn rate: {overall_churn_rate:.4f} ({overall_churn_rate*100:.2f}%)\")\n",
      "\n",
      "# Merge users with subscriptions t...\n",
      "\n",
      "============================================================\n",
      "Iteration 3\n",
      "============================================================\n",
      "Reasoning: Great findings so far! `paid_campaign_x` has a dramatically higher churn rate (44.8%) compared to others. Let me now:\n",
      "1. Compare feature usage between retained and churned users\n",
      "2. Look at cohort-base...\n",
      "Code:\n",
      "# Cancellation reasons by channel\n",
      "cancelled = merged[merged['status'] == 'cancelled']\n",
      "channel_reasons = cancelled.groupby('acquisition_channel')['cancellation_reason'].value_counts().unstack(fill_value=0)\n",
      "print(\"Cancellation reasons by channel:\")\n",
      "print(channel_reasons)\n",
      "\n",
      "# Feature usage comparison: r...\n",
      "\n",
      "============================================================\n",
      "Iteration 4\n",
      "============================================================\n",
      "Reasoning: I have strong findings so far. Let me dig a bit deeper into paid_campaign_x to understand why it has such high churn, look at cohort retention, and then formulate my final answers.\n",
      "\n",
      "Key findings so fa...\n",
      "Code:\n",
      "# Deeper look at paid_campaign_x vs others\n",
      "pcx = merged[merged['acquisition_channel'] == 'paid_campaign_x']\n",
      "others = merged[merged['acquisition_channel'] != 'paid_campaign_x']\n",
      "\n",
      "print(\"=== paid_campaign_x users ===\")\n",
      "print(\"Plan distribution:\")\n",
      "print(pcx['plan'].value_counts(normalize=True).round(3))...\n",
      "\n",
      "============================================================\n",
      "Iteration 5\n",
      "============================================================\n",
      "Reasoning: I now have a comprehensive picture. Let me summarize the key findings and prepare my final submission:\n",
      "\n",
      "1. **Overall churn rate**: 23.77% (0.2377)\n",
      "2. **Worst channel**: `paid_campaign_x` with 44.8% ch...\n",
      "Code:\n",
      "overall_churn_rate = 0.2377\n",
      "\n",
      "worst_channel = \"paid_campaign_x\"\n",
      "\n",
      "key_finding = (\n",
      "    \"The primary driver of user retention decline is the paid_campaign_x acquisition channel, which has a 44.8% churn rate — \"\n",
      "    \"nearly 3x higher than all other channels (~15%). This single channel accounts for 46.5% ...\n"
     ]
    }
   ],
   "source": [
    "for i, step in enumerate(result.trajectory):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Iteration {i+1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Reasoning: {step.get('reasoning', '')[:200]}...\")\n",
    "    print(f\"Code:\\n{step.get('code', '')[:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth\n",
    "\n",
    "For reference, here's what's embedded in the synthetic data:\n",
    "\n",
    "| Metric | Expected Value |\n",
    "|--------|---------------|\n",
    "| Overall churn rate | ~23.8% |\n",
    "| Worst channel | `paid_campaign_x` (~45% churn) |\n",
    "| Key behavioral signal | Churned `paid_campaign_x` users never use `advanced_reports` |\n",
    "\n",
    "The churn probabilities by channel:\n",
    "- `paid_campaign_x`: ~45% (3x base)\n",
    "- `social`: ~22.5% (1.5x base)\n",
    "- `organic`, `referral`, `paid_campaign_y`: ~15% (base)\n",
    "\n",
    "The behavioral signal: churned users from `paid_campaign_x` are excluded from the `advanced_reports` feature in the event generation. This means the RLM needs to cross-reference feature usage against churn status against acquisition channel to find the root cause — a multi-hop analytical join."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
