{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys; sys.path.append('/future/u/okhattab/repos/public/tmp/dspy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../../docs/images/DSPy8.png\" alt=\"DSPy7 Image\" height=\"150\"/>\n",
    "\n",
    "## Guide: **Language Models**\n",
    "\n",
    "[<img align=\"center\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" />](https://colab.research.google.com/github/stanfordnlp/dspy/blob/main/docs/guides/signatures.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Recap\n",
    "\n",
    "This guide assumes you followed the [intro tutorial]() to build your first few DSPy programs.\n",
    "\n",
    "Remember that a **DSPy program** is just Python code that calls one or more DSPy modules, like `dspy.Predict` or `dspy.ChainOfThought`, to use LMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Short Intro to LMs in DSPy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install `dspy-ai` if needed.\n",
    "\n",
    "try: import dspy\n",
    "except ImportError:\n",
    "    %pip install dspy-ai\n",
    "    import dspy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Supported LM clients.\n",
    "\n",
    "#### Remote LMs.\n",
    "\n",
    "These models are managed services. You just need to sign up and obtain an API key.\n",
    "\n",
    "1. `dspy.OpenAI` for GPT-3.5 and GPT-4.\n",
    "\n",
    "2. `dspy.Cohere`\n",
    "\n",
    "3. `dspy.Anyscale` for hosted Llama2 models.\n",
    "\n",
    "\n",
    "\n",
    "#### Local LMs.\n",
    "\n",
    "You need to host these models on your own GPU(s). Below, we include pointers for how to do that.\n",
    "\n",
    "4. `dspy.HFClientTGI`: for HuggingFace models through the Text Generation Inference (TGI) system. [Tutorial: How do I install and launch the TGI server?](language_model_details/launching_tgi.md)\n",
    "\n",
    "5. `dspy.HFClientVLLM`: for HuggingFace models through vLLM. [Tutorial: How do I install and launch the vLLM server?](language_model_details/launching_vllm.md)\n",
    "\n",
    "6. `dspy.HFModel` (experimental)\n",
    "\n",
    "7. `dspy.Ollama` (experimental)\n",
    "\n",
    "8. `dspy.ChatModuleClient` (experimental): [How do I install and use MLC?](language_model_details/launching_mlc.md)\n",
    "\n",
    "\n",
    "\n",
    "If there are other clients you want added, let us know!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Setting up the LM client.\n",
    "\n",
    "You can just call the constructor that connects to the LM. Then, use `dspy.configure` to declare this as the default LM.\n",
    "\n",
    "For example, for OpenAI, you can do it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add a graceful line for OPENAI_API_KEY.\n",
    "\n",
    "gpt3_turbo = dspy.OpenAI(model='gpt-3.5-turbo-1106', max_tokens=300)\n",
    "gpt4_turbo = dspy.OpenAI(model='gpt-4-1106-preview', max_tokens=300)\n",
    "\n",
    "# cohere = dspy.Cohere(...)\n",
    "# anyscale = dspy.Anyscale(...)\n",
    "# tgi_llama2 = dspy.HFClientTGI(model=\"meta-llama/Llama-2-7b-hf\", port=8080, url=\"http://localhost\")\n",
    "\n",
    "dspy.configure(lm=gpt3_turbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Using a different LM within a code block.\n",
    "\n",
    "The default LM above is GPT-3.5, `gpt3_turbo`. What if I want to run a piece of code with, say, GPT-4 or LLama-2?\n",
    "\n",
    "Instead of changing the default LM, you can just change it inside a block of code.\n",
    "\n",
    "**Tip:** Using `dspy.configure` and `dspy.context` is thread-safe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The castle David Gregory inherited has 7 floors.\n",
      "The number of floors in the castle David Gregory inherited cannot be determined with the information provided.\n"
     ]
    }
   ],
   "source": [
    "qa = dspy.ChainOfThought('question -> answer')\n",
    "\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "print(response.answer)\n",
    "\n",
    "with dspy.context(lm=gpt4_turbo):\n",
    "    response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "    print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Tips and Tricks.\n",
    "\n",
    "In DSPy, all LM calls are cached. If you repeat the same call, you will get the same outputs. (If you change the inputs or configurations, you will get new outputs.)\n",
    "\n",
    "To generate 5 outputs, you can use `n=5` in the module constructor, or pass `config=dict(n=5)` when invoking the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The specific number of floors in David Gregory's inherited castle is not provided here, so further research would be needed to determine the answer.\",\n",
       " 'The castle David Gregory inherited has 4 floors.',\n",
       " 'The castle David Gregory inherited has 5 floors.',\n",
       " 'David Gregory inherited 10 floors in the castle.',\n",
       " 'The castle David Gregory inherited has 5 floors.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = dspy.ChainOfThought('question -> answer', n=5)\n",
    "\n",
    "response = qa(question=\"How many floors are in the castle David Gregory inherited?\")\n",
    "response.completions.answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just call `qa(...)` in a loop with the same input, it will always return the same value! That's by design.\n",
    "\n",
    "To loop and generate one output at a time with the same input, bypass the cache by making sure each request is (slightly) unique, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The specific number of floors in David Gregory's inherited castle is not provided here, so further research would be needed to determine the answer.\n",
      "It is not possible to determine the exact number of floors in the castle David Gregory inherited without specific information about the castle's layout and history.\n",
      "The castle David Gregory inherited has 5 floors.\n",
      "We need more information to determine the number of floors in the castle David Gregory inherited.\n",
      "The castle David Gregory inherited has a total of 6 floors.\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    response = qa(question=\"How many floors are in the castle David Gregory inherited?\", config=dict(temperature=0.7+0.0001*idx))\n",
    "    print(response.answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_nov2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
