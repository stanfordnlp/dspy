{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: This is not a cookbook\n",
    "\n",
    "This is a testing notebook in order to make sure that multimodal works.\n",
    "\n",
    "Cookbook is on the way, but if you have particular ideas, message @isaac on discord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.datasets import DataLoader\n",
    "from dspy.evaluate.metrics import answer_exact_match\n",
    "from typing import List\n",
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "import dotenv\n",
    "import litellm\n",
    "\n",
    "litellm.suppress_debug_info = True\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "def debug_exact_match(example, pred, trace=None, frac=1.0):\n",
    "    print(example.inputs())\n",
    "    print(example.answer)\n",
    "    print(pred)\n",
    "    # print(trace)\n",
    "    # print(frac)\n",
    "    return answer_exact_match(example, pred, trace, frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vllm serve Qwen/Qwen2-VL-7B-Instruct --trust-remote-code --limit-mm-per-prompt image=16 --seed 42 --pipeline-parallel-size 2\n",
    "qwen_lm = dspy.LM(model=\"openai/Qwen/Qwen2-VL-7B-Instruct\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "haiku_lm = dspy.LM(model=\"anthropic/claude-3-haiku-20240307\", max_tokens=4096)\n",
    "# vllm serve meta-llama/Llama-3.2-11B-Vision-Instruct --trust-remote-code --limit-mm-per-prompt image=16 --seed 42 --enforce-eager --max-num-seqs 48\n",
    "llama_lm = dspy.LM(model=\"openai/meta-llama/Llama-3.2-11B-Vision-Instruct\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "internlm_lm = dspy.LM(model=\"openai/OpenGVLab/InternVL2-8B\", api_base=\"http://localhost:8000/v1\", api_key=\"sk-fake-key\", max_tokens=5000)\n",
    "gpt_lm = dspy.LM(model=\"openai/gpt-4o-mini\", max_tokens=5000)\n",
    "all_lms = [qwen_lm, haiku_lm, llama_lm, gpt_lm]\n",
    "\n",
    "dspy.settings.configure(lm=qwen_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogPictureSignature(dspy.Signature):\n",
    "    \"\"\"Answer the question based on the image.\"\"\"\n",
    "    image: dspy.Image = dspy.InputField()\n",
    "    question: str = dspy.InputField()\n",
    "    answer: str = dspy.OutputField()\n",
    "\n",
    "class DogPicture(dspy.Module):\n",
    "    def __init__(self) -> None:\n",
    "        self.predictor = dspy.ChainOfThought(DogPictureSignature)\n",
    "    \n",
    "    def __call__(self, **kwargs):\n",
    "        return self.predictor(**kwargs)\n",
    "\n",
    "dog_picture = DogPicture()\n",
    "\n",
    "example = dspy.Example(image=dspy.Image.from_url(\"https://i.pinimg.com/564x/78/f9/6d/78f96d0314d39a1b8a849005123e166d.jpg\"), question=\"What is the breed of the dog in the image?\").with_inputs(\"image\", \"question\")\n",
    "print(dog_picture(**example.inputs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen_lm.inspect_history()\n",
    "import rich\n",
    "rich.print(qwen_lm.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = dspy.Predict(\"question -> answer\")\n",
    "p(question=\"What is the capital of France?\")\n",
    "qwen_lm.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "input_keys = tuple([f\"image_{i}\" for i in range(1, 3)] + [\"question\", \"options\"])\n",
    "subsets = ['Accounting', 'Agriculture', 'Architecture_and_Engineering', 'Art', 'Art_Theory', 'Basic_Medical_Science', 'Biology', 'Chemistry', 'Clinical_Medicine', 'Computer_Science', 'Design', 'Diagnostics_and_Laboratory_Medicine', 'Economics', 'Electronics', 'Energy_and_Power', 'Finance', 'Geography', 'History', 'Literature', 'Manage', 'Marketing', 'Materials', 'Math', 'Mechanical_Engineering', 'Music', 'Pharmacy', 'Physics', 'Psychology', 'Public_Health', 'Sociology']\n",
    "\n",
    "devset = []\n",
    "valset = []\n",
    "with ThreadPoolExecutor(max_workers=len(subsets)) as executor:\n",
    "    def load_dataset(subset_index_subset):\n",
    "        subset_index, subset = subset_index_subset\n",
    "        dataset = DataLoader().from_huggingface(\"MMMU/MMMU\", subset, split=[\"dev\", \"validation\"], input_keys=input_keys)\n",
    "        return subset_index, dataset[\"dev\"], dataset[\"validation\"]\n",
    "    \n",
    "    results = list(executor.map(load_dataset, enumerate(subsets)))\n",
    "    \n",
    "    results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    for _, dev, val in results:\n",
    "        devset.extend(dev)\n",
    "        valset.extend(val)\n",
    "\n",
    "print(len(devset))\n",
    "print(len(valset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def count_images(dataset):\n",
    "    image_counts = {i: 0 for i in range(6)}  # Initialize counts for 0 to 2 images\n",
    "    for example in dataset:\n",
    "        count = sum(1 for key in example.inputs().keys() if key.startswith('image_') and example.inputs()[key] is not None)\n",
    "        image_counts[count] += 1\n",
    "    return image_counts\n",
    "\n",
    "def count_multiple_choice_questions(dataset):\n",
    "    return sum(1 for example in dataset if example[\"question_type\"] == \"multiple-choice\")\n",
    "max_images = 5\n",
    "\n",
    "num_images = 1\n",
    "\n",
    "devset_filtered = [example for example in devset if sum(1 for key in example.inputs().keys() if key.startswith('image_') and example.inputs()[key] is not None) == num_images]\n",
    "valset_filtered = [example for example in valset if sum(1 for key in example.inputs().keys() if key.startswith('image_') and example.inputs()[key] is not None) == num_images]\n",
    "\n",
    "devset_filtered = [example for example in devset_filtered if example[\"question_type\"] == \"multiple-choice\"]\n",
    "valset_filtered = [example for example in valset_filtered if example[\"question_type\"] == \"multiple-choice\"]\n",
    "\n",
    "def update_example_image_key(example):\n",
    "    example_copy = example.copy()\n",
    "    example_copy[\"image\"] = example_copy[\"image_1\"]\n",
    "    return example_copy.with_inputs(*example.inputs().keys(), \"image\")\n",
    "\n",
    "\n",
    "\n",
    "devset_filtered = list(map(update_example_image_key, devset_filtered))\n",
    "valset_filtered = list(map(update_example_image_key, valset_filtered))\n",
    "\n",
    "devset_image_counts = count_images(devset_filtered)\n",
    "valset_image_counts = count_images(valset_filtered)\n",
    "\n",
    "devset_multiple_choice_questions = count_multiple_choice_questions(devset_filtered)\n",
    "valset_multiple_choice_questions = count_multiple_choice_questions(valset_filtered)\n",
    "\n",
    "print(\"Image counts in devset:\")\n",
    "for count, num_examples in devset_image_counts.items():\n",
    "    print(f\"{count} image(s): {num_examples} examples\")\n",
    "\n",
    "print(\"\\nImage counts in valset:\")\n",
    "for count, num_examples in valset_image_counts.items():\n",
    "    print(f\"{count} image(s): {num_examples} examples\")\n",
    "\n",
    "print(\"\\nMultiple choice questions in devset:\")\n",
    "print(devset_multiple_choice_questions, \"out of\", len(devset_filtered))\n",
    "print(\"\\nMultiple choice questions in valset:\")\n",
    "print(valset_multiple_choice_questions, \"out of\", len(valset_filtered))\n",
    "\n",
    "def convert_multiple_choice_to_letter(dataset):\n",
    "    new_dataset = []\n",
    "    for example in dataset:\n",
    "        if example[\"question_type\"] == \"multiple-choice\":\n",
    "            # print(example[\"options\"])\n",
    "            options = ast.literal_eval(example[\"options\"])\n",
    "            example[\"choices\"] = str([chr(65 + i) + \". \" + option for i, option in enumerate(options)])\n",
    "        else:\n",
    "            example[\"choices\"] = str(ast.literal_eval(example[\"options\"]))\n",
    "            if example[\"choices\"] == []:\n",
    "                example[\"choices\"] = \"Free response\"\n",
    "\n",
    "        updated_example = example.with_inputs(*example.inputs().keys(), \"choices\")\n",
    "        new_dataset.append(updated_example)\n",
    "    return new_dataset\n",
    "\n",
    "print(devset_filtered[0])\n",
    "updated_devset = convert_multiple_choice_to_letter(devset_filtered)\n",
    "print(updated_devset[0])\n",
    "updated_valset = convert_multiple_choice_to_letter(valset_filtered)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Literal\n",
    "class MMMUSignature(dspy.Signature):\n",
    "    \"\"\"Answer with the letter of the correct answer.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField()\n",
    "    image: dspy.Image = dspy.InputField()\n",
    "    choices: List[str] = dspy.InputField()\n",
    "    answer: Literal[\"A\", \"B\", \"C\", \"D\", \"E\"] = dspy.OutputField()\n",
    "\n",
    "class MMMUModule(dspy.Module):\n",
    "    def __init__(self, cot=True):\n",
    "        super().__init__()\n",
    "        if cot:\n",
    "            self.predictor = dspy.ChainOfThought(MMMUSignature)\n",
    "        else:\n",
    "            self.predictor = dspy.Predict(MMMUSignature)\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        # Clean up predictions\n",
    "        prediction = self.predictor(**kwargs)\n",
    "        # Multiple choice case\n",
    "        if \"A.\" in kwargs[\"choices\"]:\n",
    "            # regex to extract A, B, C, or D, or E\n",
    "            answer = re.search(r'[A-E]', prediction[\"answer\"])\n",
    "            if not answer:\n",
    "                answer = prediction[\"answer\"]\n",
    "            else:\n",
    "                answer = answer.group(0)\n",
    "            prediction[\"answer\"] = answer\n",
    "        # Free response case\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sample_input = updated_devset[0]\n",
    "# print(sample_input.inputs())\n",
    "# print(encode_image(sample_input.inputs()[\"image_1\"]))\n",
    "mmmu = MMMUModule()\n",
    "print(sample_input.inputs())\n",
    "print(mmmu(**sample_input.inputs()))\n",
    "print(sample_input.answer)\n",
    "\n",
    "evaluate_mmmu = Evaluate(metric=answer_exact_match, num_threads=50, devset=updated_valset, display_progress=True, max_errors=500, return_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_lm(lm, cot=False):\n",
    "    if lm.model == \"openai/gpt-4o-mini\":\n",
    "        num_threads = 10\n",
    "    else:\n",
    "        num_threads = 30\n",
    "    evaluate_mmmu = Evaluate(metric=answer_exact_match, num_threads=num_threads, devset=updated_valset, display_progress=True, max_errors=500, return_outputs=True)\n",
    "    mmmu = MMMUModule(cot=cot)\n",
    "    with dspy.context(lm=lm):\n",
    "        scores, outputs = evaluate_mmmu(mmmu)\n",
    "        num_bad_format = sum(1 for example in outputs if example[1].get(\"answer\", None) is None)\n",
    "        return scores, num_bad_format\n",
    "\n",
    "res1 = test_lm(qwen_lm)\n",
    "res1_cot = test_lm(qwen_lm, cot=True)\n",
    "# test_lm(haiku_lm)\n",
    "# test_lm(llama_lm)\n",
    "# res1 = test_lm(internlm_lm)\n",
    "# res2 = test_lm(gpt_lm)\n",
    "# res2_cot = test_lm(gpt_lm, cot=True)\n",
    "# Results:\n",
    "# MMMU Val(single image only, multiple choice only), N=805\n",
    "# Temp 0, max_tokens=5k\n",
    "\n",
    "# 4o-mini:\n",
    "# Reported: 59.4\n",
    "\n",
    "# Measured (cot, predict): 60.0, 56.4\n",
    "# Num bad format (cot, predict): 0, 1\n",
    "\n",
    "# qwen-7b\n",
    "# Reported: 54.1\n",
    "# Measured (cot, predict): 49.0, 49.69\n",
    "# Num bad format (cot, predict): 17, 0\n",
    "print(\"MMMU Validation Set (single image only, multiple choice only), N=805\")\n",
    "print(\"Temp 0, max_tokens=5k\")\n",
    "print(\"qwen-7b\")\n",
    "print(\"Reported:\", 54.1)\n",
    "print(\"Measured (cot, predict):\", f\"{res1_cot[0]:.1f}, {res1[0]:.2f}\")\n",
    "print(\"Num bad format (cot, predict):\", f\"{res1_cot[1]}, {res1[1]}\")\n",
    "# print()\n",
    "# print(\"gpt-4o-mini\")\n",
    "# print(\"Reported:\", 59.4)\n",
    "# print(\"Measured (cot, predict):\", f\"{res2_cot[0]:.1f}, {res2[0]:.2f}\") \n",
    "# print(\"Num bad format (cot, predict):\", f\"{res2_cot[1]}, {res2[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, outputs = evaluate_mmmu(mmmu)\n",
    "# lm.inspect_history()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter([outputs[i][1].get(\"answer\", \"nothing returned\") for i in range(len(outputs))])\n",
    "non_letters = sum([1 for output in outputs if output[1].get(\"answer\", \"nothing returned\") not in [\"A\", \"B\", \"C\", \"D\"]])\n",
    "print(c)\n",
    "print(non_letters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_correct = sum(outputs[i][2] for i in range(len(outputs)) if outputs[i][0][\"question_type\"] == \"multiple-choice\")\n",
    "total_mc = sum(1 for example in outputs if example[0][\"question_type\"] == \"multiple-choice\")\n",
    "print(mc_correct, total_mc)\n",
    "print(mc_correct / total_mc)\n",
    "print(sum(outputs[i][1].get(\"answer\", None) is None for i in range(len(outputs))))\n",
    "\n",
    "# Note: Run above here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that multiple images work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "def set_image_to_black_square(example, key):\n",
    "    example_copy = example.copy()\n",
    "    example_copy[key] = PIL.Image.open(\"black_image_300x300.png\")\n",
    "    return example_copy.with_inputs(*example.inputs().keys())\n",
    "\n",
    "print(updated_devset[0][\"image_1\"])\n",
    "print(updated_devset[0][\"image_2\"])\n",
    "examples_no_image_1 = list(map(lambda x: set_image_to_black_square(x, \"image_1\"), updated_valset))\n",
    "print(examples_no_image_1[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "print(examples_no_image_1[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "examples_no_image_2 = list(map(lambda x: set_image_to_black_square(x, \"image_2\"), updated_valset))\n",
    "print(examples_no_image_2[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "print(examples_no_image_2[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "\n",
    "examples_no_actual_image = list(map(lambda x: set_image_to_black_square(x, \"image_1\"), updated_valset))\n",
    "examples_no_actual_image = list(map(lambda x: set_image_to_black_square(x, \"image_2\"), examples_no_actual_image))\n",
    "print(examples_no_actual_image[0][\"image_1\"] == PIL.Image.open(\"black_image_300x300.png\"))\n",
    "print(examples_no_actual_image[0][\"image_2\"] == PIL.Image.open(\"black_image_300x300.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmmu = MMMUModule()\n",
    "print(examples_no_image_1[0].inputs())\n",
    "print(mmmu(**examples_no_image_1[0].inputs()))\n",
    "\n",
    "print(examples_no_image_2[0].inputs())\n",
    "print(mmmu(**examples_no_image_2[0].inputs()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = evaluate_mmmu(mmmu, devset=updated_valset)\n",
    "no_image_1 = evaluate_mmmu(mmmu, devset=examples_no_image_1)\n",
    "no_image_2 = evaluate_mmmu(mmmu, devset=examples_no_image_2)\n",
    "no_actual_image = evaluate_mmmu(mmmu, devset=examples_no_actual_image)\n",
    "print(\"Testing on MMMU validation set (N=\", len(updated_valset), \")\")\n",
    "print(\"Score with both images:\", normal)\n",
    "print(\"Score with image_1 set to black square:\", no_image_1)\n",
    "print(\"Score with image_2 set to black square:\", no_image_2)\n",
    "print(\"Score with both images set to black squares:\", no_actual_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Test with bootstrapped examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure that JPGs work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert images to JPGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "\n",
    "def convert_to_jpg(example):\n",
    "    example_copy = example.copy()\n",
    "    for key in ['image_1', 'image_2']:\n",
    "        if key in example_copy and isinstance(example_copy[key], Image.Image):\n",
    "            # Convert to RGB mode (in case it's not already)\n",
    "            img = example[key].convert('RGB')\n",
    "            \n",
    "            # Save as JPG in memory\n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='JPEG')\n",
    "            buffer.seek(0)\n",
    "            \n",
    "            # Load the JPG back as a PIL Image\n",
    "            example_copy[key] = Image.open(buffer)\n",
    "    \n",
    "    return example_copy.with_inputs(*example.inputs().keys())\n",
    "\n",
    "# Convert all images in the dataset to JPG\n",
    "examples_jpg = list(map(convert_to_jpg, updated_valset))\n",
    "\n",
    "# Verify conversion\n",
    "print(\"Original image format:\", updated_valset[0]['image_1'].format)\n",
    "print(\"Converted image format:\", examples_jpg[0]['image_1'].format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_jpg = list(map(convert_to_jpg, updated_valset))\n",
    "examples_no_image_1_jpg = list(map(lambda x: convert_to_jpg(x), examples_no_image_1))\n",
    "examples_no_image_2_jpg = list(map(lambda x: convert_to_jpg(x), examples_no_image_2))\n",
    "examples_no_actual_image_jpg = list(map(lambda x: convert_to_jpg(x), examples_no_actual_image))\n",
    "\n",
    "mmmu = MMMUModule()\n",
    "print(examples_no_image_1_jpg[0].inputs())\n",
    "print(mmmu(**examples_no_image_1_jpg[0].inputs()))\n",
    "print(examples_no_image_1_jpg[0][\"image_1\"].format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = evaluate_mmmu(mmmu, devset=examples_jpg)\n",
    "no_image_1 = evaluate_mmmu(mmmu, devset=examples_no_image_1_jpg)\n",
    "no_image_2 = evaluate_mmmu(mmmu, devset=examples_no_image_2_jpg)\n",
    "no_actual_image = evaluate_mmmu(mmmu, devset=examples_no_actual_image_jpg)\n",
    "print(\"Testing on MMMU validation set (N=\", len(updated_valset), \")\")\n",
    "print(\"Score with both images:\", normal)\n",
    "print(\"Score with image_1 set to black square:\", no_image_1)\n",
    "print(\"Score with image_2 set to black square:\", no_image_2)\n",
    "print(\"Score with both images set to black squares:\", no_actual_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing that URLs work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "colors = {\n",
    "    \"White\": \"FFFFFF\",\n",
    "    \"Red\": \"FF0000\",\n",
    "    \"Green\": \"00FF00\",\n",
    "    \"Blue\": \"0000FF\",\n",
    "    \"Yellow\": \"FFFF00\",\n",
    "    \"Cyan\": \"00FFFF\",\n",
    "    \"Magenta\": \"FF00FF\",\n",
    "    \"Gray\": \"808080\",\n",
    "    \"Orange\": \"FFA500\",\n",
    "    \"Purple\": \"800080\"\n",
    "}\n",
    "def get_color_image_url(color, file_extension=\"png\"):\n",
    "    return f\"https://placehold.co/300/{colors[color]}/{colors[color]}.{file_extension}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_2_color_image_examples(n):\n",
    "    examples = []\n",
    "    for _ in range(n):\n",
    "        color_1, color_2 = random.sample(list(colors.keys()), 2)\n",
    "        chosen_color = color_1 if random.random() < 0.5 else color_2\n",
    "        chosen_image = \"image_1\" if chosen_color == color_1 else \"image_2\"\n",
    "        example_kwargs = {\n",
    "            \"image_1\": get_color_image_url(color_1),\n",
    "            \"image_2\": get_color_image_url(color_2),\n",
    "            \"question\": f\"What color is {chosen_image}?\",\n",
    "            \"answer\": chosen_color\n",
    "        }\n",
    "        examples.append(dspy.Example(**example_kwargs).with_inputs(\"image_1\", \"image_2\", \"question\"))\n",
    "    return examples\n",
    "\n",
    "examples = generate_random_2_color_image_examples(100)\n",
    "print(examples[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorSignature(dspy.Signature):\n",
    "    \"\"\"Output the color of the designated image.\"\"\"\n",
    "    image_1: dspy.Image = dspy.InputField(desc=\"An image\")\n",
    "    image_2: dspy.Image = dspy.InputField(desc=\"An image\")\n",
    "    question: str = dspy.InputField(desc=\"A question about the image\")\n",
    "    answer: str = dspy.OutputField(desc=\"The color of the designated image\")\n",
    "color_program = dspy.Predict(ColorSignature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(examples[0])\n",
    "print(color_program(**examples[0].inputs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_optimizer = dspy.BootstrapFewShot(metric=answer_exact_match, max_bootstrapped_demos=3, max_labeled_demos=10)\n",
    "smaller_few_shot_optimizer = dspy.BootstrapFewShot(metric=answer_exact_match, max_bootstrapped_demos=1, max_labeled_demos=1)\n",
    "dataset = generate_random_2_color_image_examples(1000)\n",
    "trainset = dataset[:200]\n",
    "validationset = dataset[200:400]\n",
    "evaluate_colors = Evaluate(metric=answer_exact_match, num_threads=300, devset=validationset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_color_program = few_shot_optimizer.compile(color_program, trainset=trainset)\n",
    "compiled_smaller_color_program = smaller_few_shot_optimizer.compile(color_program, trainset=trainset)\n",
    "print(evaluate_colors(color_program))\n",
    "print(evaluate_colors(compiled_color_program))\n",
    "print(evaluate_colors(compiled_smaller_color_program))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compiled_color_program(**validationset[0].inputs()))\n",
    "lm.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO(Isaac): Delete; Archive of old experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader().from_huggingface(\"Alanox/stanford-dogs\", split=\"full\", input_keys=(\"image\",), trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the field from \"image\" to \"image_1\"\n",
    "def rename_field(example, old_name, new_name):\n",
    "    try:\n",
    "        example[new_name] = example[old_name]\n",
    "        del example[old_name]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return example\n",
    "    \n",
    "dog_dataset = list(map(rename_field, dataset, [\"image\"]*len(dataset), [\"image_1\"]*len(dataset)))\n",
    "dog_dataset2 = list(map(rename_field, dog_dataset, [\"target\"]*len(dog_dataset), [\"answer\"]*len(dog_dataset)))\n",
    "dog_dataset3 = list(map(lambda x: x.with_inputs(\"image_1\"), dog_dataset2))\n",
    "dog_dataset = dog_dataset3\n",
    "random.shuffle(dog_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogPictureSignature(dspy.Signature):\n",
    "    \"\"\"Output the dog breed of the dog in the image.\"\"\"\n",
    "    image: dspy.Image = dspy.InputField(desc=\"An image of a dog\")\n",
    "    answer: str = dspy.OutputField(desc=\"The dog breed of the dog in the image\")\n",
    "\n",
    "class DogPicture(dspy.Module):\n",
    "    def __init__(self) -> None:\n",
    "        self.predictor = dspy.ChainOfThought(DogPictureSignature)\n",
    "    \n",
    "    def __call__(self, **kwargs):\n",
    "        return self.predictor(**kwargs)\n",
    "\n",
    "dog_picture = DogPicture()\n",
    "\n",
    "example = dspy.Example(image=dspy.Image.from_url(\"https://i.pinimg.com/564x/78/f9/6d/78f96d0314d39a1b8a849005123e166d.jpg\"))\n",
    "print(dog_picture(**example.inputs()))\n",
    "# print(dog_picture(**dog_dataset[0].inputs()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test inline signature\n",
    "# TODO: Test json adapter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
