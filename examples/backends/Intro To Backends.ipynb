{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8925ac0-3d6a-4200-8cf4-e9f52300a188",
   "metadata": {},
   "source": [
    "# Intro to Backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7783215-31c9-4769-a2c2-65d18b6ad2d1",
   "metadata": {},
   "source": [
    "One note from the Author - Backends are still experimental, while completely functional, there may be some bugs. I would encourage you to test out these Backends, and please file an issue with any inconsistency you find or thoughts/questions you have.\n",
    "\n",
    "Best - kcaverly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb40f45-c6d1-4689-9b4e-be1fc26623b8",
   "metadata": {},
   "source": [
    "## What is a 'Backend'\n",
    "\n",
    "Backends, fundamentally take in a `Signature`, `Example` and misc arguments and produce `Completions`.\n",
    "`Completions` are objects, which return Language Model generations as structured `Signature` aligned objects.\n",
    "\n",
    "Historically, DSPy, has hidden most Backend functionality into a singular backend, exposing only `LM` as the medium to augment generation-only functionality. Prompt templates, chat messaging, and tool use has had to fit into the existing \"backend\". Opening up these Backends to the user directly allows greater functionality, robustness, and transparency.\n",
    "\n",
    "Publically, there are three pieces of functionality that Backends expose:\n",
    "\n",
    "**1. Prompt Generation**\n",
    "- Given a Signature and Examples, generate a Prompt template and Language Model Arguments.\n",
    "\n",
    "**2. Generation**\n",
    "- Given a Prompt/Messages & Additional arguments, generate text from a Language Model.\n",
    "\n",
    "**3. Extraction**\n",
    "- Given the output of a Language Model, extract the Signature field values, generated by the Language Model.\n",
    "\n",
    "All three pieces, are tightly coupled, and make up a single Backend.\n",
    "\n",
    "Privately, there are three pieces of functionality that Backends inherit:\n",
    "\n",
    "**1. Caching**\n",
    "- Traditionally, each LM module built had to have caching built into the individual model.\n",
    "- All Backends, immediately enable caching out of the box. If you pass the same arguments into a 'generate' call in the Backend, you will return the same outputs.\n",
    "\n",
    "**2. Tracing**\n",
    "- We now track standardized inputs and outputs, for all Backends.\n",
    "- This allows us to get greater transparency into the individual inputs/outputs to each call.\n",
    "\n",
    "**3. Incomplete Retries**\n",
    "- In DSPy currently, during generation, if an incomplete Completion is provided, the Generation is retried until a complete Example is provided.\n",
    "- All backends, inherit this functionality, and will retry an individual completion a few times until a valid Example is generated.\n",
    "\n",
    "This simplifies and standardizes previous DSPy functionality.\n",
    "\n",
    "To start we have created a `TextBackend`, which mimics existing DSPy functionality, and should provide . This API allows us to expand into novel Backends as well, such as a `JSONBackend` (for JSON Mode models), `ToolBackend` (for Too Use), and even bespoke backends leveraging community projects such as `OutlinesBackend` or `InstructorBackend`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ef228-486e-4145-9902-ebd7539e7fd2",
   "metadata": {},
   "source": [
    "## How are 'Backend's used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32ee9c1-8bb7-429f-9ec1-b6fe4362ca6e",
   "metadata": {},
   "source": [
    "Previously all Language Model functionality is expressed in the `dsp.modules`.  \n",
    "Most of the existing language model functionality should ship out of the box with the new `TextBackend`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0506a141-b8a9-4570-905c-ca289970618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, to initialize GPT-3 as the primary Language Model you would do this:\n",
    "import dspy\n",
    "from dspy import OpenAI\n",
    "\n",
    "lm = OpenAI(model=\"gpt-3.5-turbo-instruct\")\n",
    "\n",
    "dspy.settings.configure(lm=lm)\n",
    "\n",
    "# If on the other hand, you want to use Cohere's Command-R, you would do this:\n",
    "from dspy import Cohere\n",
    "\n",
    "lm = Cohere(model=\"command-r\")\n",
    "dspy.settings.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9627c0-2606-4999-a2a2-7c9694a89a7e",
   "metadata": {},
   "source": [
    "Backends are configured in the same way as language models have been historically. And All Predict modules, will just leverage a backend if initialized. To mirror the behaviour above, with the new Backends, you can use the new `TextBackend`. This backend, leverages [LiteLLM](https://docs.litellm.ai/docs/providers) behind the scenes, which enables a wide variety of Language Model providers out of the box, without additional maintenance work needed on the DSPy side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d6441-e715-48e6-a574-30035bf6e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable GPT-3 as the primary language model with the new backends, you would do this:\n",
    "import dspy\n",
    "from dspy.modeling import TextBackend\n",
    "\n",
    "backend = TextBackend(model=\"gpt-3.5-turbo-instruct\")\n",
    "dspy.settings.configure(backend=backend)\n",
    "\n",
    "# And if you wanted to change this to cohere, you would simply do the following:\n",
    "backend = TextBackend(model=\"command-r\")\n",
    "dspy.settings.configure(backend=backend)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b488e40-ea57-4e93-9a5d-1979733e20c5",
   "metadata": {},
   "source": [
    "With one string, and no additional dependencies, we've opened up 100+ language models and provided a much greater level of robustness and flexibility, while reducing maintenance burden on the team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab7d7a-ac77-409d-8f0d-e2ce0c8dfc87",
   "metadata": {},
   "source": [
    "## How are 'Backends' expressed in code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b962ca32-f0b2-43c4-b564-18136ccc7b80",
   "metadata": {},
   "source": [
    "At a high level the private logic for a Backend and abstract methods are illustrated in the `BaseBackend` object.  \n",
    "The public api looks something like the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01dcdf-5b29-447c-b3a5-67603604b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "from abc import abstractmethod\n",
    "from dspy import Signature, Example\n",
    "\n",
    "class BaseBackend:\n",
    "    ...\n",
    "    \n",
    "    @abstractmethod\n",
    "    def prepare_request(self, signature: Signature, example: Example, config: dict, **kwargs) -> dict:\n",
    "        \"\"\"Given a Signature, Example, and Config kwargs, provide a dictionary of arguments for the Backend.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def process_response(\n",
    "        self,\n",
    "        signature: Signature,\n",
    "        example: Example,\n",
    "        response: t.Any,\n",
    "        input_kwargs: dict,\n",
    "        **kwargs,\n",
    "    ) -> Completions:\n",
    "        \"\"\"Given a Signature, Example, and Generated Output, process generations and return completions.\"\"\"\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def make_request(self, **kwargs) -> t.Any:\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd397e4-e206-4c56-afb6-1b895bd5237b",
   "metadata": {},
   "source": [
    "This api, allows us to express all Language Model interactions as a single, composable, but extensible API.  \n",
    "If you are interested in how more of this functionality is provided, I would encourage you to take a look at the `TextBackend` and `JSONBackend` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c5904-cd21-4b7c-b234-dd5652670ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
