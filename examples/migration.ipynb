{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../docs/docs/static/img/dspy_logo.png\" alt=\"DSPy7 Image\" height=\"120\"/>\n",
    "\n",
    "### Migrating from DSPy 2.4 to 2.5\n",
    "\n",
    "**DSPy 2.5** focuses on improving the _pre-optimization_ developer experience, i.e., making it easier to obtain higher-quality outputs from various LMs _out of the box_ prior to setting up a DSPy optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tl;dr Just set up the LM differently at the start.\n",
    "\n",
    "For most applications, you will only need to change 1-2 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "lm = dspy.LM('openai/gpt-4o-mini')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of your code can remain unchanged. However, make sure to experiment with your modules. Improvements to **Adapters** will affect the underlying prompts, before and after optimization, and will enforce stricter validation for LM outputs.\n",
    "\n",
    "If you want to dive deeper, here are some new things and things to keep in mind when upgrading:\n",
    "\n",
    "1. **Use the `dspy.LM` class for setting up language models.**\n",
    "2. **Invoke LMs in the same way as before.**\n",
    "3. **This internally uses new DSPy `Adapters`, leading to better prompting out of the box.**\n",
    "4. **Advanced: If needed, you can set up your own Adapter for LMs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Use the `dspy.LM` class for setting up language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier versions of DSPy involved various clients for different LM providers, like `dspy.OpenAI`, `dspy.GoogleVertexAI`, and `dspy.HFClientTGI`. These are now deprecated and will be removed in DSPy 2.6.\n",
    "\n",
    "Instead, use `dspy.LM` to access any LM endpoint for local and remote models. This relies on [LiteLLM](https://github.com/BerriAI/litellm) to translate the different client APIs into an OpenAI-compatible interface.\n",
    "\n",
    "Any [provider supported in LiteLLM](https://docs.litellm.ai/docs/providers) should work with `dspy.LM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI, which can authenticate via OPENAI_API_KEY.\n",
    "lm = dspy.LM('openai/gpt-4o-mini')\n",
    "\n",
    "# Anthropic, which can authenticate via ANTHROPIC_API_KEY.\n",
    "anthropic_lm = dspy.LM('anthropic/claude-3-opus-20240229')\n",
    "\n",
    "# You can also pass auth information directly.\n",
    "anthropic_lm = dspy.LM('anthropic/claude-3-opus-20240229', api_key=\"..\", api_base=\"..\")\n",
    "\n",
    "# Cohere, which can authenticate via COHERE_API_KEY.\n",
    "cohere_lm = dspy.LM('cohere/command-nightly')\n",
    "\n",
    "# Databricks, which can authenticate via DATABRICKS_API_KEY & DATABRICKS_API_BASE (or automatically on a DB workspace).\n",
    "databricks_llama3 = dspy.LM('databricks/databricks-meta-llama-3-1-70b-instruct')\n",
    "\n",
    "# or many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any OpenAI-compatible endpoint is easy to set up with an `openai/` prefix. This works well for open LMs from HuggingFace hosted locally with SGLang, VLLM, or HF Text-Generation-Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sglang_port = 7501\n",
    "sglang_url = f\"http://localhost:{sglang_port}/v1\"\n",
    "sglang_llama = dspy.LM(\"openai/meta-llama/Meta-Llama-3-8B-Instruct\", api_base=sglang_url)\n",
    "\n",
    "# You could also use text mode, in which the prompts are *not* formatted as messages.\n",
    "sglang_llama_text = dspy.LM(\"openai/meta-llama/Meta-Llama-3-8B-Instruct\", api_base=sglang_url, model_type='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring LM attributes\n",
    "\n",
    "For any LM, you can configure any of the following attributes at initialization or per call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini = dspy.LM('openai/gpt-4o-mini', temperature=0.9, max_tokens=3000, stop=None, cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching is supported. Old clients will show a deprecation warning and will be removed in DSPy 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Invoke LMs in the same way as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoking the LM directly with a prompt is unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(\"What is 2+2?\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For chat LMs, you can pass a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm(messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "             {\"role\": \"user\", \"content\": \"What is 2+2?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using DSPy modules remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = dspy.ChainOfThought('question -> answer')\n",
    "module(question=\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each LM object maintains a history of interactions, including inputs, outputs, token usage, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lm.history)  # e.g., number of LM calls\n",
    "lm.history[-1].keys()  # access metadata of the last call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) This internally uses new DSPy `Adapters`, leading to better prompting out of the box.\n",
    "\n",
    "DSPy 2.5 introduces **Adapters**, which act as a layer between Signatures and LMs, responsible for formatting instructions, examples, and other I/O fields as well as parsing the outputs. The new Adapters enhance pre-optimization prompt quality and enforce stricter validation for outputs."
   ]
