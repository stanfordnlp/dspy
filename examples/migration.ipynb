{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../docs/images/DSPy8.png\" alt=\"DSPy7 Image\" height=\"120\"/>\n",
    "\n",
    "### Migrating from DSPy 2.4 to 2.5\n",
    "\n",
    "**DSPy 2.5** focuses on improving the _pre-optimization_ developer experience, i.e. making it easier to obtain higher-quality outputs from various LMs _out of the box_ prior to setting up a DSPy optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tl;dr Just set up the LM differently at the start.\n",
    "\n",
    "For most applications, you will only need to change 1-2 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "\n",
    "lm = dspy.LM('openai/gpt-4o-mini')\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of your code can remain unchanged. However, make sure to experiment with your modules. Improvements to **Adapters** will affect the underlying prompts, before and after optimization, and will enforce stricter validation for LM outputs.\n",
    "\n",
    "If you want to dive deeper, here's are some new things and things to keep in mind when upgrading.\n",
    "\n",
    "1. **Use the `dspy.LM` class for setting up language models.**\n",
    "2. **Invoke LMs in the same way as before.**\n",
    "3. **This internally uses new DSPy `Adapters`, leading to better prompting out of the box.**\n",
    "4. **Advanced: If needed, you can set up your own Adapter for LMs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Use the `dspy.LM` class for setting up language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier versions of DSPy involved tens of clients for different LM providers, e.g. `dspy.OpenAI`, `dspy.GoogleVertexAI`, and `dspy.HFClientTGI`, etc. These are now deprecated and will be removed in DSPy 2.6.\n",
    "\n",
    "Instead, use `dspy.LM` to access any LM endpoint for local and remote models. This relies on [LiteLLM](https://github.com/BerriAI/litellm) to translate the different client APIs into an OpenAI-compatible interface.\n",
    "\n",
    "Any [provider supported in LiteLLM](https://docs.litellm.ai/docs/providers) should work with `dspy.LM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI, which can authenticate via OPENAI_API_KEY.\n",
    "lm = dspy.LM('openai/gpt-4o-mini')\n",
    "\n",
    "# Anthropic, which can authenticate via ANTHROPIC_API_KEY.\n",
    "anthropic_lm = dspy.LM('anthropic/claude-3-opus-20240229')\n",
    "\n",
    "# You can also pass auth information directly.\n",
    "anthropic_lm = dspy.LM('anthropic/claude-3-opus-20240229', api_key=\"..\", api_base=\"..\")\n",
    "\n",
    "# Cohere, which can authenticate via COHERE_API_KEY.\n",
    "cohere_lm = dspy.LM('cohere/command-nightly')\n",
    "\n",
    "# Databricks, which can authenticate via DATABRICKS_API_KEY & DATABRICKS_API_BASE (or automatically on a DB workspace).\n",
    "databricks_llama3 = dspy.LM('databricks/databricks-meta-llama-3-1-70b-instruct')\n",
    "\n",
    "# or many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any OpenAI-compatible endpoint is easy to set up with an `openai/` prefix as well. This works great for open LMs from HuggingFace hosted locally with SGLang, VLLM, or HF Text-Generation-Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sglang_port = 7501\n",
    "sglang_url = f\"http://localhost:{sglang_port}/v1\"\n",
    "sglang_llama = dspy.LM(\"openai/meta-llama/Meta-Llama-3-8B-Instruct\", api_base=sglang_url)\n",
    "\n",
    "# You could also use text mode, in which the prompts are *not* formatted as messages.\n",
    "sglang_llama_text = dspy.LM(\"openai/meta-llama/Meta-Llama-3-8B-Instruct\", api_base=sglang_url, model_type='text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuring LM attributes\n",
    "\n",
    "For any LM, you can configure any of the following attributes at initialization or per call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini = dspy.LM('openai/gpt-4o-mini', temperature=0.9, max_tokens=3000, stop=None, cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching.\n",
    "\n",
    "Old clients still exist but you'll get a deprecation warning. They will be removed in DSPy 2.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Invoke LMs in the same way as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoking the LM directly with a prompt is the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 + 2 equals 4.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(\"What is 2+2?\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For chat LMs, you can pass a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2 + 2 equals 4.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm(messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "             {\"role\": \"user\", \"content\": \"What is 2+2?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using DSPy modules is the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='To solve the equation 2 + 2, we simply add the two numbers together. The number 2 represents a quantity, and when we add another 2 to it, we are combining these quantities. Therefore, 2 + 2 equals 4.',\n",
       "    answer='4'\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = dspy.ChainOfThought('question -> answer')\n",
    "module(question=\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every LM object maintains the history of its interactions, including inputs, outputs, token usage (and $$$ cost), and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['prompt', 'messages', 'kwargs', 'response', 'outputs', 'usage', 'cost'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm.history)  # e.g., 3 calls to the LM\n",
    "\n",
    "lm.history[-1].keys()  # access the last call to the LM, with all metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) This internally uses new DSPy `Adapters`, leading to better prompting out of the box.\n",
    "\n",
    "Prompt optimizers in DSPy generate and tune the _instructions_ or the _examples_ in the prompts corresponding to your Signatures. DSPy 2.5 introduces **Adapters** as a layer between Signatures and LMs, responsible for formatting these pieces (Signature I/O fields, instructions, and examples) as well as generating and parsing the outputs. In DSPy 2.5, the default Adapters are now more natively aware of chat LMs and are responsible for enforcing types, building on earlier experimental features from `TypedPredictor` and `configure(experimental=True)`. In our experience, this change tends to deliver more consistent pre-optimization quality from various LMs, especially for sophisticated Signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='The first claim states that \"Python was released in 1991,\" which is true. Python was indeed first released by Guido van Rossum in February 1991. The second claim states that \"Python is a compiled language.\" This is false; Python is primarily an interpreted language, although it can be compiled to bytecode, it is not considered a compiled language in the traditional sense like C or Java.',\n",
       "    verdicts=[True, False]\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_checking = dspy.ChainOfThought('claims -> verdicts: list[bool]')\n",
    "fact_checking(claims=[\"Python was released in 1991.\", \"Python is a compiled language.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can as usual inspect the last prompt (and output) from an LM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `claims` (str)\n",
      "\n",
      "Your output fields are:\n",
      "1. `reasoning` (str)\n",
      "2. `verdicts` (list[bool])\n",
      "\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## claims ## ]]\n",
      "{claims}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## verdicts ## ]]\n",
      "{verdicts}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "In adhering to this structure, your objective is: \n",
      "        Given the fields `claims`, produce the fields `verdicts`.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## claims ## ]]\n",
      "[1] «Python was released in 1991.»\n",
      "[2] «Python is a compiled language.»\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `reasoning`, then `verdicts`, and then ending with the marker for `completed`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "The first claim states that \"Python was released in 1991,\" which is true. Python was indeed first released by Guido van Rossum in February 1991. The second claim states that \"Python is a compiled language.\" This is false; Python is primarily an interpreted language, although it can be compiled to bytecode, it is not considered a compiled language in the traditional sense like C or Java.\n",
      "\n",
      "[[ ## verdicts ## ]]\n",
      "[True, False]\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stricter validation for LM outputs.\n",
    "\n",
    "Previously, only `TypedPredictor`s had strict validation for LM outputs. This is now in all DSPy modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When parsing the output: Expected dict_keys(['reasoning', 'output']) but got dict_keys(['reasoning'])\n"
     ]
    }
   ],
   "source": [
    "# a module with insufficient tokens to format the output\n",
    "bad_module = dspy.ChainOfThought('input -> output: list[str]', max_tokens=5)\n",
    "\n",
    "try:\n",
    "    bad_module(input='oops')\n",
    "except ValueError as e:\n",
    "    print(\"When parsing the output:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are currently considering a \"soft failure\" mode, which fills failed fields with `None`s for cases that don't require explicit exception handling. We are also working on more accessible exceptions and error messages in future 2.5.* releases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Advanced: If needed, you can set up your own Adapter for LMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapters are the first step towards DSPy optimizers that tune the templates themselves. They also make it much easier for you to adapt the templates to your needs programatically, though that remains discouraged for most applications since such effort is often best spent on a program's control flow or optimization. At a minimum, an Adapter defines a method `format(signature, demos, inputs)` that prepares a list of messages (or a prompt string) and a method `parse(signature, completion)` that extract a `dict` of output fields. You can also overwrite `__call__(lm, lm_kwargs, signature, demos, inputs)` to refine the generation or decoding logic for your signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.configure(adapter=dspy.ChatAdapter())  # ChatAdapter is the default adapter, but you can override it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next few weeks, we will release into DSPy 2.5 a number of new updates, including: (1) optimizable adapters, smarter retries, better exceptions, support for images, support for multi-turn signatures, improved finetuning and assertions, concurrent with other ongoing efforts from the [DSPy Roadmap](https://github.com/stanfordnlp/dspy/blob/main/docs/docs/roadmap.md), which include more powerful optimizers, human-in-the-loop processes, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jun2024_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
