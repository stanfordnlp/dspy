{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr-ssd/dilara/.miniconda3/envs/dspyprod/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Enable reloading on code changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Setting the environment variables\n",
    "import os # noqa\n",
    "\n",
    "# os.environ[\"DSPY_CACHEDIR\"] =\n",
    "# os.environ[\"DSP_CACHEDIR\"] =\n",
    "# os.environ[\"OPENAI_API_KEY\"] =\n",
    "\n",
    "# Import the library\n",
    "import dspy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Showcasing `LM.finetune()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The average distance from the Earth to the Moon is about 238,855 miles (384,400 kilometers). However, this distance can vary slightly due to the Moon's elliptical orbit, ranging from approximately 225,623 miles (363,104 kilometers) at its closest (perigee) to about 252,088 miles (405,696 kilometers) at its farthest (apogee).\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example call to an LM before fine-tuning\n",
    "lm = dspy.LM('gpt-4o-mini-2024-07-18')\n",
    "lm(\"How far is the Moon from Earth?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LM.finetune(), BSFT, and BetterTogether requires this flag\n",
    "dspy.settings.experimental = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Validating the data format"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dspy.clients.openai.TrainingJobOpenAI"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Finetune] Saving the data to a file\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Uploading the data to the provider\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Finetune] Start remote training\n",
      "[Finetune] Wait for training to complete\n",
      "[Finetune] Get trained model if the run was a success\n"
     ]
    }
   ],
   "source": [
    "# Let's construct a dummy dataset\n",
    "message = {\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"384,400 kilometers\"},\n",
    "  ]\n",
    "}\n",
    "training_data = [message] * 20\n",
    "\n",
    "# Let's finetune the model\n",
    "train_kwargs = {\n",
    "  \"n_epochs\": 1,\n",
    "}\n",
    "\n",
    "job = lm.finetune(\n",
    "  train_data=training_data,\n",
    "  train_kwargs=train_kwargs,\n",
    "  data_format=\"chat\",  # Could be left empty, inferred from \"lm.model_type\" as a default\n",
    ")\n",
    "type(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Running the cell below immediately after the cell above returns `False`, indicating that the job is not done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will return False until the job is complete\n",
    "job.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once started, a `job` object can be polled for status, assuming that a provider has implemented the status checking.\n",
    "Note: It takes a bit for the `job.done()` to update once `job.status()` turns to succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingStatus.pending\n",
      "TrainingStatus.pending\n",
      "TrainingStatus.pending\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.running\n",
      "TrainingStatus.succeeded\n",
      "TrainingStatus.succeeded\n",
      "TrainingStatus.succeeded\n"
     ]
    }
   ],
   "source": [
    "while not job.done():\n",
    "    print(job.status())\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<dspy.clients.lm.LM object at 0x7f08be785730>\n",
      "Base model: gpt-4o-mini-2024-07-18\n",
      "Fine-tuned model: ft:gpt-4o-mini-2024-07-18:stanford::AOHSK6y9\n"
     ]
    }
   ],
   "source": [
    "# Once the job is complete, the fine-tuned LM can be obtained via job.result()\n",
    "finetuned_lm = job.result()\n",
    "print(finetuned_lm)\n",
    "\n",
    "# We can look at the model IDs to ensure that the fine-tuned model is different\n",
    "print(f\"Base model: {lm.model}\")\n",
    "print(f\"Fine-tuned model: {finetuned_lm.model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['384,400 kilometers']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can check how the fine-tuned LM responds to the query we used for\n",
    "# fine-tuning.\n",
    "finetuned_lm(\"How far is the Moon from Earth?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. LM fine-tuning with a custom `Provider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "from dspy.clients.provider import Provider, TrainingJob, DataFormat\n",
    "\n",
    "# Using LM.finetune(), BSFT, and BetterTogether requires this flag\n",
    "dspy.settings.experimental = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a custom provider with a dummy fine-tune method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomProvider(Provider):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.finetunable = True\n",
    "\n",
    "    @staticmethod\n",
    "    def finetune(\n",
    "        job: TrainingJob,\n",
    "        model: str,\n",
    "        train_data: List[Dict[str, Any]],\n",
    "        train_kwargs: Optional[Dict[str, Any]] = None,\n",
    "        data_format: Optional[DataFormat] = None,\n",
    "    ) -> str:\n",
    "\n",
    "        # Fake fine-tuning\n",
    "        print(\"Fake fine-tuning has started!!\")\n",
    "        time.sleep(15)\n",
    "        print(\"Done\")\n",
    "\n",
    "        # Return the new model name; we are hard-coding an OpenAI model as a\n",
    "        # demo placeholder\n",
    "        model = \"ft:gpt-4o-mini-2024-07-18:stanford::AMDsC653\"\n",
    "        return model\n",
    "    \n",
    "    # # We could also override the launch/kill methods if needed\n",
    "    # def launch(model: str, launch_kwargs: dict):\n",
    "    #     pass\n",
    "\n",
    "    # def kill(model: str, launch_kwargs: dict):\n",
    "    #     pass\n",
    "\n",
    "\n",
    "# We could also create a custom TrainingJob class to implement\n",
    "# .status() and .cancel() methods, but we don't have to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`launch()` is called for the auto-launched model openai/MyAmazingCustomModel -- no action is taken!\n",
      "`kill()` is called for the auto-launched model openai/MyAmazingCustomModel -- no action is taken!\n"
     ]
    }
   ],
   "source": [
    "# We could also pass launch_kwargs if this model needs to be launched before\n",
    "# use, assuming that the launch and kill methods are implemented by the\n",
    "# custom provider.\n",
    "launch_kwargs = {\n",
    "  \"gpu\": 1,\n",
    "  \"max_prompt_length\": 1000,\n",
    "}\n",
    "\n",
    "# Create the LM we want to fine-tune, using a dummy model name\n",
    "model = \"openai/MyAmazingCustomModel\"\n",
    "provider = CustomProvider()\n",
    "lm = dspy.LM(model, provider=provider, launch_kwargs=launch_kwargs)\n",
    "lm.launch()\n",
    "\n",
    "# Query the model -- commented out because the model is not real\n",
    "# lm(\"How far is the Moon from Earth?\")\n",
    "\n",
    "# kill the model once done\n",
    "lm.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake fine-tuning has started!!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dspy.clients.provider.TrainingJob"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Try fine-tune\n",
    "dspy.settings.experimental = True\n",
    "\n",
    "# Let's construct a dummy dataset\n",
    "message = {\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"384,400 kilometers\"},\n",
    "  ]\n",
    "}\n",
    "training_data = [message] * 20\n",
    "\n",
    "# Let's finetune the model\n",
    "train_kwargs = {\n",
    "  \"n_epochs\": 1,\n",
    "}\n",
    "\n",
    "job = lm.finetune(\n",
    "  train_data=training_data,\n",
    "  train_kwargs=train_kwargs,\n",
    "  data_format=\"chat\"\n",
    ")\n",
    "type(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0s] job.done(): False\n",
      "[20s] job.done(): True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Running the command below immediately after the cell above returns `False`, indicating that the job is not done.\n",
    "print(f\"[0s] job.done(): {job.done()}\")\n",
    "\n",
    "# Wait\n",
    "time.sleep(20)\n",
    "\n",
    "# Check again\n",
    "print(f\"[20s] job.done(): {job.done()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the fine-tuned model as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['384,400 kilometers']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = job.result()\n",
    "lm(\"How far is the Moon from Earth?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Showcasing `BootstrapFinetune`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Task Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example setup using HotPotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.datasets import HotPotQA\n",
    "from dspy.evaluate import Evaluate # noqa\n",
    "from dsp.utils.utils import deduplicate # noqa\n",
    "\n",
    "\n",
    "# We are setting the experimental flag to True to make use of the fine-tuning\n",
    "# features that are still in development.\n",
    "dspy.settings.configure(experimental=True)\n",
    "\n",
    "# Define the program\n",
    "class BasicMH(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, num_hops=2):\n",
    "        super().__init__()\n",
    "        self.num_hops = 2\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_query = [dspy.ChainOfThought(\"context, question -> search_query\") for _ in range(self.num_hops)]\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.num_hops):\n",
    "            search_query = self.generate_query[hop](context=context, question=question).search_query\n",
    "            passages = self.retrieve(search_query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        answer = self.generate_answer(context=context, question=question).copy(context=context)\n",
    "        return answer\n",
    "\n",
    "# Prepare the dataset\n",
    "TRAIN_SIZE = 1000\n",
    "DEV_SIZE = 500\n",
    "dataset = HotPotQA(train_seed=1, eval_seed=2023, test_size=0, only_hard_examples=True)\n",
    "trainset = [x.with_inputs('question') for x in dataset.train][:TRAIN_SIZE]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev][:DEV_SIZE]\n",
    "\n",
    "# Prepare the metric and evaluator\n",
    "NUM_THREADS = 12\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluate = Evaluate(devset=devset, metric=metric, num_threads=NUM_THREADS, display_progress=True)\n",
    "\n",
    "# Prepare the retriever model\n",
    "COLBERT_V2_ENDPOINT = \"http://20.102.90.50:2017/wiki17_abstracts\"\n",
    "retriever = dspy.ColBERTv2(url=COLBERT_V2_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Demo of `BootstrapFinetune`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LM.finetune(), BSFT, and BetterTogether requires this flag\n",
    "dspy.settings.experimental = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows the different ways the `BootstrapFinetune` can be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) BSFT can be initilized with no arguments!\n",
    "weight_optimizer = dspy.BootstrapFinetune()\n",
    "\n",
    "# (2) Better to optimize with a metric to be used for filtering data before\n",
    "# fine-tuning\n",
    "weight_optimizer = dspy.BootstrapFinetune(\n",
    "  metric=metric\n",
    ")\n",
    "\n",
    "# (3) Bootstrap fine-tune accepts other parameters as well, as shown below\n",
    "train_kwargs = {\n",
    "  \"n_epochs\": 1,\n",
    "}\n",
    "adapter = dspy.ChatAdapter()\n",
    "\n",
    "weight_optimizer = dspy.BootstrapFinetune(\n",
    "  metric=metric,               # Can be left empty, leads to no filtering\n",
    "  multitask=True,              # We can also handle False!\n",
    "  train_kwargs=train_kwargs,   # Can be left empty\n",
    "  adapter=adapter,             # Can be left empty, leads to adapters inferred from the LM\n",
    "  exclude_demos=False,         # Can be left empty\n",
    "  num_threads = 1,             # Can be left empty\n",
    ")\n",
    "\n",
    "\n",
    "# (4) The adapter and train_kwargs arguments could be passed as dictionaries\n",
    "# mapping LMs to their respective adapters/train_kwargs. This is useful when the\n",
    "# predictors of the program point to different LMs.\n",
    "lm = dspy.LM('gpt-4o-mini-2024-07-18')\n",
    "adapter = dspy.ChatAdapter()\n",
    "\n",
    "train_kwargs = {\n",
    "  lm: {\n",
    "    \"n_epochs\": 1,\n",
    "  },\n",
    "  # lm2: train_kwargs2,\n",
    "}\n",
    "adapter = {\n",
    "  lm: adapter,\n",
    "  # lm2: adapter2,\n",
    "}\n",
    "\n",
    "weight_optimizer = dspy.BootstrapFinetune(\n",
    "  metric=metric,               # Can be left empty, leads to no filtering\n",
    "  multitask=True,              # We can also handle False!\n",
    "  train_kwargs=train_kwargs,   # Can be left empty\n",
    "  adapter=adapter,             # Can be left empty, leads to adapters inferred from the LM\n",
    "  exclude_demos=False,         # Can be left empty\n",
    "  num_threads = 1,             # Can be left empty\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below shows an example of running `BootstrapFinetune`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the student and teacher programs...\n",
      "Ensuring that the student is not compiled.\n",
      "No teacher provided. Using a copy of the student program as the teacher.\n",
      "Bootstrapping data...\n",
      "Average Metric: 5 / 10  (50.0): 100%|██████████| 10/10 [00:00<00:00, 416.94it/s]\n",
      "Preparing the train data...\n",
      "Collected data for 10 examples\n",
      "After filtering for score, 5 examples remain\n",
      "Using 15 data points for fine-tuning the model: gpt-4o-mini-2024-07-18\n",
      "Starting LM fine-tuning...\n",
      "1 fine-tuning job(s) to start.\n",
      "Starting 1 fine-tuning jobs...\n",
      "[OpenAI Provider] Validating the data format\n",
      "[OpenAI Provider] Saving the data to a file\n",
      "[OpenAI Provider] Data saved to /scr-ssd/dilara/.cache/dspy-new/finetune/4fd944783dbb3639.jsonl\n",
      "[OpenAI Provider] Uploading the data to the provider\n",
      "[OpenAI Provider] Start remote training\n",
      "[OpenAI Provider] Job started with the OpenAI Job ID ftjob-gUXOgSEqEyV3v9Q6JgAqic9G\n",
      "[OpenAI Provider] Wait for training to complete\n",
      "[OpenAI Provider] Attempting to retrieve the trained model\n",
      "[OpenAI Provider] Model retrieved: ft:gpt-4o-mini-2024-07-18:stanford::AOI4YHf2\n",
      "Job 1/1 completed.\n",
      "Updating the student program with the fine-tuned LMs...\n",
      "BootstrapFinetune has finished compiling the student program.\n"
     ]
    }
   ],
   "source": [
    "# Using method (3) from above to create a weight-optimized program\n",
    "train_kwargs = {\n",
    "  \"n_epochs\": 1,\n",
    "}\n",
    "adapter = dspy.ChatAdapter()\n",
    "\n",
    "weight_optimizer = dspy.BootstrapFinetune(\n",
    "  metric=metric,               # Can be left empty, leads to no filtering\n",
    "  multitask=True,              # We can also handle False!\n",
    "  train_kwargs=train_kwargs,   # Can be left empty\n",
    "  adapter=adapter,             # Can be left empty, leads to adapters inferred from the LM\n",
    "  exclude_demos=False,         # Can be left empty\n",
    "  num_threads = 1,             # Can be left empty\n",
    ")\n",
    "\n",
    "lm = dspy.LM('gpt-4o-mini-2024-07-18')\n",
    "small_trainset = trainset[:10] # Use a small subset of the training data\n",
    "\n",
    "with dspy.context(lm=lm, rm=retriever):\n",
    "  weight_optimized_program = weight_optimizer.compile(\n",
    "    student=BasicMH(),\n",
    "    trainset=small_trainset,\n",
    "    teacher=None,             # Doesn't need to be set, student is used as the teacher by default\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-4o-mini-2024-07-18:stanford::AOI4YHf2\n",
      "ft:gpt-4o-mini-2024-07-18:stanford::AOI4YHf2\n",
      "ft:gpt-4o-mini-2024-07-18:stanford::AOI4YHf2\n"
     ]
    }
   ],
   "source": [
    "for p in weight_optimized_program.predictors():\n",
    "  print(p.lm.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Demo of `BetterTogether`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Task Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example setup using HotPotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.datasets import HotPotQA\n",
    "from dspy.evaluate import Evaluate # noqa\n",
    "from dsp.utils.utils import deduplicate # noqa\n",
    "\n",
    "\n",
    "# We are setting the experimental flag to True to make use of the fine-tuning\n",
    "# features that are still in development.\n",
    "dspy.settings.configure(experimental=True)\n",
    "\n",
    "# Define the program\n",
    "class BasicMH(dspy.Module):\n",
    "    def __init__(self, passages_per_hop=3, num_hops=2):\n",
    "        super().__init__()\n",
    "        self.num_hops = 2\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.generate_query = [dspy.ChainOfThought(\"context, question -> search_query\") for _ in range(self.num_hops)]\n",
    "        self.generate_answer = dspy.ChainOfThought(\"context, question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        context = []\n",
    "        \n",
    "        for hop in range(self.num_hops):\n",
    "            search_query = self.generate_query[hop](context=context, question=question).search_query\n",
    "            passages = self.retrieve(search_query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        answer = self.generate_answer(context=context, question=question).copy(context=context)\n",
    "        return answer\n",
    "\n",
    "# Prepare the dataset\n",
    "TRAIN_SIZE = 1000\n",
    "DEV_SIZE = 500\n",
    "dataset = HotPotQA(train_seed=1, eval_seed=2023, test_size=0, only_hard_examples=True)\n",
    "trainset = [x.with_inputs('question') for x in dataset.train][:TRAIN_SIZE]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev][:DEV_SIZE]\n",
    "\n",
    "# Prepare the metric and evaluator\n",
    "NUM_THREADS = 12\n",
    "metric = dspy.evaluate.answer_exact_match\n",
    "evaluate = Evaluate(devset=devset, metric=metric, num_threads=NUM_THREADS, display_progress=True)\n",
    "\n",
    "# Prepare the retriever model\n",
    "COLBERT_V2_ENDPOINT = \"http://20.102.90.50:2017/wiki17_abstracts\"\n",
    "retriever = dspy.ColBERTv2(url=COLBERT_V2_ENDPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LM.finetune(), BSFT, and BetterTogether requires this flag\n",
    "dspy.settings.experimental = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 4 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n"
     ]
    }
   ],
   "source": [
    "# (1) The only required argument we require for BetterTogether is the metric\n",
    "better_together = dspy.BetterTogether(\n",
    "  metric=metric        # This is the only metric we require!\n",
    "                       # We could also consider not requiring it if BootstrapFewShotWithRandomSearch is modified.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 3 traces per predictor.\n",
      "Will attempt to bootstrap 6 candidate sets.\n"
     ]
    }
   ],
   "source": [
    "# (2) We can also pass the weight and prompt optimizers we initialized\n",
    "train_kwargs = {\n",
    "  \"n_epochs\": 1,\n",
    "}\n",
    "adapter = dspy.ChatAdapter()\n",
    "\n",
    "weight_optimizer = dspy.BootstrapFinetune(\n",
    "  metric=metric,               # Can be left empty, leads to no filtering\n",
    "  multitask=True,              # We can also handle False!\n",
    "  train_kwargs=train_kwargs,   # Can be left empty\n",
    "  adapter=adapter,             # Can be left empty, leads to adapters inferred from the LM\n",
    "  exclude_demos=True,          # We are dropping the demos for fine-tuning \n",
    "  num_threads = 1,             # Can be left empty\n",
    ")\n",
    "\n",
    "prompt_optimizer = dspy.BootstrapFewShotWithRandomSearch(\n",
    "    metric=metric,\n",
    "    max_bootstrapped_demos=3,\n",
    "    max_labeled_demos=3,\n",
    "    num_candidate_programs=6,\n",
    "    num_threads=6\n",
    ")\n",
    "\n",
    "# Initialize BetterTogether\n",
    "better_together = dspy.BetterTogether(\n",
    "  metric=metric,\n",
    "  weight_optimizer=weight_optimizer,   # Can be left empty\n",
    "  prompt_optimizer=prompt_optimizer,   # Can be left empty\n",
    "  seed=2023,                           # Can be left empty\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BetterTogether] Validating the strategy\n",
      "[BetterTogether] Preparing the student program...\n",
      "Ensuring that the student is not compiled\n",
      "[BetterTogether] Compiling the student program...\n",
      "[BetterTogether] Step 1 of 3 - Strategy `p`\n",
      "[BetterTogether] Shuffling the trainset...\n",
      "[BetterTogether] Preparing for prompt optimization...\n",
      "[BetterTogether] Launching the program LMs for sampling...\n",
      "`launch()` is called for the auto-launched model `gpt-4o-mini-2024-07-18` -- no action is taken!\n",
      "[BetterTogether] Compiling the prompt optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 5  (40.0): 100%|██████████| 5/5 [00:02<00:00,  2.05it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 40.0 for seed -3\n",
      "Scores so far: [40.0]\n",
      "Best score so far: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 5  (40.0): 100%|██████████| 5/5 [00:02<00:00,  2.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [40.0, 40.0]\n",
      "Best score so far: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5/45 [00:11<01:28,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0): 100%|██████████| 5/5 [00:06<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [40.0, 40.0, 20.0]\n",
      "Best score so far: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5/45 [00:18<02:25,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0): 100%|██████████| 5/5 [00:07<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [40.0, 40.0, 20.0, 20.0]\n",
      "Best score so far: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3/45 [00:05<01:10,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 5  (40.0): 100%|██████████| 5/5 [00:05<00:00,  1.11s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [40.0, 40.0, 20.0, 20.0, 40.0]\n",
      "Best score so far: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/45 [00:02<01:32,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0): 100%|██████████| 5/5 [00:09<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [40.0, 40.0, 20.0, 20.0, 40.0, 20.0]\n",
      "Best score so far: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/45 [00:01<01:04,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0): 100%|██████████| 5/5 [00:06<00:00,  1.28s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [40.0, 40.0, 20.0, 20.0, 40.0, 20.0, 20.0]\n",
      "Best score so far: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/45 [00:04<03:36,  4.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 5  (40.0): 100%|██████████| 5/5 [00:08<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [40.0, 40.0, 20.0, 20.0, 40.0, 20.0, 20.0, 40.0]\n",
      "Best score so far: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 7/45 [00:11<01:02,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 5  (60.0): 100%|██████████| 5/5 [00:06<00:00,  1.37s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 60.0 for seed 5\n",
      "Scores so far: [40.0, 40.0, 20.0, 20.0, 40.0, 20.0, 20.0, 40.0, 60.0]\n",
      "Best score so far: 60.0\n",
      "9 candidate programs found.\n",
      "[BetterTogether] Killing the LMs used for sampling...\n",
      "`kill()` is called for the auto-launched model `gpt-4o-mini-2024-07-18` -- no action is taken!\n",
      "[BetterTogether] Step 2 of 3 - Strategy `p -> w`\n",
      "[BetterTogether] Shuffling the trainset...\n",
      "[BetterTogether] Preparing for weight optimization...\n",
      "[BetterTogether] Compiling the weight optimizer...\n",
      "[BootstrapFinetune] Preparing the student and teacher programs...\n",
      "Ensuring that the student is not compiled\n",
      "No teacher provided. Using a copy of the student program as the teacher.\n",
      "[BootstrapFinetune] Bootstrapping data...\n",
      "Average Metric: 28 / 50  (56.0): 100%|██████████| 50/50 [03:55<00:00,  4.72s/it]\n",
      "[BootstrapFinetune] Preparing the train data...\n",
      "[BootstrapFinetune] Collected data for 50 examples\n",
      "[BootstrapFinetune] After filtering for score, 28 examples remain\n",
      "Using 84 data points for fine-tuning the model: gpt-4o-mini-2024-07-18\n",
      "[BootstrapFinetune] Starting LM fine-tuning...\n",
      "[BootstrapFinetune] 1 fine-tuning job(s) to start\n",
      "[BootstrapFinetune] Starting 1 fine-tuning jobs...\n",
      "[OpenAI Provider] Validating the data format\n",
      "[OpenAI Provider] Saving the data to a file\n",
      "[OpenAI Provider] Data saved to /scr-ssd/dilara/.cache/dspy-new/finetune/c4bb7c084d3a7ad3.jsonl\n",
      "[OpenAI Provider] Uploading the data to the provider\n",
      "[OpenAI Provider] Starting remote training\n",
      "[OpenAI Provider] Job started with the OpenAI Job ID ftjob-75MiMHGY9xW1gNMGZSs5ht8v\n",
      "[OpenAI Provider] Waiting for training to complete\n",
      "[OpenAI Provider] Attempting to retrieve the trained model\n",
      "[OpenAI Provider] Model retrieved: ft:gpt-4o-mini-2024-07-18:stanford::AOIn7Dm8\n",
      "Job 1/1 completed.\n",
      "[BootstrapFinetune] Updating the student program with the fine-tuned LMs...\n",
      "[BootstrapFinetune] BootstrapFinetune has finished compiling the student program\n",
      "[BetterTogether] Step 3 of 3 - Strategy `p -> w -> p`\n",
      "[BetterTogether] Shuffling the trainset...\n",
      "[BetterTogether] Preparing for prompt optimization...\n",
      "[BetterTogether] Launching the program LMs for sampling...\n",
      "`launch()` is called for the auto-launched model `ft:gpt-4o-mini-2024-07-18:stanford::AOIn7Dm8` -- no action is taken!\n",
      "[BetterTogether] Compiling the prompt optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0): 100%|██████████| 5/5 [00:02<00:00,  2.42it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 20.0 for seed -3\n",
      "Scores so far: [20.0]\n",
      "Best score so far: 20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 5  (60.0): 100%|██████████| 5/5 [00:01<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 60.0 for seed -2\n",
      "Scores so far: [20.0, 60.0]\n",
      "Best score so far: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4/45 [00:08<01:22,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 5  (60.0): 100%|██████████| 5/5 [00:06<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [20.0, 60.0, 60.0]\n",
      "Best score so far: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/45 [00:02<01:03,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 4 / 5  (80.0): 100%|██████████| 5/5 [00:06<00:00,  1.22s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 80.0 for seed 0\n",
      "Scores so far: [20.0, 60.0, 60.0, 80.0]\n",
      "Best score so far: 80.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/45 [00:01<00:58,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 5  (20.0): 100%|██████████| 5/5 [00:06<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [20.0, 60.0, 60.0, 80.0, 20.0]\n",
      "Best score so far: 80.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3/45 [00:04<01:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 5  (60.0): 100%|██████████| 5/5 [00:06<00:00,  1.27s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [20.0, 60.0, 60.0, 80.0, 20.0, 60.0]\n",
      "Best score so far: 80.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/45 [00:01<00:47,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 5  (60.0): 100%|██████████| 5/5 [00:07<00:00,  1.43s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [20.0, 60.0, 60.0, 80.0, 20.0, 60.0, 60.0]\n",
      "Best score so far: 80.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/45 [00:03<02:19,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 5  (60.0): 100%|██████████| 5/5 [00:05<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [20.0, 60.0, 60.0, 80.0, 20.0, 60.0, 60.0, 60.0]\n",
      "Best score so far: 80.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5/45 [00:14<01:58,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 3 / 5  (60.0): 100%|██████████| 5/5 [00:06<00:00,  1.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [20.0, 60.0, 60.0, 80.0, 20.0, 60.0, 60.0, 60.0, 60.0]\n",
      "Best score so far: 80.0\n",
      "9 candidate programs found.\n",
      "[BetterTogether] Killing the LMs used for sampling...\n",
      "`kill()` is called for the auto-launched model `ft:gpt-4o-mini-2024-07-18:stanford::AOIn7Dm8` -- no action is taken!\n",
      "[BetterTogether] BetterTogether has finished compiling the student program.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Running BetterTogether on a small dataset\n",
    "\n",
    "lm = dspy.LM('gpt-4o-mini-2024-07-18')\n",
    "small_trainset = trainset[:50] # Use a small subset of the training data\n",
    "\n",
    "with dspy.context(lm=lm, rm=retriever):\n",
    "  optimized_program = better_together.compile(\n",
    "    student=BasicMH(),\n",
    "    trainset=small_trainset,\n",
    "    strategy=\"p -> w -> p\",\n",
    "    valset_ratio=0.1\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-4o-mini-2024-07-18:stanford::AOIn7Dm8\n",
      "ft:gpt-4o-mini-2024-07-18:stanford::AOIn7Dm8\n",
      "ft:gpt-4o-mini-2024-07-18:stanford::AOIn7Dm8\n"
     ]
    }
   ],
   "source": [
    "for p in optimized_program.predictors():\n",
    "  print(p.lm.model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspy25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
